<!doctype html>
<html lang="fr">
<head>
<meta charset="utf-8">
<title>Paradis artificiels - Carnet de notes d'un traducteur passionn√© d'IA</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="icon" type="image/png" href="stars.png">
<style>

/* Import Inter + Roboto Flex */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Roboto+Flex:wght@300;400;500;600&display=swap');

:root {
	--back-color: rgb(255, 247, 211);
	--text-color: rgb(12, 12, 12);
	--url-color:rgb(120, 120, 120);
}

/* minimalist reset */
* {
	padding: 0;
	margin: 0;
	font-size: 1em;
}

body {
	background-color: var(--back-color);
	color: var(--text-color);
	font-family: Tahoma, Verdana, Arial, sans-serif;
}

main {
	margin: 0 auto;
	padding: 30px 140px 0 140px;
	max-width: 600px;
	min-height: 100vh;
}

h1 {
	font-size: 1.5em;
	font-weight: 700;
	display: inline;
	margin-right: 5px;
}
h2 {
	font-weight: 400;
	margin-top: 0px;
}
h3 {
	font-weight: 400;
	margin-bottom: 30px;
}

footer {
	text-align: right;
	margin-top: 80px;
	padding-bottom: 20px;
}

p {
	line-height: 1.5em;
	margin-top: 20px;
	display: block;
}

a {
	color: rgb(10, 57, 199); /*inherit;*/
	word-wrap: break-word;
	overflow-wrap: break-word;
    text-decoration: underline;
}
a:hover, a:focus {
    text-decoration: underline;
	/*color: var(--url-color);*/
}
code {
  background-color: rgb(255, 228, 171);
  border-radius: 5px;
  font-size: 1.2em;
}
.code-box {
  display: inline-block;
  padding: 4px 10px;
  background-color: rgba(255, 228, 171, 1.0);
  border-radius: 8px;
  font-family: Monospace;
  font-size: 1.2em;
}
.highlight-box {
  display: inline-block;
  padding: 4px 10px;
  background-color: rgba(12, 12, 12, 0.08);
  border-radius: 8px;
  font-family: Georgia, 'Times New Roman', serif;
  font-style: italic;
}
pre {
	overflow-x: auto;
	white-space: pre-wrap;
	word-wrap: break-word;
}

ol {
    counter-reset: item;
}
ol>li {
    counter-increment: item;
    list-style: none inside;
    margin: 20px 0;
    overflow: hidden;
    line-height: 1.5em;
}
ol>li:before {
    content: counter(item) ;
    margin-right: 10px;
    padding: 8px;
    border-radius: 50%;
    width: 25px;
    background: var(--text-color);
    color: var(--back-color);
    text-align: center;
    float: left;
    font-weight: 700;
}

@media (max-width: 600px) {
  main {
  	max-width: 100%;
    padding: 20px 15px;
    font-family: 'Roboto Flex', sans-serif;
    font-size: 1.1em;
  }
  h2 {
    font-size: 0.9em;
    margin-top: 5px;
  }
}

</style>
</head>
<body>

<main>
<h1>Paradis artificiels ‚ú®</h1>
<h2><i>Carnet de notes d'un traducteur passionn√© d'IA</i></h2>
<p style="color: var(--url-color); font-size: 0.9em; margin-top: 10px;">Derni√®re modification : 20/02/2026 √† 16:56:53<br>
Contact : sebington arobase g√©mel point com
</p>

<p><b>20-02-2026</b></p>
<p>Je demande √† pi de me dire comment il g√®re le degr√© de "thinking" qu'un mod√®le d√©ploie. Voici sa r√©ponse.</p>
<p>How Pi Regulates Thinking Level</p>
<p>Pi controls the thinking/reasoning budget through token limits that are sent to the LLM provider's API. Here's the mechanism:</p>
<p>1. Configuration Levels<br>Pi supports these thinking levels: off, minimal, low, medium, high</p>
<p>2. Token Budget Mapping<br>The thinking level translates to specific token budgets. The default mapping (configurable via thinkingBudgets in settings):</p>
<p><pre><code>{
  "thinkingBudgets": {
    "minimal": 1024,
    "low": 4096,
    "medium": 10240,
    "high": 32768
  }
}</code></pre></p>
<p>3. How It's Applied<br>Pi sends the thinking budget to the LLM provider via API parameters. For models that support extended thinking (like Claude, Gemini, etc.), this is typically sent as:</p>
<p>- <code>max_tokens</code> for thinking/reasoning (separate from output tokens)<br>- Provider-specific parameters like <code>thinking.budget</code> (Anthropic) or <code>thoughts</code> (Google)</p>
<p>The thinking budget directly controls how many tokens the model can use for internal reasoning before producing its actual response - higher budgets allow for more complex problem-solving but use more tokens (cost) and take longer.</p>
<p>---</p>
<p>Sur une suggestion d'Olivier je fais un nouvel essai avec le fichier de sous-titres et le texte de la vid√©o en anglais (voir hier), cette fois avec le nouveau Sonnet 4.6.</p>
<p>La diff√©rence de fonctionnement entre Opus 4.6 et Sonnet 4.6 est flagrante. Sonnet R√âFL√âCHIT VRAIMENT BEAUCOUP et consomme une quantit√© impressionnante de "thinking tokens". Beaucoup plus qu'Opus.</p>
<p>Sonnet d√©tecte n√©anmoins quelque chose sur lequel Opus est pass√© sans bruit : un paragraphe est manquant dans le texte en anglais. Pour ce passage, Opus a d√©cid√© de ne pas respecter mes consignes et a traduit lui-m√™me le passage manquant, tandis que Sonnet s'arr√™te pour me demander comment il faut g√©rer le probl√®me :</p>
<p><pre><code>What would you like me to do with those cues?</code></pre></p>
<p><pre><code>a) Remove them entirely from the output VTT
b) Leave the French text in place for those cues only
c) You supply an English version and I slot it in</code></pre></p>
<p>Je lui demande de laisser ces passages en fran√ßais (b) pour pouvoir facilement les retrouver et voir ce qu'il en est.</p>
<p>Pour le reste Sonnet ne se d√©brouille pas aussi bien qu'Opus. M√™me s'il respecte le texte source et n'alt√®re les timestamps, le placement de nombreux sous-titres laisse √† d√©sirer. Cette d√©synchronisation est flagrante quand quand on regarde la vid√©o. Cela n'√©tait pas le cas avec Opus, qui reste synchro du d√©but √† la fin.</p>
<p><b>19-02-2026</b></p>
<p>Ce soir je fais une petite exp√©rience in√©dite. Mon id√©e est de donner √† une IA deux fichiers : un fichier de sous-titres finalis√©s en fran√ßais (VTT) et un fichier contenant le texte de la vid√©o traduit en anglais mais non cal√©. Je demande √† l'IA de conserver les rep√®res temporels du fichier VTT en fran√ßais et d'ins√©rer le texte en anglais √† la place du fran√ßais, si possible en le d√©coupant comme il faut.</p>
<p>Je fais un premier essai avec Codex/gpt-5.3-codex avec le prompt suivant :</p>
<p><pre><code>In this folder there is a subtitle file called video_2.vtt and there is a transcription of the video that was then translated into English: transcription.txt. I would like you to replace the French text in the VTT file with English text in the transcription.txt file. You musn't translate the VTT file but find the appropriate text in transcription.txt and insert it in the VTT file without modifying the VTT timestamps. Ask me questions if this is unclear.</code></pre></p>
<p>Le r√©sultat est tr√®s d√©cevant. C'est peut-√™tre en partie ma faute car mon prompt n'est pas top. Je reprompte Codex plusieurs fois lors de la m√™me session sans obtenir de r√©sultat probant. Codex g√©n√®re bien des ST mais ils sont trop charg√©s ou mal d√©coup√©s ou m√™me d√©cal√©s alors que je lui avait dit de ne pas toucher aux "timestamps". Bref, je n'arrive pas √† rattraper le coup.</p>
<p>Je fais un deuxi√®me essai avec pi/claude-opus-4-6-thinking ‚Ä¢ medium [thinking off/minimal/low/medium/high], disponible via antigravity, avec le prompt suivant :</p>
<p><pre><code>There are two files in this project. I want you to modify the VTT file and replace the French already cued text with the loose English text in the TXT file, while leaving the timestamps untouched. Only use text in @transcription.txt. Do not translate @video_02.fr.vtt yourself. You can adapt the text in @transcription.txt if you deem it necessary but in general you keep it as is. Pay attention to the way text is structured and be careful when cutting up the text into chuncks. Make sure sentences are easy to read and subtitles are professional. Let me know if you have any questions or need any extra tools.</code></pre></p>
<p>Le r√©sultat est absolument nickel. Je suis bluff√©. En un seul prompt.</p>
<p>Du coup je me dis que je me dois d'essayer √† nouveau avec Codex et le m√™me prompt, pour lui donner une seconde chance. Le deuxi√®me essai n'est toujours pas satisfaisant. Cette fois Codex ne touche pas aux timestamps mais contrairement √† Opus, chaque nouveau sous-titre en anglais ne correspond pas au contenu du sous-titre original en fran√ßais et donc aux propos tenus dans la vid√©o.</p>
<p>Codex √©choue l√† o√π Opus, √† la mani√®re d'un humain, semble comprendre le SENS de chaque sous-titre. C'est ce qui m'impressionne le plus. Je me pose la question de savoir si j'aurais obtenu le m√™me r√©sultat dans l'interface web de ChatGPT et de Claude. Est-ce que l'utilisation d'un outil d'agentic coding √©tait vraiment n√©cessaire ? A v√©rifier. En tous cas, l'avantage de l'utilisation d'un "coding harness" c'est qu'on a le choix du mod√®le, ce qui en g√©n√©ral n'est pas le cas avec les interfaces web.</p>
<p><b>18-02-2026</b></p>
<p>Grosse prise de t√™te avec Codex depuis plusieurs jours. D√®s que je le d√©marre et √©cris un prompt, j'obtiens l'erreur suivante : <code>{"error":{"message":"invalid character '(' looking for beginning of value","type":"invalid_request_error","param":null,"code":null}}"</code>.</p>
<p>Je finis par trouver la raison en questionnant ChatGPT : par le pass√© certains de mes scripts en Python faisaient appel √† Ollama comme API, ce qui impliquait l'ajout de la ligne <code>export OPENAI_BASE_URL=http://localhost:11434/v1</code> dans le fichier .bashrc d'Ubuntu. J'avais compl√®tement oubli√© ce d√©tail.</p>
<p><b>16-02-2026</b></p>
<p>Sur models.dev, je filtre les mod√®les pour afficher uniquement opus-4-6 et constate qu'Amazon Bedrock, "un service complet, s√©curis√© et flexible destin√© √† la cr√©ation d‚Äôagents et d‚Äôapplications d‚ÄôIA g√©n√©rative", d√©cline Opus en version "EU", "GLOBAL" et "US". Ce genre de pratique ne me dit rien qui vaille. On peut facilement en d√©duire que l'Europe pourrait avoir acc√®s √† une version l√©g√®rement d√©grad√©e d'Opus, pour que les Etats-Unis puissent conserver un avantage comp√©titif.</p>
<p>EDIT : Mais il n'en est rien, Olivier a trouv√© l'explication. Les services propos√©s sont segment√©s par "r√©gions", ce qui permet aux utilisateurs d'optimiser leurs connexions en <a href="https://docs.aws.amazon.com/fr_fr/bedrock/latest/userguide/cross-region-inference.html">se connectant aux serveurs les plus proches</a>.</p>
<p><b>15-02-2026</b></p>
<p>Kyutai sort <a href="https://kyutai.org/blog/2026-02-12-hibiki-zero">Hibiki-Zero</a>, un nouveau mod√®le de traduction vocale multilingue en temps r√©el. Hibiki-Zero traduit √† partir de 4 langues (dont le fran√ßais) vers l'anglais. Kyutai annonce "pr√©cision, faible latence, qualit√© audio √©lev√©e". En outre, la voix traduite est un clone de la voix source. Ils utilisent une m√©thode d'entra√Ænement bas√©e sur l'apprentissage par renforcement et toutes sortes d'astuces pour r√©duire la latence et am√©liorer la fluidit√©.</p>
<p>Le syst√®me s'appuie sur le codec audio <a href="https://huggingface.co/kyutai/mimi">Mimi</a>, qui transforme les ondes sonores en s√©quences de tokens s√©mantiques et acoustiques pouvant √™tre trait√©s comme du texte par le transformer. Il s'agit de traduction "z√©ro-shot" : lors de l'entra√Ænement du mod√®le, le besoin de donn√©es align√©es mot √† mot n'est plus n√©cessaire. Le mod√®le apprend par lui-m√™me la meilleure strat√©gie de traduction et fonctionne avec un syst√®me de r√©compenses (cf. DeepSeek-R1-Zero).</p>
<p>Pour tester Hibiki-Zero, rien de plus simple : avec uv install√©, il suffit d'utiliser la commande suivante <code>uvx -p 3.13 hibiki-zero serve --gradio-tunnel</code>. On obtient un lien vers une application web ex√©cutable en local. Ne disposant pas d'un GPU je fais l'essai sur Google Colab, qui propose maintenant un terminal. Le r√©sultat est moyen, √ßa fonctionne un peu au d√©but, mais avec une grosse latence et beaucoup d'hallucinations (le mod√®le se met √† parler tout seul et raconte n'importe quoi). Les config propos√©es par Colab ne sont pas assez puissantes.</p>
<p>Je me dis que √ßa vaudrait le coup de tester Hibiki-Zero sur une machine appropri√©e et me renseigne aupr√®s de ChatGPT et Claude sur la d√©marche √† suivre. Apr√®s un temps infini pass√© √† questionner les IA, j'arrive √† la conclusion qu'il me faut une machine √©quipp√©e d'un GPU de type RTX 3090 ou 4090 avec 24 GB VRAM. C√¥t√© fournisseur, <a href="https://www.runpod.io/">RunPod</a> semble √™tre un bon choix.</p>
<p>Je questionne Claude sans fin sur les diff√©rents aspects de la d√©marche, autant techniques que financiers. Il y a en effet quelques "pi√®ges" potentiels quand on utilise ces services, le plus saillant √©tant la gestion de l'espace disque. Il faut bien comprendre la distinction entre "Container Disk", "Volume Disk" et "Network Volume", et s'assurer de <b>ne rien laisser en mode persistant</b> une fois la session termin√©e, sous peine d'avoir une mauvaise surprise √† la fin du mois.</p>
<p><b>13-02-2026</b></p>
<p>Le d√©but de la fin ? Un agent autonome g√©r√© par OpenClaw <a href="https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/">se rebelle</a> contre la personne en charge de la gestion des pull requests de Matplotlib. Conseil : lire en premier le billet de blog de <a href="https://simonwillison.net/">Simon Willison</a> ("An AI Agent Published a Hit Piece on Me"). Internet est en train de devenir un bordel sans nom. Je suis inquiet.</p>
<p>La puissance de Google continue de m'impressionner. Avec un prompt d'une seule phrase, je cr√©e une appli dans Google AI Studio qui transcrit et identifie parfaitement les 4 interlocuteurs d'un enregistrement audio. Quel contraste avec la bataille que je m√®ne depuis plusieurs jours avec WhisperX, whisper-diarization, etc., usines √† gaz aux r√©sultats incertains...</p>
<p>Le mieux est l'ennemi du bien : hier soir tentative d'am√©lioration de mon appli voice2voice 100 % locale (whisper/ollama/pocket-tts) avec Sonnet. Je d√©pense 1,50 $ et le r√©sultat est pire que le point de d√©part. J'ai peut-√™tre fait l'erreur de d√©marrer la session avec GLM-5 avant de switcher pour Sonnet √† cause du tr√®s faible free tier de HuggingFace.</p>
<p><b>12-02-2026</b></p>
<p>Pour mon outil de dialogue oral avec une IA (renomm√© voice2voice), je voudrais pouvoir m'exprimer en fran√ßais. Quand je choisis un mod√®le multilingue (whisper tiny, base ou small) et parle en fran√ßais, √ßa ne fonctionne pas. Alors je cherche et trouve un petit mod√®le de Whisper (base) finetun√© pour le fran√ßais. Mais √ßa ne marche toujours pas. Je demande √† pi de m'aider, ce qu'il fait avec gemini, puis sonnet. J'apprends ainsi qu'il ne faut pas juste changer le mod√®le, il faut √©galement ajouter <code>language = 'fr'</code> dans la fonction qui effectue la transcription avec pywhispercpp.</p>
<p>Avant :</p>
<p><pre><code>segments = whisper_model.transcribe(tmp_path)</code></pre></p>
<p>Apr√®s :</p>
<p><pre><code>segments = whisper_model.transcribe(tmp_path, language='fr')</code></pre></p>
<p>L'utilisation d'un mod√®le multilingue de Whisper devrait donc √™tre possible.</p>
<p><b>11-02-2026</b></p>
<p>Hier soir je fais une session avec pi/sonnet pour essayer d'obtenir une version "cpu-friendly" de <code>whisper-diarization</code>, un des nombreux outils de transcription bas√© sur Whisper. La diarisation consiste √† identifier les diff√©rents interlocuteurs d'une conversation. Le r√©sultat est tr√®s d√©cevant. Le mod√®le perd vite de vue que l'objectif de l'outil est pr√©cis√©ment la diarisation et me propose des solutions au rabais comme s'il voulait √† tout prix terminer la t√¢che qui lui a √©t√© assign√©e, quitte √† cacher la poussi√®re sous le tapis. Int√©ressant de prendre conscience de cela.</p>
<p>Tout comme Ha√Øku, Sonnet √©crit des tonnes de fichiers markdown. Je modifie les instructions de AGENTS.md pour limiter cette tendance. Bref, quand il s'agit d'adapter du code existant, il faut √™tre tr√®s strict et pr√©cis dans les instructions donn√©es au mod√®le et √©viter de parler de "simplification", de "solution alternative", etc. Les choses se passent un peu comme dans la vraie vie. D√®s que nous baissons nos exigences ou que nous exposons une faiblesse, le mod√®le s'y engouffre et adopte une solution de facilit√©. Tirer les le√ßons de cela.</p>
<p><b>10-02-2026</b></p>
<p>Ce que je trouve particuli√®rement jouissif avec les outils d'agentic coding, c'est la possibilit√© de poser des questions sur le fonctionnement d'une application (d'un d√©p√¥t github par exemple) directement au niveau du code. L'outil va scanner le code pour le comprendre dans sa globalit√© ou pour en extraire la substantifique mo√´lle. Libre √† l'utilisateur de demander ensuite ce qu'il veut : comprendre, extraire, am√©liorer, refactorer, adapter... L‚ÄôIA fait tomber les barri√®res et traite chaque ligne de code ou chaque script comme une brique de LEGO.</p>
<p><b>09-02-2026</b></p>
<p>Hier, je demande √† <code>anthropic/claude-haiku-4-5</code> d'effectuer plusieurs modifications majeures sur du code en Python et termine mon prompt par "...or any other improvements you could think of". Haiku effectue (tr√®s mal) les t√¢ches demand√©es et dans la foul√©e se met √† √©crire rapport apr√®s rapport sur ce qu'il vient de faire. Je dirais qu'il br√ªle 60 % de tokens sur les rapports et 40 % sur les t√¢ches. Je suis tr√®s d√©√ßu du r√©sultat car rien de ce que j'ai demand√© ne fonctionne.</p>
<p>Je lui demande de d√©faire ses conneries et de ne pas continuer √† produire tous ces rapports inutiles. Ce comportement a peut-√™tre √©t√© induit par mon prompt trop permissif et au fait que pi est en mode YOLO par d√©faut. Ceci dit je ne peux m'emp√™cher de penser que cela est voulu par Anthropic. Il est possible que le mod√®le Haiku, grand public et moins cher, soit particuli√®rement bavard voire "indisciplin√©" pour nous faire griller davantage de tokens.</p>
<p><b>08-02-2026</b></p>
<p>Les sessions avec pi s'encha√Ænent et me permettent d'am√©liorer mes applis bas√©es sur la voix. Il y en a maintenant trois : voice2prompt, voice2llm_local et voice2llm_groq. Les deux derni√®res sont inspir√©es comme dit pr√©c√©demment des travaux de <a href="https://kyutai.org/">Kyutai</a> en mati√®re de speech-to-text (STT) et de text-to-speech (TTS) combin√©s avec un LLM au milieu.</p>
<p>Comme la partie STT ne fonctionne pas sur mon ordi, je la remplace par Whisper, avec le code de voice2prompt. J'en fais une version qui fonctionne enti√®rement en local (Whisper/Ollama/Pocket-TTS) et une autre dont la partie "LLM/TTS" est g√©r√©e par Groq. Cette version semi-locale fonctionne plut√¥t mieux, avec moins de latence que la version 100 % locale.</p>
<p>L'√©cueil principal est la latence entre la sortie du LLM et sa reconstitution sonore. Plusieurs strat√©gies sont utilis√©es pour tenter de r√©soudre ce probl√®me, avec notamment l'id√©e de "streamer" les phrases ou les mots par "chunks" dans l'entr√©e du mod√®le TTS plutot que lui balancer le texte en un seul bloc. Mais cela ne fonctionne que partiellement.</p>
<p>En revanche, je fais une grande d√©couverte : je trouve enfin un moyen de ne plus utiliser la version standard de PyTorch (ce que fait Pocket-TTS par ex.) qui est √©norme et t√©l√©charge toutes sortes de biblioth√®ques CUDA compatibles avec les machines √©quip√©es de GPU. Si comme moi vous avez une modeste machine sans GPU, il est pr√©f√©rable d'utiliser <code>pytorch-cpu</code>, biblioth√®que ultra-l√©g√®re qui est t√©l√©charg√©e par uv en un clin d'oeil.</p>
<p>Pour cela, il faut modifier les "inline scripts" de uv comme suit.</p>
<p>Avant :</p>
<p><pre><code># /// script
# requires-python = ">=3.10"
# dependencies = [
#     "numpy",
#     "torch",
# ]
# ///</code></pre></p>
<p>Apr√®s :</p>
<p><pre><code># /// script
# requires-python = ">=3.10"
# dependencies = [
#     "numpy",
#     "torch",
# ]
#
# [tool.uv]
# extra-index-url = ["https://download.pytorch.org/whl/cpu"]
# ///</code></pre></p>
<p>Le reste du temps j'√©coute et r√©√©coute <a href="https://www.youtube.com/watch?v=AEmHcFH1UgQ">une interview</a> de mes deux stars du moment parler de leur travaux autour de <a href="https://shittycodingagent.ai/">pi</a>, qui je le rappelle est utilis√© comme orchestrateur dans OpenClaw. Pour saisir toutes les subtilit√©s de cet entretien passionnant, j'en fais une transcription avec Whisper, que je suis en train d'am√©liorer.</p>
<p>En √©coutant les propos tenus, je commence √† comprendre √† quel point pi est diff√©rent des autres outils de la m√™me famille. Il est moins "cl√© en main" et "safe", mais il est aussi beaucoup plus puissant et modulable.</p>
<p><b>05-02-2026</b></p>
<p>La traduction via LLM (ex. ChatGPT) fonctionne de mieux en mieux. Le LLM traite le texte dans son int√©gralit√© et est capable d'une certaine abstraction ce qui lui fait traduire certaines phrases de fa√ßon non-litt√©rale. Pour moi ChatGPT traduit beaucoup mieux que DeepL. J'√©change avec ChatGPT sur le sujet. Conversation int√©ressante m√™me si ChatGPT marche un peu sur des oeufs et avance des choses qu'il temp√®re juste apr√®s, typique.<br>------<br>J'utilise beaucoup Codex CLI avec <code>gpt-5.2-codex</code>, le mod√®le "recommand√©". Tellement que j'ai quasiment atteint la limite hebdomadaire d'usage (il me reste 3%). Je dois attendre jusqu'au 11 f√©vrier 16:13 pour avoir plus. Et moi qui pensais que c'√©tait open bar.<br>------<br>J'ai r√©ussi √† remanier l'appli "Unmute" avec Codex. J'ai d√©cid√© de tout passer en local :<br>- STT avec <code>pywhispercpp</code> et le mod√®le <code>base.en</code> de whisper (code de mon appli voice2prompt)<br>- LLM avec Ollama et <code>gemma3:1b</code>, un petit mod√®le tr√®s rapide en mode CPU<br>- TTS toujours avec Pocket-TTS. Le probl√®me est la latence, que j'ai r√©ussi √† r√©duire √† force de r√©glages (6 sec. > 1 sec.).</p>
<p>Exp√©rimenter ainsi est tr√®s amusant. On ne s'exprime pas de la m√™me fa√ßon √† l'oral et √† l'√©crit. On voit les limites des connaissances et les hallucinations des petits mod√®les. J'ai essay√© <code>qwen3:0.6b</code> mais c'est un "thinking model" donc pas exploitable √† cause de la latence (√† ma connaissance, on ne peut pas l'emp√™cher de penser). J'ai aussi essay√© <code>smollm2:135m</code> mais il si petit qu'il d√©raille assez vite.</p>
<p><b>04-02-2026</b></p>
<p>Retour sur <a href="https://kyutai.org/">Kyutai</a> et ses mod√®les STT et TTS. L'id√©e serait de reproduire le comportement de <a href="https://unmute.sh/">Unmute</a> en local. Selon Kyutai, cela ce fait en 3 parties : un mod√®le STT qui √©coute l'utilisateur et produit une transcription, un LLM qui r√©cup√®re la transcription et pr√©pare une r√©ponse aux propos tenus et un mod√®le TTS qui g√©n√®re une r√©ponse audio √† partir de la sortie du LLM. Pour info, il est possible de dialoguer √† l'oral sur Unmute et de poser des questions sur son fonctionnement (choisir "Explanations").</p>
<p>Pour le text-to-speech, le syst√®me Pocket-TTS de Kyutai fonctionne merveilleusement bien pour un mod√®le de cette taille (236 Mo), notamment avec uv. Pour le speech-to-text en revanche, c'est plus compliqu√© : le mod√®le fait quasiment 2 Go et le script t√©l√©charge aussi un autre fichier de 385 Mo, plus toute la ribambelle de fichiers pytorch et autre (environ 13 Go !).</p>
<p>Moi qui voulais un truc l√©ger, c'est rat√©. De plus, je n'arrive pas √† faire fonctionner la partie STT en local. Je finis par la remplacer par le whisper local que j'utilise d√©j√† avec mon assistant vocal. J'exp√©rimente aussi une variante avec un mod√®le g√©n√©raliste + un mod√®le TTS de Groq : √ßa fonctionne, mais avec un temps de latence de plusieurs secondes.</p>
<p>Je r√©alise ces exp√©rimentations avec <a href="https://developers.openai.com/codex/cli/">Codex CLI</a> qui propose un acc√®s gratuit et (apparemment pleine balle) √† gpt-5.2-codex pendant un mois.</p>
<p><b>03-02-2026</b></p>
<p>J'essaie de comprendre comment les agents effectuent des recherches sur le web. Comme pi est beaucoup plus "bas niveau" que Claude Code, √† chaque fois que pi effectue une recherche, j'examine comment il fait, afin de dissiper des id√©es pr√©con√ßues ou fausses croyances que je pourrais avoir sur son fonctionnement.</p>
<p>Je demande √† pi/kimi-k2-free de cr√©er de nouvelles skills en s'inspirant de skills existantes. Pi cr√©e des skills "sur mesure" qu'il teste abondamment. Je lui demande notamment de faire appel √† l'<a href="https://ai.google.dev/gemini-api/docs">API de Gemini</a>, dont je d√©couvre l'acc√®s par un heureux hasard. Google Cloud a longtemps √©t√© si compliqu√© d'acc√®s que j'avais depuis des ann√©es compl√®tement renonc√© ! Pi me cr√©e une skill "genai-web-search" aux petits oignons. Je lui demande √©galement de s'inspirer de "web-search-summarize" (qui utilise duckduckgo ET groq) pour en cr√©er une version simplifi√©e ("simple-ddg-web-search") qui utilise uniquement DDG via le module pypi "ddgs".</p>
<p>J'explore les possibilit√©s offertes par l'API de Gemini. En faisant des essais et alors que je pointe vers un fichier en utilisant l'arobase, je constate que pi/kimi-k2.5 pense que @ fait partie du nom du fichier et me dit qu'il ne le trouve pas. Je regarde sur GitHub si d'autres utilisateurs ont eu le m√™me probl√®me mais rien. Alors je cr√©e une <a href="https://github.com/badlogic/pi-mono/issues/1206">GitHub Issue</a> en expliquant le probl√®me. Mario Zechner me r√©pond en me disant que c'est le mod√®le qui est un peu simplet ! Il met n√©anmoins en place une strat√©gie pour √©viter que √ßa ne se reproduise.</p>
<p>Ce soir, apr√®s avoir perdu beaucoup de temps √† installer et tester amp, droid et codex (avec z√©ro r√©sultats), je retourne sur pi et ‚Äì√¥ surprise‚Äì constate que <code>claude-opus-4-5-thinking</code> est disponible via google-antigravity. J'en profite pour am√©liorer mon wavideo-editor avec un opus v√©loce et pr√©cis, c'est jouissif. Trente minutes plus tard opus me l√¢che et je termine la session avec <code>anthropic/claude-sonnet-4-5</code> (payant via l'api). Mais les deux mod√®les ont fait du super boulot. Je suis content.</p>
<p><b>02-02-2026</b></p>
<p>Je suis tr√®s surpris <a href="https://lucumr.pocoo.org/2026/1/31/pi/">d'apprendre</a> que pi, l'outil d'agentic coding que j'utilise depuis le 1er janvier, est ce qui fait fonctionner <a href="https://openclaw.ai/">OpenClaw</a> (appel√© pr√©c√©demment ClawdBot, puis MoltBot). OpenClaw fait le buzz sur les r√©seaux depuis plusieurs jours et j'ignorais compl√®tement le lien entre pi et OpenClaw. Par curiosit√© j'ai fait une recherche de la plus ancienne vid√©o YouTube trouv√©e avec le mot cl√© "clawdbot", ce serait <a href="https://www.youtube.com/watch?v=qz23zld5oX4">celle-ci</a>. Pour info le tout premier commit d'OpenClaw date du 24 novembre 2025. Mont√©e fulgurante donc, mais je continue de penser que ce truc est potentiellement source de chaos, et <a href="https://www.youtube.com/watch?v=esXXuejofgk">je ne suis pas le seul</a>. Simon Willison, un fervent pourfendeur du slop, se montre <a href="https://simonwillison.net/2026/Jan/30/moltbook/">√©trangement indulgent</a> envers OpenClaw, m√™me s'il <a href="https://til.simonwillison.net/llms/openclaw-docker">reste sur ses gardes</a> quand il s'agit de l'utiliser.</p>
<p><b>01-02-2026</b></p>
<p>Il arrive qu'on doive configurer un tout petit truc une seule fois en sachant que √ßa peut prendre un certain temps. Dans ce cas, trois possibilit√©s :<br>1) aller voir la doc et se d√©brouiller (le plus chronophage);<br>2) demander √† ChatGPT puis appliquer la solution propos√©e (gain de temps);<br>3) d√©marrer notre outil agentique pr√©f√©r√© et lui demander de s'en occuper (le plus rapide). C'est <a href="http://bytepacking.free.fr/files/pi-session-2026-02-01T18-56-27-486Z_75642a18-d280-4f6f-a5c3-cc32867c602e.html">ce que je viens de faire</a> avec un changement de raccourci clavier dans ghostty. Probl√®me r√©gl√© en quelques secondes avec pi/kimi-k2.</p>
<p><b>30-01-2026</b></p>
<p>Vid√©o <a href="https://www.youtube.com/watch?v=IjjMsfhJEcE">int√©ressante et stimulante</a> d'Alejandro AO (Hugging Face), avec un test comparatif de 4 outils CLI et 3 mod√®les (kimi-k2.5, opus-4.5, codex) sur le th√®me de simulations de syst√®mes physiques (particules d'un gaz, galaxies). Ce qui a attir√© mon attention c'est l'<a href="https://attractors-eight.vercel.app/">appli assez impressionnante</a> cr√©√©e par "@averycode" (√† partir de 8:05) qu'Alejandro essaie de reproduire. On peut controler l'animation √† la main au travers d'une webcam. Tr√®s cool. Alejandro partage ses prompts dans les commentaires de la vid√©o.<br>------<br>Je viens de faire une session avec pi/kimi-k2, puis une autre avec Gemini CLI et franchement, je continue de me demander si les skills sont r√©ellement utiles. Vraiment. Les mod√®les savent se fabriquer tous les outils possibles en quelques minutes, il suffit de leur dire ce que l'on veut obtenir. J'utilise rarement les skills que j'ai import√©es dans pi, et quand je dis au mod√®le de les utiliser, les choses se passent rarement comme pr√©vu et le mod√®le finit par r√©√©crire la skill pour la faire fonctionner comme il l'entend. Pourquoi s'emmerder ?ü§î<br>------<br>Selon <a href="https://www.youtube.com/watch?v=eRu5kIYAAz8">cette vid√©o</a>, le nouveau mod√®le Kimi-K2 aurait un mode "swarm" qui le rendrait plus efficace, avec de multiples "sub-agents" travaillant de concert. Cette fonctionnalit√© serait √©galement possible dans Claude Code, √† condition d'utiliser une version "d√©brid√©e" de Claude Code, <a href="https://github.com/mikekelly/claude-sneakpeek">claude-sneakpeek</a>. Je me demande si ce mode serait disponible "out-of-the-box" dans pi.<br>------<br>La plupart de mes coll√®gues ou amis ont une compr√©hension rudimentaire du fonctionnement d'un LLM. Deux notions essentielles leur √©chappent : (1) le fait qu'un LLM poss√®de une fen√™tre contextuelle finie et (2) le fait que pour chaque nouvelle question d'une m√™me conversation c'est tout le fil de la conversation (les questions et les r√©ponses pr√©c√©dentes) qui est redonn√© au LLM, avec une d√©gradation progressive de la performance (notion de "context rot").</p>
<p>Un ami a par exemple r√©ussi l'exploit d'√©puiser la fen√™tre contextuelle de ChatGPT en utilisant toujours la m√™me conversation pour un probl√®me au long cours. ChatGPT lui a alors propos√© de d√©marrer une nouvelle conversation bas√©e sur une version compact√©e de la pr√©c√©dente. Le m√™me probl√®me se pose pour les d√©veloppeurs qui doivent surveiller la fen√™tre contextuelle comme le lait sur le feu sous peine de perdre plusieurs heures de travail.</p>
<p>L'autre concept m√©connu et pourtant central √† l'IA et aux LLM en particulier est celui d'embeddings.</p>
<p><b>29-01-2026</b></p>
<p>J'am√©liore beaucoup mon assistant vocal (<a href="https://github.com/sebington/voice2prompt">voice2prompt</a>) pour Linux. Olivier le teste sur EndeavourOS / Gnome 3 / X11 et √ßa fonctionne ! C'est encourageant.<br>------<br>Mario Zechner a ajout√© <a href="https://github.com/badlogic/pi-mono/issues/994">Hugging Face</a> aux fournisseurs par d√©faut de <a href="https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent">pi</a>. Plus besoin donc pour ce fournisseur de cr√©er un fichier <code>models.json</code> comme mentionn√© il y a trois jours.<br>------<br>Je teste <code>npx skills add owner/repo</code> (voir <a href="https://skills.sh/">skills.sh</a>). Il faut faire attention aux nombreuses options qu'ils proposent car √ßa a tendance a vouloir installer beaucoup de choses, surtout si on r√©pond oui √† <code>Install the find-skills skill?</code>. Si on se contente de l'installation d'une skill ou deux, √ßa marche tr√®s bien. Deux options sont propos√©es pour la m√©thode d'installation : <code>Symlink (Recommended)</code> ou <code>Copy to all agents</code>, ce qui manque de clart√©. Apparemment il y a un module de tracking anonyme qui recense quelles skills sont install√©es (peut-√™tre activ√© quand on choisit l'option "Symlink"). Plus de d√©tails dans la <a href="https://www.youtube.com/watch?v=PkeC2hNQ9Zw"> vid√©o de Substack</a> ou dans celle de <a href="https://www.youtube.com/watch?v=rcRS8-7OgBo">Hugging Face</a>. Pour info, j'ai volontairement fait le test de t√©l√©charger une skill que j'avais d√©j√† et le script √©crase la skill d√©j√† pr√©sente.<br>------<br>Il y a deux jours, le 27 janvier, j'√©cris ceci dans mon cahier : "R√©flexion sur les skills : sont-elles vraiments utiles ? Dans quel(s) cas le sont-elles ?". Et aujourd'hui je d√©couvre <a href="https://www.youtube.com/watch?v=i0W90tlKfYg">cette vid√©o</a> qui fait r√©f√©rence √† <a href="https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals">cet article</a> de Vercel (dat√© du 27 janvier !) selon lequel les skills ne seraient efficaces que 50 % du temps et pr√©senteraient en outre pas mal d'inconv√©nients. Selon l'article, un seul fichier AGENTS.md charg√© d√®s le d√©part de la session en m√™me temps que le "prompt syst√®me" serait beaucoup plus efficace que de multiples skills que l'agent doit "d√©cider" d'utiliser, ce que, la plupart du temps, il ne fait pas. Contrairement aux outils qui se chargent √† la demande, le contexte de AGENTS.md est accessible √† chaque √©tape du raisonnement. Les outils non utilis√©s peuvent distraire l'agent, un probl√®me inexistant avec le format statique de AGENTS.md. √âb√©!</p>
<p><b>27-01-2026</b></p>
<p>J'utilise pi comme assistant linguistique, toujours avec claude-sonnet-4-5-thinking sous le capot. Je veux compl√©ter un glossaire incomplet (fichier .csv bilingue fran√ßais-anglais) et l'adapter le mieux possible √† un texte ("source") √† traduire. Je demande √† pi/sonnet de comparer le glossaire au texte source et de l'adapter en cons√©quence. Je lui demande aussi de traduire les √©ventuelles nouvelles entr√©es du glossaire, soit par lui-m√™me, soit en utilisant DeepL via son API (je lui donne un exemple de script en Python).</p>
<p><pre><code>Read @texte_brut_fr.txt and @glossaire_general.csv, then do 2 things: (a) find new potential entries to be added to the glossary and (b) remove entries that are not in @texte_brut_fr.txt from @glossaire_general.csv. Entries can be a single word or expressions. For new entries, propose a translation in English, using your own capabilities or @deepl_test.py Python script.</code></pre></p>
<p>Le syst√®me se d√©brouille tr√®s bien avec mes instructions et se cr√©e les outils au fur √† mesure pour atteindre l'objectif. Le r√©sultat est vraiment chouette. Voici un extrait du log.</p>
<p>SUMMARY<br>--------------------------------------------------------------------------------------<br>Original glossary entries:    145<br>Entries removed:               91 (not found in source text)<br>Entries kept:                  54 (found in source text)<br>New entries added:            114 (extracted from source text)<br>Final glossary entries:       168</p>
<p>TRANSLATION METHOD<br>--------------------------------------------------------------------------------------<br>- New entries were translated using DeepL API<br>- 23 translations were manually corrected for better accuracy in academic publishing context<br>- All entries verified to exist in the source text (texte_brut_fr.txt)</p>
<p>FILES CREATED<br>--------------------------------------------------------------------------------------<br>- glossaire_general_backup.csv (backup of original glossary)<br>- glossaire_general.csv (updated glossary - 168 entries)<br>- new_entries_translated.csv (114 new translated entries)<br>- new_entries_to_translate.txt (list of terms to translate)</p>
<p>Je constate dans le log de session de pi que l'outil a bien utilis√© DeepL pour traduire les 114 nouvelles entr√©es et que ‚Äìcerise sur le g√¢teau‚Äì il a ensuite d√©cid√© comme un grand de modifier 23 d'entre-elles, jug√©es "mal-traduites" par DeepL ! Par exemple "coquille" (dans le sens de faute de frappe) avait √©t√© traduit par "shell" au lieu de "typo". L'outil a donc tenu compte du contexte pour proposer une traduction plus pertinente, qualit√© essentielle √† tout traducteur. Waoh.<br>------<br>Un <a href="https://korben.info/llm-biologie-decouverte-structurelle.html">billet</a> tr√®s int√©ressant de Korben sur une "capacit√© √©mergente" des LLM.<br>------<br>Il y aurait un risque av√©r√© que Clawdbot s√®me la pagaille (et je reste poli) sur Internet. En tous cas c'est ce que <a href="https://www.youtube.com/watch?v=Fh-9Y5Q4c20">certains pr√©disent</a>. Bref, on va voir si "Clawdbot = Recipe for disaster".</p>
<p><b>26-01-2026</b></p>
<p>Je trouve la solution au probl√®me de GLM-4.7-Flash / Hugging Face (voir 24/01/2026). Je demande √† claude-sonnet-4-5-thinking directement dans pi.</p>
<p><pre><code>I am getting the following error: "Error: 400 Incorrect role information" when trying to use the Hugging Face model set in @/home/seb/.pi/agent/models.json. Please investigate.</code></pre></p>
<p>Le syst√®me fait une sorte d'auto-diagnostic et trouve la solution en quelques secondes. Je suis sur le cul.</p>
<p><pre><code>I've updated your Hugging Face provider configuration with compatibility flags:
‚Ä¢ Added "supportsDeveloperRole": false - This tells Pi to use `system` role instead of `developer` role, which many models don't support
‚Ä¢ Added "supportsUsageInStreaming": false - Disables streaming usage stats which HuggingFace router may not support
The "400 Incorrect role information" error typically occurs when a model receives a message role it doesn't recognize (like developer instead of system). This should fix the issue.</code></pre></p>
<p>Voici le fichier de configuration r√©par√© :</p>
<p><pre><code>{
  "providers": {
    "huggingface": {
      "baseUrl": "https://router.huggingface.co/v1",
      "apiKey": "HUGGINGFACE_API_KEY",
      "api": "openai-completions",
      "compat": {
        "supportsDeveloperRole": false,
        "supportsUsageInStreaming": false
      },
      "models": [
        {
          "id": "zai-org/GLM-4.7-Flash",
          "name": "GLM-4.7-Flash (HuggingFace)",
          "reasoning": false,
          "input": ["text"],
          "cost": {"input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0},
          "contextWindow": 128000,
          "maxTokens": 32000
        }
      ]
    }
  }
}</code></pre></p>
<p>------</p>
<p>HYPE : sortie de Clawdbot accompagn√© d'une <a href="https://www.youtube.com/watch?v=Qkqe-uRhQJE">rumeur</a> partiellement <a href="https://ucstrategies.com/news/why-everyone-is-suddenly-buying-mac-minis-to-run-clawdbot-you-probably-dont-need-one/">expliqu√©e</a> selon laquelle il faudrait un Mac Mini pour en tirer pleinement parti. Heureusement <a href="https://www.youtube.com/watch?v=ALXaWL70w9M">certains youtubeurs</a> font l'effort de r√©tablir un semblant de rationalit√© dans leurs explications.</p>
<p><b>24-01-2026</b></p>
<p>L'algorithme de YouTube me recommande une vid√©o "de" Richard Feynman [kWCl7diBGos]. Je clique. On entend la voix de Richard Feynman et des propos sur la vitesse de la lumi√®re s'affichent en temps r√©el √† l'√©cran. Tr√®s vite je questionne l'origine de ce truc. Aujourd'hui on peut cloner n'importe quelle voix et lui faire dire ce qu'on veut. Je regarde le nombre de vues : 2,3 million. Je regarde les commentaires. Il y en a plus de 5000. Les auteurs de la vid√©o ont √©pingl√© un commentaire expliquant que ce n'est pas Richard Feynman qu'on entend :</p>
<p>"This channel isn't officially connected to Richard Feynman or his estate. We're here to share his incredible way of explaining physics with a new generation who never got to learn from him directly. This isn't his voice ‚Äî it's our tribute to his teaching style, created purely for education and inspiration. No impersonation intended, just deep respect for one of history's greatest teachers."</p>
<p>Reste √† savoir si les propos tenus sont bien ceux de Feynman et s'ils sont tir√©s d'un cours, d'une conf√©rence ou d'un livre. Pour le savoir il faut aujourd'hui faire tout un travail d'investigation. Un internaute r√©sume la situation :</p>
<p>"cloned voice, 15% fictionalized, this is what Youtube has become".</p>
<p>Je me renseigne sur les revenus potentiellement g√©n√©r√©s par une vid√©o de 2,3 M de vues. ChatGPT donne la fourchette suivante : entre 2000 et 8000 dollars, tandis que Claude est plus optimiste : entre 6900 et 11500 dollars. Pas mal pour du Feynman recycl√©.</p>
<p><b>22-01-2026</b></p>
<p>Le mod√®le <a href="https://huggingface.co/zai-org/GLM-4.7-Flash">zai-org/GLM-4.7-Flash</a> vient d'√™tre mis √† jour. Je tente de l'ajouter √† la liste de mod√®les dans pi en utilisant Hugging Face (HF) Inference Providers. J'utilise les infos trouv√©es quand on clique sur "Deploy > Inference Providers" que je recopie dans <a href="https://github.com/badlogic/pi-mono/blob/main/packages/coding-agent/docs/models.md">un mod√®le de fichier JSON</a> propre √† pi.</p>
<p><pre><code>{
  "providers": {
    "huggingface": {
      "baseUrl": "https://router.huggingface.co/v1",
      "apiKey": "HUGGINGFACE_API_KEY",
      "api": "openai-completions",
      "models": [
        {
          "id": "zai-org/GLM-4.7-Flash",
          "name": "GLM-4.7-Flash (HuggingFace)",
          "reasoning": true,
          "input": ["text"],
          "cost": {"input": 0, "output": 0, "cacheRead": 0, "cacheWrite": 0},
          "contextWindow": 128000,
          "maxTokens": 32000
        }
      ]
    }
  }
}</code></pre></p>
<p>J'ajoute la cl√© API de HF √† .bashrc puis je teste et au d√©but √ßa semble marcher. Puis en fait non : "Error: 400 Incorrect role information". Apparemment pi n'est pas tout √† fait param√©tr√© pour parler correctement √† l'API de HF.</p>
<p><b>22-01-2026</b></p>
<p>Un client nous donne acc√®s √† un glossaire terminologique sous la forme d'un tableur multi-colonnes. Je l'exporte en CSV. Petit probl√®me : il peut y avoir plusieurs termes sur la m√™me ligne pour une langue donn√©e, s√©par√©s par "|". Par exemple :</p>
<p><pre><code>item,item1|item2|item3</code></pre></p>
<p>Je demande √† big-pickle de modifier le glossaire pour n'avoir qu'un terme par ligne et par langue, quitte √† multiplier les lignes.</p>
<p><pre><code>@glossary.csv is a bilingual glossary. Modify its entries in order to remove the pipe '|' character and add more lines. For example if you find item,item1|item2|item3 (one-to-many) you will create 3 lines: item,item1 item,item2 and item,item3. Do the same thing if you have many-to-one or many-to-many lines. Is this clear?</code></pre></p>
<p>Pi/big-pickle r√©soud le probl√®me en un peu plus d'une minute. Il √©crit un script en Python qui fait appel √† Pandas et modifie mon glossaire dans la foul√©e. Je v√©rifie que j'ai bien √† pr√©sent des donn√©es du type</p>
<p><pre><code>item,item1
item,item2
item,item3</code></pre></p>
<p><b>21-01-2026</b></p>
<p>Je constate sur models.dev qu'il y a deux fournisseurs distincts pour les mod√®les MiniMax : minimax et minimax-cn, avec leurs sites respectifs <a href="https://www.minimax.io/">https://www.minimax.io/</a> et <a href="https://www.minimaxi.com/">https://www.minimaxi.com/</a>. Les deux sites sont identiques. Les tarifs aussi. √âtrange. J'imagine que https://www.minimax.io/ est la vitrine occidentale du MiniMax chinois.</p>
<p><b>20-01-2026</b></p>
<p>Petite annecdote symptomatique de cette folie agentique dans laquelle nous baignons. Ce matin je veux r√©cup√©rer une image par seconde d'une vid√©o pour un projet. Je sais que je l'ai d√©j√† fait, que c'√©tait facile, et que j'avais probablement utilis√© ffmpeg. Mais je ne me souviens pas de la m√©thode. Je fais des recherches dans mes tablettes, rien. Je me dis, demandons √† Claude, il va me chier un truc en deux secondes.</p>
<p>Je prompte Claude et par curiosit√© ne mentionne pas ffmpeg ("Write a Python script that will do a video frame freeze every second"). Claude me pond un script d'une soixantaine de lignes faisant appel √† une biblioth√®que nomm√©e <code>opencv-python</code>. Le script ne fonctionne pas, je demande √† pi de le r√©parer, rien √† faire. Je lui demande alors de "refactor" le code en utilisant ffmpeg, pi s'ex√©cute puis v√©rifie dans la foul√©e si ffmpeg est install√© sur le syst√®me (WSL), et comme ce n'est pas le cas, s'arr√™te.</p>
<p>Par paresse, je demande alors √† pi d'installer ffmpeg. Pi essaie, mais est emp√™ch√© par l'absence de mot de passe "sudo". Avant que j'aie le temps de dire ouf, il a trouv√© une solution alternative : il t√©l√©charge une version "statique" de ffmpeg ("Let me try downloading a static ffmpeg binary that doesn't require installation"), configure le script en cons√©quence et √ßa fonctionne : je r√©cup√®re une image par seconde de vid√©o.</p>
<p>Ce soir je v√©rifie. Des recherches plus pouss√©es r√©v√®lent que Claude il y a un an m'avait donn√© une solution mille fois plus √©l√©gante sous la forme du oneliner suivant : <code>ffmpeg -i video.mp4 -vf fps=1 slide_%d.jpg</code>. En une quarantaine de caract√®res espaces compris, ffmpeg en mode console r√©glait le probl√®me en une fraction de seconde. Le prompt √©tait simplement : "I want to extract slides from a YouTube conference video. What are my options?".</p>
<p>Moralit√© #1 : on le savait d√©j√† mais oui, le prompt conditionne √©norm√©ment la r√©ponse. Le prompt de ce matin manquait de clart√© tout en √©tant trop directif. Le prompt d'il y a un an √©tait plus neutre, plus ouvert et plus pr√©cis.</p>
<p>Moralit√© #2 : √©vitons de lancer certaines commandes non-essentielles au sein d'un outil d'agentic coding. Leur r√©sultat est en effet ajout√© au contexte de la conversation et cela peut engendrer une consommation parfois inutile de tokens. Mieux vaut ex√©cuter ces commandes dans autre instance de terminal. Et s'il faut absolument que le LLM "voie" le r√©sultat, pr√©f√©rons le copier-coller "manuel" d'un terminal √† l'autre.</p>
<p><b>19-01-2026</b></p>
<p>"Many of us got hit by the agent coding addiction. It feels good, we barely sleep, we build amazing things. Every once in a while that interaction involves other humans, and all of a sudden we get a reality check that maybe we overdid it. The most obvious example of this is the massive degradation of quality of issue reports and pull requests. As a maintainer many PRs now look like an insult to one‚Äôs time, but when one pushes back, the other person does not see what they did wrong. They thought they helped and contributed and get agitated when you close it down."<br>-- Armin Ronacher - <a href="https://lucumr.pocoo.org/2026/1/18/agent-psychosis/">Agent Psychosis: Are We Going Insane?</a></p>
<p>------</p>
<p>J'essaie de faire du web scraping avec pi en utilisant une "skill" (browser-tools) cr√©√©e par l'auteur de l'outil. La skill en question s'appuie sur le <b>Chrome DevTools Protocol</b> pour fonctionner. Je me rends bient√¥t compte qu'il est possible d'installer Chrome ou Chromium sur WSL, et qu'ils peuvent fonctionner en mode "headless". On peut alors lancer des commandes du genre :</p>
<p><pre><code># Basic headless mode
chromium-browser --headless --disable-gpu --dump-dom https://example.com</code></pre></p>
<p><pre><code># Take a screenshot
chromium-browser --headless --disable-gpu --screenshot=/tmp/screenshot.png https://example.com</code></pre></p>
<p><pre><code># Generate a PDF
chromium-browser --headless --disable-gpu --print-to-pdf=/tmp/output.pdf https://example.com</code></pre></p>
<p>Lorsque je fais un test, big-pickle m'indique que le script <code>browser-start.js</code> n'est pas compatible avec Linux car il contient du code sp√©cifique √† MacOS. Je demande √† l'outil de r√©parer le probl√®me, ce qu'il fait. Ensuite la skill fonctionne correctement et r√©cup√®re les infos demand√©es. Enfin, je demande √† pi/big-pickle d'√©crire une "GitHub Issue" signalant le probl√®me au d√©veloppeur. Par pr√©caution je consulte les Pull Requests en cours et constate que quelqu'un a d√©j√† signal√© le probl√®me et propos√© une solution. Je n'ai donc rien √† faire mais suis content de constater que mon diagnostic √©tait le bon.</p>
<p><b>18-01-2026</b></p>
<p>Mon outil <code>voice2prompt</code> fonctionne √† nouveau. La deuxi√®me partie de la session de "refactoring" s'est bien termin√©e. Le mod√®le principalement utilis√© √©tait <code>google-antigravity/gemini-3-pro-high</code>. Une fois le free tier √©puis√©, je suis pass√© √† <code>opencode/glm-4.7-free</code>, puis √† <code>opencode/big-pickle</code>. Tous les deux m'ont laiss√© tomb√© apr√®s quelques minutes, mais l'essentiel du travail avait √©t√© fait et l'outil √©tait fonctionnel. J'ai supprim√© les fichiers temporaires et ai fait un <code>/init</code> avec Claude Code pour recr√©er un fichier CLAUDE.md "propre" et mettre √† jour le fichier README.md de GitHub (que j'ai modifi√© √† la main ensuite).</p>
<p>L'outil se comporte maintenant comme les autres outils du m√™me genre. Un mod√®le de Whisper est automatiquement t√©l√©charg√© si pas d√©j√† pr√©sent sur la machine. On enregistre sa voix en restant appuy√© sur une touche du clavier et quand on l√¢che la touche, la transcription s'effectue et le texte est coll√© automatiquement l√† o√π se trouve le curseur. L'ensemble est constitu√© de 3 fichiers : deux scripts en Python et un script en Bash.</p>
<p>Mise en ligne de <a href="https://github.com/sebington/voice2prompt">la nouvelle version</a> sur GitHub. Claude Code me propose de g√©rer les commandes git √† ma place mais je pr√©f√®re m'astreindre √† les passer manuellement pour mieux comprendre. Finalement on peut s'en sortir avec un nombre r√©duit de commandes. Voici celles que j'ai utilis√©es jusqu'√† maintenant (en ordre alphab√©tique) :</p>
<p><pre><code>git add .
git clone
git commit
git config
git diff
git init
git log --oneline
git push
git remote add origin
git rm --cached fichier.md
git status</code></pre></p>
<p><b>17-01-2026</b></p>
<p>Sortie il y a 3 jours de <a href="https://kyutai.org/blog/2026-01-13-pocket-tts">Pocket TTS</a>, un <a href="https://huggingface.co/kyutai/pocket-tts">mod√®le</a> text-to-speech de seulement 236_Mo qui peut donc fonctionner en local et semble donner de tr√®s bons r√©sultats. <a href="https://kyutai.org/">Kyutai</a> est un laboratoire fran√ßais de recherche ouverte en intelligence artificielle. En consultant leur site, je constate qu'ils ont √©galement d√©velopp√© un mod√®le de <a href="https://huggingface.co/kyutai/stt-1b-en_fr">speech-to-text</a> comparable √† Whisper et surtout <a href="https://unmute.sh/">Unmute.sh</a>, un outil qui permet de dialoguer en direct avec un mod√®le sp√©cialement prompt√© pour adopter un comportement ou incarner un personnage d√©fini. Je discute un moment avec le G√©n√©ral de Gaulle, puis avec un personnage f√©minin qui parle fran√ßais avec un d√©licieux accent anglais et m'explique comment Unmute a √©t√© con√ßu (apparemment avec un mod√®le de Mistral finetun√©). En tout cas l'interface est superbe et √ßa marche du feu de Dieu.</p>
<p>Le probl√®me des MCP qui bouffent le contexte de Claude Code aurait √©t√© r√©solu par Anthropic, si l'on en croit <a href="https://www.youtube.com/watch?v=1RpGVqgqLaE">cette vid√©o</a>.</p>
<p><b>16-01-2026</b></p>
<p>Sortie de <a href="https://ollama.com/library/translategemma">translategemma</a>. J'installe le mod√®le 4B sur Ollama et effectue quelques tests rapides. Le r√©sultat semble assez correct pour un mod√®le de cette taille. Il faudrait comparer avec d'autres mod√®les g√©n√©ralistes de taille similaire, puisque, sauf erreur, la traduction est une "capacit√© √©mergente" de presque tous les LLM.</p>
<p>Je passe un certain temps √† passer en revue les distributions Linux qui peuvent se charger enti√®rement en m√©moire (fonctionnalit√© "TO_RAM"). C'est une obsession que j'ai depuis des ann√©es et de temps en temps je fais une petite crise/rechute. J'en retrouve pas mal (Slax, SliTaz, Porteus...) d√©j√† utilis√©es maintes fois mais rien de tr√®s nouveau ou d√©cisif. Ce chargement en m√©moire est √©galement possible avec certaines "grosses" distributions. Je crois que l'ISO de CachyOS se charge enti√®rement en m√©moire quand on boote √† partir d'une cl√© USB. J'aime bien cette id√©e d'un OS volatile et l√©ger, m√™me si cela implique aussi de grosses contraintes. A suivre.</p>
<p>Ce soir je tente l'impl√©mentation du plan de Claude Code pour voice2prompt. En gros il s'agit de rendre l'installation et l'utilisation plus simples et fluides. Je d√©marre avec Gemini 3 et poursuit avec Big Pickle. Session tr√®s difficile car Wayland est vraiment d√©licat √† g√©rer au niveau des autorisations. Par moments pi explore les entrailles d'Ubuntu pour tenter de r√©soudre les probl√®mes. L'appli se lance et r√©pond aux √©v√®nements clavier mais impossible de capter ma voix et encore moins de la transcrire. Il faut que je regarde le code et que j'isole les diff√©rents modules pour savoir o√π √ßa coince. En mati√®re de d√©veloppement informatique, je crois que l'humain a encore de beaux jours devant lui.</p>
<p><b>15-01-2026</b></p>
<p>Je fais une session in√©dite et excitante sur pi avec claude-sonnet-4-5. Je lui demande de se connecter √† l'API de models.dev (https://models.dev/api.json) pour faire quelques analyses statistiques. Pi se d√©brouille avec Curl pour r√©cup√©rer le fichier JSON et l'aventure commence. Je pose mes questions sur les donn√©es directement dans pi, qui utilise les outils ad√©quats pour me r√©pondre (au d√©but du Bash tr√®s sophistiqu√©). Je lui conseille d'utiliser uv, Python et Pandas pour de meilleurs r√©sultats. Il faut √™tre attentif √† ce qu'il trouve quand il filtre les donn√©es mais globalement √ßa marche plut√¥t bien. Je lui pose des questions de plus en plus complexes qui demandent une analyse crois√©e et il fait du super boulot. Vraiment g√©nial comme outil car tellement polyvalent.</p>
<p>Je refais refais le m√™me exercice avec Mistral-Vibe et Gemini CLI, en utilisant exactement les m√™mes prompts (au moins au d√©but). Tous les deux s'en sortent honorablement, Vibe √©tant beaucoup plus r√©actif que Gemini. Olivier fait la m√™me chose avec Claude Code. A premi√®re vue les r√©sultats semblent diverger et demanderaient une analyse comparative plus pouss√©e. A noter que Claude Code et Gemini CLI "nettoient derri√®re eux" et suppriment la plupart des fichiers temporaires utilis√©s pour g√©n√©rer les statistiques, tandis que pi et Vibe laissent tout le bazar dans leurs r√©pertoires de travail, qui √† la fin de la session font 109 et 114 Mo respectivement.</p>
<p>------</p>
<p>Ce soir je visionne la <a href="https://www.youtube.com/watch?v=WNx-s-RxVxk">derni√®re vid√©o</a> de Matt Pocock, qui pour la premi√®re fois me d√©√ßoit car elle promet sans tenir, ce qui est typique du comportement de certains youtubeurs avides de clics et d'auto-promotion. Toute une vid√©o pour deux malheureuses lignes qui sont sens√©es r√©volutionner la mani√®re avec laquelle Claude Code va planifier votre futur projet. Du vent.</p>
<p>Bon, la vid√©o de Pocock m'a tout de m√™me donn√© envie de tester le "mode plan" de Claude Code. Je l'utilise pour planifier la suite des am√©liorations pr√©vues pour mon assistant vocal, ce qui me revient √† $0.86. A la fin de la phase de planification Claude me propose de visualiser le plan dans VSCode en faisant Ctrl + G (ou qq ch comme √ßa) et quand je reviens dans le terminal, Claude Code bugue grave et "freeze" au point que je dois faire un double Ctrl + C sauvage pour sortir.</p>
<p><b>13-01-2026</b></p>
<p>Bien avanc√© sur mon assistant vocal. Ajout d'ic√¥nes dans la barre des t√¢ches en haut. Pas mal d'essais avant d'obtenir quelque chose qui fonctionne √† peu pr√®s. Un carr√© vert s'affiche quand le processus tourne en arri√®re plan et un carr√© rouge clignotant remplace le carr√© vert quand √ßa enregistre et √ßa transcrit. C'est le script en Python qui construit l'ic√¥ne √† la vol√©e au lieu d'utiliser une image par exemple. Utilisation de Git pour r√©cup√©rer une version ant√©rieure de mon code si le LLM s'emballe.</p>
<p>J'ai fait appel √† GLM 4.6. Mod√®le tr√®s r√©actif et pertinent, vraiment un plaisir √† utiliser. Pour l'instant mon outil reste un peu d√©licat √† manipuler mais il fait le taf. Je refais une session avec Gemini qui en remet une couche. Il trouve des am√©liorations possibles au niveau du "Clipboard Manager" qui est maintenant compatible avec Wayland ET X11. Je fais un git push sur GitHub, mais n'ai plus la force de m'occuper du fichier README.md ce soir. C'est de la folie ces outils.üòµ</p>
<p><b>11-01-2026</b></p>
<p>Journ√©e maussadissime comme seul(e) Brest sait en produire üåßÔ∏èüåßÔ∏èüåßÔ∏è</p>
<p>J'organise les dossiers des skills que je compte utiliser avec pi. Elles proviennent de 3 sources : <a href="https://github.com/badlogic/pi-skills">Mario Zechner</a>, <a href="https://github.com/mitsuhiko/agent-stuff">Armin Ronacher</a> et <a href="https://github.com/anthropics/skills">Claude</a>. Puis je demande √† pi de cr√©er une skill √† partir d'un script en Python qui fait une recherche sur le web avec DuckDuckGo et en fait un petit r√©sum√©. Pi s'ex√©cute avec Big Pickle et Minimax et me cr√©e une nouvelle skill : <code>web-search-summarize</code>, que je teste et qui fonctionne.</p>
<p>Cet apr√®s-midi, visio avec Olivier o√π nous √©changeons pendant 2h sur nos projets respectifs et l'√©volution des outils de codage agentique.ü§ì</p>
<p>Ce soir comme pr√©vu je modifie mon script "voice assistant daemon/groq" pour le faire fonctionner en local avec <code>pywhispercpp</code>. Minimax s'en occupe en deux temps trois mouvements. Je teste avec un mod√®le multilingue et non anglais-seul : √† ma surprise quand je parle en fran√ßais Whisper traduit ce que je dis et me livre la transcription en anglais. Je fais plusieurs essais, rien √† faire. C'est peut-√™tre le mod√®le qui a √©t√© mal √©tiquet√©. On verra √ßa plus tard.</p>
<p><b>10-01-2026</b></p>
<p>Je teste pi avec le mod√®le <code>opencode/big-pickle</code>. J'essaie de modifier le script de d√©part de mon assistant vocal, qui fonctionnait avec <a href="https://github.com/ggml-org/whisper.cpp">whisper.cpp</a> et qui avait n√©cessit√© de compiler whisper.cpp depuis la source, ce qui rendait le projet tr√®s fouilli. Je propose √† pi d'utiliser <a href="https://pypi.org/project/pywhispercpp/">pywhispercpp</a> √† la place.</p>
<p>Big-pickle d√©marre au quart de tour et s'attaque au probl√®me sans tra√Æner. Je suis impressionn√©. Suite √† une erreur, il propose d'utiliser <code>arecord</code> un outil natif de Python √† la place de <code>pyaudio</code>.</p>
<p>"I've successfully converted the script to use arecord (Linux command-line tool) instead of pyaudio/sounddevice. This eliminates the PortAudio dependency issue."</p>
<p>Je teste le script avec <a href="https://huggingface.co/ggerganov/whisper.cpp/blob/main/ggml-base.en.bin">ce mod√®le de whisper</a> de 148 Mo et √ßa marche nickel, avec une vitesse comparable √† celle de Groq.</p>
<p>Note : Olivier d√©couvre que derri√®re l'appellation "Big Pickle" se cache <a href="https://github.com/anomalyco/opencode/issues/4276">le mod√®le GLM 4.6</a> de z.ai.</p>
<p><b>09-01-2026</b></p>
<p>J'aimerais bien utiliser l'API payante de Groq pour la rapidit√© de l'inf√©rence, mais je trouve <a href="https://console.groq.com/docs/models">leur s√©lection de mod√®les</a> assez bizarre, assez pauvre en termes d'agentic coding, et √† chaque fois √ßa me dissuade de sauter le pas. J'en parle √† ChatGPT, qui me r√©pond que le produit phare de Groq est maintenant <b>Compound</b>, un syst√®me d'IA agentique sp√©cialement con√ßu qui int√®gre l'utilisation d'outils (recherche sur le Web, ex√©cution de code, automatisation du navigateur, etc.) dans un seul appel API. Pas s√ªr de comprendre vraiment comment √ßa marche. En plus, le tarif pour 1M de tokens de Compound ou Compound Mini est bizarrement absent de leur tableau.</p>
<p><b>08-01-2026</b></p>
<p>Remarques sur les diff√©rents outils d'agentic coding que je teste :</p>
<p>-<b>OpenCode</b> : ne montre pas assez ce qu'il fait quand il travaille. C'est assez frustrant de ne pas savoir o√π il en est. En revanche OpenCode propose toujours quelques mod√®les "gratuits" (Big Pickle, MiniMax M2.1 ou GLM 4.7).<br>-<b>Mistral Vibe</b> : marche bien, assez r√©actif une fois qu'on l'a mis en mode YOLO sinon il s'arr√™te toutes les deux secondes. Documentation tr√®s minimale et pas √† jour. Bugs. L'acc√®s au free tier peut √™tre interrompu √† tout moment.<br>-<b>Gemini CLI</b> : Gratuit mais parfois assez peu r√©actif, voire lent. Interface et affichage au top je trouve. Bon pour analyser et d√©bugguer. Fiable. Fais parfois des merveilles.<br>- <b>Pi</b> : tr√®s fluide, rapide et facile √† prendre en main. Peut entre autre se connecter aux mod√®les free tier d'Antigravity. Possibilit√© de g√©n√©rer un fichier html de la session avec une interface de recherche tr√®s bien pens√©e pour retrouver le contenu de la session dans ses moindres d√©tails. Les commandes <code>/tree</code> et <code>/branch</code> sont tr√®s utiles pour revenir en arri√®re en cas de probl√®me.<br>- <b>Claude Code</b> : pas grand chose √† lui reprocher sauf peut-√™tre son c√¥t√© un peu usine √† gaz (tellement de fonctionnalit√©s qu'on s'y perd). Mais c'est probablement le meilleur outil en ce moment.</p>
<p>------</p>
<p>Depuis hier je retravaille sur l'assistant vocal commenc√© le 4. Travail acharn√© et approches multiples : whisper.cpp, vosk, whisper/groq. Script qui se limite √† une seule instance de terminal puis processus (daemon) qui tourne en arri√®re plan (incontournable). Script en bash d√©clenchable avec raccourci, demandes de conseils √† ChatGPT pour l'approche et la planification √† cause de Wayland, multiples outils CLI (ils y sont tous pass√©s). C'est Gemini CLI qui finit par tout d√©coincer.</p>
<p>L'outil lance un processus via un script en Python qui attend que l'utilisateur tape une touche sur le clavier. La premi√®re frappe de la touche d√©clenche l'enregistrement de la voix et la seconde arr√™te l'enregistrement et envoie les donn√©es audio √† Whisper via l'API de Groq. Groq renvoie la transcription, qui est stock√©e dans le "presse-papier" de la machine. L'utilisateur fait Ctrl+V pour coller le texte √† l'endroit voulu. J'ai essay√© de faire en sorte que le collage se fasse automatiquement mais pour l'instant rien √† faire, Wayland est verrouill√© de tous les c√¥t√©s.</p>
<p>Ce genre de projet est difficile √† d√©bugguer car il fait appel √† des √©l√©ments multim√©dia. Quand √ßa ne marche pas il faut d√©crire pr√©cis√©ment ce qui ne va pas. L'outil ne peux pas se contenter de lancer le script et d'analyser la sortie puisqu'il y a un √©l√©ment humain au beau milieu : la voix de l'utilisateur. Cela demande de prompter de fa√ßon ultra pr√©cise, mais √ßa finit par payer.</p>
<p><b>06-01-2026</b></p>
<p>Je continue ma qu√™te du Graal sous la forme d'un outil performant de correction-r√©vision par IA. J'installe les <a href="https://github.com/anthropics/skills">Claude Skills</a> sur pi et demande √† Gemini-3-pro-high et √† Sonnet-4-5 de r√©viser un document Word en utilisant la skill "docx". J'adopte donc l'approche : "ne fabrique pas un outil, tu es l'outil".</p>
<p>Prompt : "Use the docx skill to perform a professional revision, copy-editing and proofreading of @file_original.docx. Output a .docx file in track change."</p>
<p>Gemini bosse pendant 5 bonnes minutes et produit un document en mode suivi des modifications assez correct, sans √™tre parfait. Je r√©it√®re avec Sonnet-4-5 mais grosse d√©ception, l'op√©ration se solde par un √©chec alors que je pensais que Sonnet ferait mieux que Gemini. Le document n'a subi quasiment aucune modification. La qu√™te continue mais je sens que je tourne en rond et que le fait de vouloir absolument un document Word en sortie est tr√®s limitant.</p>
<p>------</p>
<p>Github a ajout√© un chatbot sur sa page d'accueil (GPT5-mini par d√©faut). Mes premi√®res questions concernant des statistiques portant sur des d√©p√¥ts Github ne rencontrent pas un franc succ√®s. Le chatbot me r√©pond qu'il n'a pas acc√®s √† certaines donn√©es. Ma troisi√®me tentative porte sur un outil de voice-to-text et g√©n√®re une r√©ponse int√©ressante, qui dispara√Æt au bout de quelques minutes. Pas encore tr√®s au point leur truc.</p>
<p><b>05-01-2026</b></p>
<p>Je continue d'utiliser <a href="https://shittycodingagent.ai/">pi</a> pour coder. Les versions de pi sur Github se succ√®dent et l'outil me semble performant et fluide. Impression d'√™tre √† la pointe de ce qui se fait en agentic coding, ce qui est grisant. Je m'en sert pour d√©velopper une appli sur Linux qui g√®re mes centaines de fichiers GPX. Gemini la d√©crit comme suit : "A desktop GUI application built with <code>PyQt6</code> for managing, viewing, and analyzing GPX (GPS Exchange Format) files." Rien de r√©volutionnaire comme appli mais une fois la base construite et √† peu pr√®s fonctionnelle, je peux demander toutes les fonctionnalit√©s ou am√©liorations que je souhaite et les teste dans la foul√©e. Le kiff int√©gral.</p>
<p>Hugging Face met en ligne un autre <a href="https://www.youtube.com/watch?v=dSGS6-iGhyo">tuto</a> int√©ressant sur <a href="https://huggingface.co/docs/smolagents/index">smolagents</a>, une de leurs biblioth√®ques Python.</p>
<p>Et pendant ce temps-l√†, <a href="https://z.ai/blog/glm-4.7">les mod√®les chinois</a> ne cessent de <a href="https://www.minimax.io/news/minimax-m21">progresser</a>.</p>
<p><b>04-01-2026</b></p>
<p>Je demande √† pi de me fabriquer un assistant vocal pour dicter mes prompts dans le terminal d'Ubuntu. Je lui demande d'utiliser uv et <a href="https://github.com/ggml-org/whisper.cpp">whisper.cpp</a>. L'op√©ration prend une petite heure et est rendue compliqu√©e par le fait que whisper.cpp ne propose plus de fichiers pr√©compil√©s pour Linux. Il faut utiliser CMake et compiler soi-m√™me. J'utilise exclusivement claude-sonnet-4-5 et grille du token √† la pelle.</p>
<p><pre><code>‚Üë154 ‚Üì32k R2.0M W99k $1.449 26.7%/200k (auto)</code></pre></p>
<p>A la fin du processus j'ai un outil (v1) qui fonctionne et qui se d√©clenche avec une touche du clavier. Seul souci (de taille) √† r√©gler : comment faire pour que l'outil fonctionne dans une autre instance de terminal que celle qui lui est d√©di√©e ? J'y travaille quand je suis arr√™t√© par Claude qui me notifie une erreur 429 "rate_limit_error". C'est assez barbant. Chose √©trange, si je passe de pi √† Claude Code, tout fonctionne et je n'ai plus l'erreur 429.</p>
<p>La suite est moins glorieuse. J'utilise tour √† tour Claude Code, OpenCode et √† nouveau pi sans parvenir √† r√©gler le probl√®me. L'interruption de pi/sonnet alors qu'il semblait en si bon chemin semble avoir √©t√© fatale. Les combinaisons de touches ("hotkeys") ne fonctionnent pas, Sonnet incrimine Wayland et se perd dans des v√©rifications sans fin. Je vais devoir me contenter de la v1 pour l'instant.</p>
<p>Finalement Gemini me donne l'explication : c'est bien Wayland le coupable, Sonnet avait raison. "Wayland prioritizes security and isolation, which presents significant hurdles for "System Utility" apps. One of the key obstacles on Wayland is Input Isolation. Applications cannot "spy" on other windows. There is no simple API to detect a global hotkey press unless the app is currently focused." Ce n'est pas la premi√®re fois que j'ai des soucis √† cause de Wayland.</p>
<p><b>03-01-2026</b></p>
<p>Je recommande chaudement la lecture de <a href="https://simonwillison.net/2025/Dec/31/the-year-in-llms/">The year in LLMs</a> de Simon Willison.</p>
<p>Un truc qui passionne les foules est la g√©n√©ration d'images. Quand ChatGPT a ajout√© cette fonctionnalit√© en mars dernier, elle a suscit√© la cr√©ation de 100 millions de comptes en une semaine, avec un pic d'un million atteint en seulement une heure. Un peu triste quand m√™me.</p>
<p>Idem avec l'√©norme succ√®s de Nano Banana chez Google, sorti en ao√ªt. Ce qui me rassure presque, c'est qu'Anthropic reste en retrait de ces "frivolit√©s" (qui n'en sont pas vraiment, je le conc√®de) pour se concentrer sur des produits fiables pour d√©veloppeurs. Leur victoire √† eux, c'est Claude Code et pas des images de petits chats en guimauve ronronnant sur Mars.</p>
<p>"Google‚Äôs biggest advantage lies under the hood. Almost every other AI lab trains with NVIDIA GPUs, which are sold at a margin that props up NVIDIA‚Äôs multi-trillion dollar valuation. Google use their own in-house hardware, TPUs, which they‚Äôve demonstrated this year work exceptionally well for both training and inference of their models."</p>
<p>------</p>
<p>Je visionne <a href="https://www.youtube.com/watch?v=I6Bgu2QV1D4">ce tutoriel</a> mis en ligne il y a quelques jours par <a href="https://huggingface.co/">Hugging Face</a>, qui va clairement dans le sens d'une d√©mocratisation du travail collaboratif sur Github. Je me dis qu'une telle vid√©o appelant quasiment tout un chacun √† r√©parer ou optimiser du code aurait √©t√© inconcevable il y a moins d'un an. C'est dire si les choses sont en train de changer.</p>
<p><b>02-01-2026</b></p>
<p>Je lis l'int√©gralit√© du <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/">billet de blog</a> de Mario Zechner dont je parlais hier. Il explique sa philosophie minimaliste (j'adore) et la mani√®re de d√©velopper <a href="https://shittycodingagent.ai/">son outil d'agentic coding customis√©</a>.</p>
<p>Malgr√© certains passages tr√®s techniques, ce billet est vraiment passionnant et me permet de mieux comprendre le fonctionnement, les avantages et les limites des outils comme Claude Code.</p>
<p>Mario insiste sur l'inutile complexit√© de certains de ces outils, qui se fait au d√©triment de la performance.</p>
<p>"Over the past few months, Claude Code has turned into a spaceship with 80% of functionality I have no use for. The system prompt and tools also <a href="https://mariozechner.at/posts/2025-08-03-cchistory/">change on every release</a>, which breaks my workflows and changes model behavior. I hate that."</p>
<p>Il explique que le prompt "syst√®me" de pi est tr√®s court. Tout est fait pour ne pas surcharger inutilement le contexte du mod√®le.</p>
<p>"It turns out that all the frontier models have been RL-trained up the wazoo, so they inherently understand what a coding agent is. There does not appear to be a need for 10,000 tokens of system prompt. pi's system prompt and tool definitions together come in below 1000 tokens."</p>
<p>Concernant la s√©curit√© et l'utilisation du mode "YOLO", il explique que la plupart des soit-disant mesures de s√©curit√© des autres outils populaires sont de la poudre au yeux ("security theater").</p>
<p>"pi runs in full YOLO mode and assumes you know what you're doing. It has unrestricted access to your filesystem and can execute any command without permission checks or safety rails. No permission prompts for file operations or commands. No pre-checking of bash commands by Haiku for malicious content. Full filesystem access. Can execute any command with your user privileges.</p>
<p>If you look at the security measures in other coding agents, they're mostly security theater. As soon as your agent can write code and run code, it's pretty much game over."</p>
<p>Concernant d'autres fonctionalit√©s comme les MCP, les processus lanc√©s en arri√®re plan ("background bash") ou les "sub-agents", Zechner rejette tout en bloc par souci de simplicit√© et d'efficacit√©.</p>
<p>"Popular MCP servers like Playwright MCP (21 tools, 13.7k tokens) or Chrome DevTools MCP (26 tools, 18k tokens) dump their entire tool descriptions into your context on every session. That's 7-9% of your context window gone before you even start working. Many of these tools you'll never use in a given session."</p>
<p>Je continue d'apprivoiser pi. Je le fais fonctionner en local avec Ollama et Qwen2.5:7b. C'est bien s√ªr hyper lent (CPU-only) mais √ßa donne de bons r√©sultats. J'installe les pi-skills propos√©es par Mario <a href="https://github.com/badlogic/pi-skills">sur son Github</a>. Au d√©but √ßa ne marche pas, probl√®me de configuration de npm que pi r√©soud en quelques secondes, puisqu'il a acc√®s √† toute la machine. Super pratique. Je teste une skill (transcription d'une vid√©o YouTube de 5 minutes). √áa fonctionne du tonnerre et fait appel √† Groq via un script en Bash. H√¢te de continuer !</p>
<p><b>01-01-2026</b></p>
<p>Journ√©e riche en d√©couvertes. Encore Armin Ronacher. Hier je regarde <a href="https://www.youtube.com/watch?v=ClTvgE-sFls">sa derni√®re vid√©o</a> (√† laquelle je ne comprends pas grand chose sur le moment) mais constate cependant que sa version de Claude Code affiche des infos en temps r√©el sur la session en cours (avec notamment le co√ªt en dollars, la taille du contexte et le mod√®le utilis√©) en bas de son terminal. Exemple :</p>
<p><pre><code>‚Üë94 ‚Üì22k R783k W164k $1.178 22.2%/200k (auto) claude-sonnet-4-5</code></pre></p>
<p>Je demande √† Armin s'il utilise un outil sp√©cifique pour cela et il me r√©pond que ce qu'il utilise dans la vid√©o n'est pas Claude Code comme je le croyais mais un autre outil qui s'appelle <a href="https://shittycodingagent.ai/">pi</a>. Je me dis que si Ronacher utilise ce truc, c'est que √ßa doit valoir le coup et je tente l'exp√©rience.</p>
<p>Je l'installe et lance une session avec Sonnet-4-5. C'est comme Claude Code en plus direct, genre "on arr√™te de tourner autour du pot". Si on essaie de r√©sumer : outil minimaliste, avec une philosophie assum√©e. Quatre "tools" de base : read, write, edit, bash. D√©tecte les cl√©s API pr√©sentes sur la machine et affiche les mod√®les correspondants. Mode YOLO par d√©faut. L'outil peut "sortir" du r√©pertoire qui lui a √©t√© assign√© et aller voir autour. C'est d'ailleurs ce qu'il fait d√®s le d√©part en "lisant" <code>Downloads/AGENTS.md</code>, un fichier qui ne lui √©tait pas destin√©.</p>
<p>Autre fonctionnalit√© saillante, un mode d'auto-compactage de la fen√™tre contextuelle activ√© par d√©faut. C'est super pratique, on n'est pas oblig√© de surveiller le contexte sans arr√™t.</p>
<p>Ce qui est diff√©rent : par d√©faut pas de MCP (les Skills sont encourag√©es √† la place), pas de "sub-agents", pas de "plan mode", pas de listes de t√¢ches (built-in to-dos). Explications sur <a href="https://mariozechner.at/posts/2025-11-30-pi-coding-agent/#toc_11">le blog du d√©veloppeur</a>.</p>
<p>Un truc super pratique : l'outil peut exporter nativement l'<a href="http://bytepacking.free.fr/files/pi-session-2026-01-01T16-54-52-265Z_4d40a505-9b5f-43eb-b7da-eacbe78fd6e1.html">ensemble de la session en HTML</a>, m√™me si elle a √©t√© auto-compact√©e et que le d√©but de la session n'est plus visible dans le terminal. Et le rendu du HTML est magnifique je trouve. Je suis conquis.</p>
<p>Cerise sur le g√¢teau : en cherchant le site de "pi", je tombe sur le blog d'Armin Ronacher, dont j'ignorais l'existence et que je conseille fortement. L'avant dernier billet, <a href="https://lucumr.pocoo.org/2025/12/22/a-year-of-vibes/">A year of vibes</a>, est un r√©capitulatif de cette ann√©e 2025 compl√®tement dingue avec l'<a href="https://sebington.substack.com/p/antigravity">agentic coding</a> arrivant √† maturit√©. Le dernier billet, <a href="https://lucumr.pocoo.org/2025/12/23/advent-of-slop/">Advent of Slop</a>, a √©t√© √©crit par Claude √† la demande de Ronacher (prompt complet et "disclaimer" √† la fin).</p>
<p>Enfin, une partie du blog est consacr√©e √† des <a href="https://dark.ronacher.eu/">sujets plus sombres</a>, avec une esth√©tique (nuages "programm√©s" en perp√©tuels mouvements) tr√®s r√©ussie je trouve. Ce mec a du talent.</p>
<p><b>31-12-2025</b></p>
<p>Hier Karpathy d√©sempar√©. Aujourd'hui Ronacher enthousiaste.</p>
<p>"The puzzle is still there. What‚Äôs gone is the labor. I never enjoyed hitting keys, writing minimal repro cases with little insight, digging through debug logs, or trying to decipher some obscure AWS IAM permission error. That work wasn‚Äôt the puzzle for me. It was just friction, laborious and frustrating. The thinking remains; the hitting of the keys and the frustrating is what‚Äôs been removed."<br>-- <a href="https://lobste.rs/s/x1xqtv/llms_are_not_fun#c_xccjtq">Armin Ronacher</a> - 29 d√©cembre 2025</p>
<p>De mani√®re compl√®tement inattendue, <a href="https://github.com/mistralai/mistral-vibe">Mistral-Vibe</a> fonctionne √† nouveau (gratuitement). Je l'ai juste mis √† jour (v1.3.3). Vibe a tent√© de d√©velopper une appli de gestion de fichier GPX avec <a href="https://www.electronjs.org/">electronjs</a> mais √† chaque tentative de lancement (<code>npm run dev</code>), √ßa √©choue. Vibe a essay√© de d√©buguer plusieurs fois mais sans succ√®s. Je laisse tomber pour l'instant et retourne sur CC.</p>
<p><b>30-12-2025</b></p>
<p>"I've never felt this much behind as a programmer. The profession is being dramatically refactored as the bits contributed by the programmer are increasingly sparse and between. I have a sense that I could be 10X more powerful if I just properly string together what has become available over the last ~year and a failure to claim the boost feels decidedly like skill issue. There's a new programmable layer of abstraction to master (in addition to the usual layers below) involving agents, subagents, their prompts, contexts, memory, modes, permissions, tools, plugins, skills, hooks, MCP, LSP, slash commands, workflows, IDE integrations, and a need to build an all-encompassing mental model for strengths and pitfalls of fundamentally stochastic, fallible, unintelligible and changing entities suddenly intermingled with what used to be good old fashioned engineering. Clearly some powerful alien tool was handed around except it comes with no manual and everyone has to figure out how to hold it and operate it, while the resulting magnitude 9 earthquake is rocking the profession. Roll up your sleeves to not fall behind."<br>-- Andrej Karpathy - 26 d√©cembre 2025</p>
<p><b>29-12-2025</b></p>
<p>D√©couverte sur Ollama.com de <a href="https://ollama.com/library/functiongemma">functiongemma</a>, "a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling". Je t√©l√©charge le mod√®le attir√© par son poids plume (301 Mo) et le teste avec le script propos√© dans la description, qui fonctionne mais ne fait pas grand chose (d√©mo). Je donne le code √† Claude et lui demande de me g√©n√©rer d'autres scripts plus int√©ressants. Claude me propose entre autre un script sens√© servir de calculatrice requ√™table en langage naturel. Le script fonctionne mais on sent les limites de functiongemma, qui √©choue quand on lui demande de calculer l'aire d'un cercle par exemple. Je d√©cide d'essayer d'autres mod√®les compatibles avec le "function calling" (Mistral, Llama3.2 et Qwen2.5). C'est avec Qwen2.5 (7b) que j'obtiens les meilleurs r√©sultats. Rappelons que les calculs sont effectu√©s par le script en Python, et que le mod√®le n'est l√† que pour "orchestrer" les choses.</p>
<p><b>28-12-2025</b></p>
<p>Pour d√©marrer mon appli de gestion de fichiers GPX, je tente une petite exp√©rience. Je lance simultan√©ment 3 outils d'agentic coding (OpenCode avec GML-4.7, Gemini CLI avec Gemini 3 et Claude Code avec Sonnet pour la planification et Haiku pour l'impl√©mentation) dans 3 r√©pertoires diff√©rents. Chaque r√©pertoire contient exactement les m√™mes instructions dans un fichier nomm√© AGENTS.md et quelques fichiers GPX pour tester.</p>
<p>Je demande avant tout aux outils de produire un plan d'action. Quand chaque outil a accompli cette t√¢che de planification, je leur dis de commencer √† impl√©menter l'appli. Au bout d'un quart d'heure Gemini me propose une interface web compl√®tement st√©rile avec de jolis logos mais rien de fonctionnel et OpenCode me montre une liste de fichiers GPX mais est incapable d'en afficher un sur une carte malgr√© mes multiples relances.</p>
<p>Seul Claude Code propose d'embl√©e une appli basique mais fonctionnelle, avec la liste des fichiers GPX √† gauche, une carte (OpenStreetMap) au centre sur laquelle on visualise le premier fichier GPX de la liste et une colonne √† droite avec le d√©tail de chaque fichier. Je continue d'interagir avec Claude Code, qui non seulement r√©pond avec pertinence √† chacune de mes demandes, mais prend le temps de m'expliquer pourquoi l'impl√©mentation de certaines fonctionnalit√©s est conditionn√©e √† certains imp√©ratifs comme par ex. le type de navigateur utilis√©. J‚Äôapprends donc des choses tout en faisant et cela renforce le c√¥t√© agr√©able de la session.</p>
<p>Il reste beaucoup de travail avant d'obtenir ce que je souhaite. Une appli comme celle-ci demande de nombreux ajustements tant sur le fond que la forme. Le fait que l'appli soit fonctionnelle d√®s le d√©but de la session permet de tester chaque changement en temps r√©el. Je fais la navette entre la fen√™tre du navigateur et celle du terminal. Le code en TypeScript, un langage que je ne ma√Ætrise pas, reste abstrait et secondaire. Pour pallier √† ce probl√®me et visualiser le code en direct, on peut utiliser VSCode avec le Claude Code plugin ou bien Antigravity. Mais attention, les possibilit√©s sont infinies et il n'y a toujours que 24 heures dans une journ√©e.</p>
<p><b>27-12-2025</b></p>
<p>Je regarde √† nouveau <a href="https://www.youtube.com/watch?v=7xTGNNLPyMI">Deep Dive into LLMs like ChatGPT</a> d'Andrej Karpathy, une vid√©o qui devrait √™tre traduite et rendue obligatoire au lyc√©e et √† l'universit√© tellement elle explique bien la conception et le fonctionnement des grands mod√®les de langage (LLM) comme ChatGPT. Je t√©l√©charge les 3400+ commentaires de la vid√©o avec youtube-comment-downloader et obtient un √©norme fichier JSON que (sans trop y croire) je donne tel quel √† Gemini avec le prompt suivant :</p>
<p><span class="code-box">Can you extract interesting and constructive comments out of this json file, people asking questions for example, and thus discarding comments that just say 'thank you' or joke?</span></p>
<p>Voici <a href="http://bytepacking.free.fr/files/7xTGNNLPyMI_gemini_analysis.md">le r√©sultat</a>.<br>------</p>
<p>J'essaie de mettre en application <a href="https://simonwillison.net/2025/Nov/6/async-code-research/">cette excellente id√©e</a> de Simon Willison. Je d√©cide d'utiliser Google Jules (que j'ai d√©j√† utilis√© une fois auparavant), avec le prompt suivant.</p>
<p><span class="code-box">I plan to develop a web app that will be able to parse, display and run stats on hundreds of GPX files of walking/running and mostly cycling activities. Please advise on the features this app could propose and the technology used (e.g. JavaScript, TypeScript, npm, etc.) by exploring various codebases and draft a report.md file.</span></p>
<p>Tout se passe relativement bien malgr√© quelques maladresses de ma part. Jules finit par g√©n√©rer 2 fichiers et produit un Pull Request dans Github. Voici <a href="https://github.com/sebington/research/blob/main/gpx_app_research/report.md">le r√©sultat</a> du processus.<br>------</p>
<p>David Louapre fait √† nouveau <a href="https://www.arte.tv/fr/videos/121344-002-A/scope-dans-les-coulisses-de-l-ordinateur-quantique/">parler de lui</a>. D√©cid√©ment !</p>
<p><b>26-12-2025</b></p>
<p>Je regarde la derni√®re vid√©o d'Alex Finn sur les Claude Skills et une fois de plus je suis frapp√© par sa mani√®re de parler. Tout (intonation, timbre, rythme, diction, d√©bit...) fait penser √† la mani√®re de parler de Trump. Le fait-il expr√®s ? Pour en avoir le c≈ìur net je me rends sur <a href="https://ytcomment.kmcat.uk/">https://ytcomment.kmcat.uk/</a>, colle le nom de la cha√Æne (https://www.youtube.com/@AlexFinnOfficial/) et fais une recherche avec "Trump" comme seul mot cl√©. Avalanche de r√©sultats : 34 internautes se font exactement la m√™me remarque. Me voil√† rassur√©.</p>
<p><b>22-12-2025</b></p>
<p>Un <a href="https://ngrok.com/blog/prompt-caching/">article tr√®s int√©ressant</a> sur le "prompt caching", une technique qui r√©duit le nombre de tokens consomm√©s quand on utilise un outil comme Claude Code ou Gemini CLI. Ce n'est pas du tout ce √† quoi je pensais ! Mention sp√©ciale aux liens en bas de la page.</p>
<p>David Louapre, l'animateur de la cha√Æne <a href="https://www.youtube.com/@ScienceEtonnante">ScienceEtonnante</a> travaille maintenant pour Hugging Face. Il d√©bute fort avec une <a href="https://www.youtube.com/watch?v=F2jd5WuT-zg">vid√©o passionnante</a> sur une mani√®re de guider le comportement d'un LLM sans faire appel au "fine-tuning". Il y est question de <a href="https://www.neuronpedia.org/">neuronpedia.org</a>, un site qui √† premi√®re vue semble plein de promesses pour qui souhaite mieux comprendre comment fonctionnent des LLM.</p>
<p>Faire attention √† la taille des requ√™tes (prompts) que l'on est tent√© d'envoyer √† l'API d'un LLM. J'essaie par exemple de faire analyser les 961 commentaires d'une vid√©o YouTube √† l'aide de la technique dont j'ai parl√© le 24 novembre dernier.</p>
<p><pre><code>cat o_swEgbBhMU.json | llm -m claude-3.7-sonnet "Analyse these YouTube comments and make a summary of the trends" > o_swEgbBhMU.txt</code></pre></p>
<p>Voici le r√©sultat :</p>
<p><pre><code>Error: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'prompt is too long: 225343 tokens > 200000 maximum'}, 'request_id': 'req_011CWNQm6M6MMRY9bnHuPX78'}</code></pre></p>
<p>225343 tokens, c'est l'√©quivalent d'environ 169 000 mots. C'est √©norme. On ne se rend pas forc√©ment compte.</p>
<p><b>19-12-2025</b></p>
<p>Depuis des mois je me dis qu'il serait temps de faire un backup de tous mes fichiers GPX en ligne sur Komoot. C'√©tait autrefois possible avec un outil en Python disponible sur Github : https://github.com/ThePBone/KomootGPX. Le hic c'est que √ßa ne fonctionnait plus depuis plusieurs ann√©es. J'y retourne √† tout hasard et constate avec surprise que le d√©p√¥t a d√©m√©nag√© vers https://github.com/timschneeb/KomootGPX, que √ßa fonctionne √† nouveau et qu'en plus uv g√®re tr√®s bien l'outil. Je fais d'abord <code>uv tool install komootgpx</code>, puis <code>uv run komootgpx -a</code>, je tape mes "credentials" et √† mon grand ravissement l'outil t√©l√©charge les 776 fichiers GPX de mon compte Komoot.</p>
<p><b>18-12-2025</b></p>
<p>Je me demande si des chercheurs ont √©tudi√© les effets de petites variations dans la formulation d'un prompt sur les r√©sultats d'un LLM. Je tape "prompt variations" dans Google et tombe sur <a href="https://mirascope.com/docs/mirascope/guides/prompt-engineering/chaining-based/diverse">cette page</a> du site Mirascope, une sorte de boite √† outils bien fournie pour travailler avec les LLM. Je copie le script propos√© dans Claude et lui demande de le modifier pour qu'il fonctionne avec Groq et uv. Dans un second temps je fais en sorte que le script affiche toutes les √©tapes internes et pas seulement le r√©sultat final. <a href="https://gist.github.com/sebington/f0b3147f0d0b6a60a616a3e6bfe44ffd">R√©sultat</a>.</p>
<p>J'ai peut-√™tre un nouveau projet en t√™te. Cr√©er un syst√®me pour automatiser la <a href="https://fr.wikipedia.org/wiki/Traduction_aller-retour">traduction aller-retour</a> d'un texte (appel√© aussi traduction inverse ou r√©tro-traduction), avec un module robuste d'√©valuation du texte r√©tro-traduit par rapport √† l'original. Je me souviens maintenant que l'IA fait beaucoup appel √† cette technique pour augmenter artificiellement le nombre de textes parall√®les afin d'entra√Æner les mod√®les de traduction.</p>
<p><b>17-12-2025</b></p>
<p>Mon <a href="https://github.com/google-gemini/gemini-cli/issues/15070">rapport de bug</a> sur Gemini CLI a √©t√© cl√¥tur√© car consid√©r√© identique √† <a href="https://github.com/google-gemini/gemini-cli/issues/15154">celui-ci</a>.</p>
<p>Je vois sur le site de Willison que <a href="https://simonwillison.net/2025/Dec/12/openai-skills/">OpenAI adopte √† son tour les skills</a> cr√©√©es par Anthropic. C'est l'occasion de revoir un peu le concept et la mani√®re concr√®te de s'en servir. Les <a href="https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview">skills</a> sont potentiellement tr√®s utiles √† un service de traduction. <a href="https://www.youtube.com/watch?v=421T2iWTQio">Cette vid√©o</a> me semble un bon point de d√©part pour essayer de comprendre comment √ßa marche.</p>
<p>Impression de stagner en ce moment, ce qui est normal dans tout apprentissage, mais me semble en m√™me temps paradoxal. Trop de LLM, trop d'outils, trop de fonctionnalit√©s et trop de monde pour expliquer tout √ßa. On se cogne √† l'infini. Il faut g√©rer la fatigue et se restreindre, quasiment se r√©signer, ce qui est difficile √† accepter. Peut-√™tre que se concentrer sur la doc officielle des outils serait suffisant.</p>
<p>Plus probablement mieux vaut partir d'une id√©e, d'un besoin r√©el, et se mettre au travail avec ce qu'on a sous la main sans chercher √† vouloir tout ma√Ætriser. C'est ce que j'ai fait quand j'ai cr√©√© le <a href="https://github.com/sebington/llm-hf">plugin</a> pour Willison's LLM. C'est venu comme √ßa. Sans prise de t√™te. Quasiment par instinct.</p>
<p><b>15-12-2025</b></p>
<p>Ce matin je mets toute mon √©nergie attentionnelle √† la lecture d'un article d'une coll√®gue sur les biais des LLM. L'approche, tr√®s int√©ressante, fait ressortir le fait que, comme les humains, les LLM ont tendance √† r√©pondre de fa√ßon biais√©e. La mani√®re dont le prompt est formul√© par l'utilisateur conditionne √©norm√©ment (bien davantage que ce que l'on croit) la r√©ponse que l'on va obtenir.</p>
<p>Ce soir j'√©coute dans le noir <a href="https://www.youtube.com/watch?v=tt3kY19ciFA">la derni√®re vid√©o</a> d'Armin Ronacher, une discussion avec un autre d√©veloppeur. Ils parlent tr√®s vite et je ralentis la vitesse de 10 %. Il y est encore et toujours question des agentic tools comme Claude Code. Ils √©voquent notamment la fatigue des d√©veloppeurs devant les sorties incessantes de nouveaux mod√®les qui bousculent leurs habitudes. Armin explique qu'un d√©veloppeur apprivoise peu √† peu un mod√®le, apprend √† le conna√Ætre, d√©couvre ses points forts et ses faiblesses et doit tout recommencer √† chaque nouvelle version ou changement de mod√®le.</p>
<p>Un point essentiel : les mod√®les semblent √™tre entra√Æn√©s par renforcement pour fonctionner de mani√®re optimale avec des outils sp√©cifiques. Par exemple les mod√®les d'Anthropic sont "align√©s" sur Claude Code et les workflows de type Bash. En cons√©quence les agents construits autour des mod√®les d'Anthropic sont moins performants si on les utilise avec un mod√®le diff√©rent (par exemple Gemini). Ainsi, les outils de codage agentique deviennent des plateformes sp√©cifiques √† un (ou une famille de) mod√®le(s), et non des coques interchangeables.</p>
<p>Autre conseil d'Armin : s√©parer la recherche et l'ex√©cution. Demander √† un agent √† la fois de faire des recherches approfondies et d'impl√©menter le code √©choue souvent. Il vaut mieux utiliser le mod√®le en premier lieu comme un assistant de recherche et dans un second temps comme un codeur, voire d'utiliser deux mod√®les distincts pour chacune de ces t√¢ches.</p>
<p>Il est longuement question de la taille des fen√™tres contextuelles. Armin pense que les performances du syst√®me se d√©gradent √©norm√©ment au del√† de 150 000 tokens, peu importe la taille th√©orique de la fen√™tre contextuelle du mod√®le. Il est partisan d'effectuer un compactage manuel de la session sans attendre que l'outil d√©cide de le faire de lui-m√™me. Ce compactage manuel consiste la plupart du temps √† cr√©er un fichier markdown de tout ce qui a √©t√© effectu√© pendant la session en cours et de le relire/modifier si besoin. Ensuite on efface l'ardoise et on peut continuer.</p>
<p>Dans la discussion il est question de <a href="https://ampcode.com/">AMP</a>, un autre outil d'agentic coding, que je finis par installer comme √ßa pour voir.ü•±</p>
<p><b>14-12-2025</b></p>
<p>Je soumet un <a href="https://github.com/google-gemini/gemini-cli/issues/15070">rapport de bug</a> sur Github concernant le dr√¥le de comportement de Gemini CLI hier soir.</p>
<p><b>13-12-2025</b></p>
<p>L'appli en TypeScript g√©n√©r√©e par Google AI Studio il y a plus d'un mois (AI Proofreader Plus) fonctionne mieux que la v4 de docx-copy-editor dont je parlais hier. Je t√©l√©charge AI Proofreader Plus (fichier zipp√©) et tente quelque chose de diff√©rent. Je demande √† Gemini CLI de l'analyser et de faire en sorte de remplacer gemini-2.5-flash par claude-haiku-4-5. Gemini y parvient non sans difficult√© et au passage <a href="http://bytepacking.free.fr/files/gemini-cli-bug.png">me propose d'effacer un fichier</a> qu'il vient de g√©n√©rer (<code>I created GEMINI.md with project details and instructions, completing the task. Now, deleting it.</code>). Bizarre. Apr√®s quelques soucis de cl√© API, la connexion √† Claude fonctionne mais l'appli est incompatible avec le fonctionnement de haiku, qui re√ßoit les infos mais est incapable de renvoyer des suggestions d'am√©lioration du texte. Logiquement, l'appli g√©n√©r√©e par Google est compatible avec un mod√®le... de chez Google. J'arr√™te l√† pour l'instant.</p>
<p><b>12-12-2025</b></p>
<p>Je demande √† Gemini CLI d'analyser la v4 de docx-copy-editor et de me d√©crire exactement ce que fait le script √©tape par √©tape. Gemini CLI me fait une analyse tr√®s fine de chaque √©tape et produit un rapport d√©taill√©. Apr√®s l'avoir lu, je me pose la question de savoir quelles sont les informations pass√©es au LLM (et sous quelle forme) une fois que le texte a √©t√© extrait du document Word. Je demande √† Gemini de se d√©brouiller pour r√©cup√©rer l'info avant qu'elle soit soumise au LLM et de l'√©crire dans un fichier TXT. Gemini cr√©e une version modifi√©e de v4 qui fait pr√©cis√©ment cela. Je r√©cup√®re ce qui est envoy√© au LLM. C'est tr√®s instructif et permet de confirmer que le d√©coupage du texte du document Word se fait bien au niveau du paragraphe. Mais je constate aussi que TOUS les paragraphes sont envoy√©s au LLM en une seule fois, avec toute une s√©rie d'instructions sens√©es guider le LLM.</p>
<p><pre><code>You are an expert copy editor. Follow these rules STRICTLY:</code></pre></p>
<p><pre><code>CRITICAL RULES:
1. LANGUAGE: The text is in French. Keep ALL edits in French. Do NOT translate to any other language.
2. STRUCTURE: Each paragraph is marked with [P1], [P2], etc. You MUST preserve these markers EXACTLY in your response.
3. MINIMAL EDITS: Make only necessary corrections for grammar, spelling, punctuation, and syntax errors.
4. WORD-LEVEL PRECISION: Change only the specific words that need correction. Do NOT rewrite or paraphrase correct text.
5. PRESERVE WORDING: If a word, phrase, or sentence is already correct, keep it EXACTLY as-is.
6. NO ADDITIONS: Do not add introductions, conclusions, summaries, comments, or explanations.
7. NO DELETIONS: Do not remove paragraphs or sentences unless they are exact duplicates.
8. NO REORGANIZATION: Keep paragraphs in the same order with their original markers.
9. OUTPUT FORMAT: Return ONLY the edited paragraphs with their [P#] markers. Each paragraph on a new line, separated by blank lines.</code></pre></p>
<p><b>11-12-2025</b></p>
<p><a href="https://antigravity.google/">Antigravity</a> donne actuellement un acc√®s gratuit √† <code>Opus 4.5 thinking</code> et √† <code>Sonnet 4.5 thinking</code> en plus de <code>Gemini 3 Pro</code>. Toujours bon √† prendre. En revanche la f√™te est d√©j√† finie pour Mistral Vibe (v1.1.2). Cl√© API maintenant requise pour utiliser l'outil.</p>
<p>Arch√©ologie : je tombe <a href="https://stackoverflow.com/questions/24282505/search-for-a-particular-spoken-word-in-audio-files">ce vieux post</a> de Stack Overflow que j'avais sauvegard√© je ne sais plus quand. Probl√®me d√©finitivement r√©solu.</p>
<p>J'utilise Antigravity pour faire une v4 de mon "docx-copy-editor". Je demande √† Opus de faire la planification et confie l'ex√©cution √† Sonnet. Depuis que je travaille sur ce projet, je suis pass√© d'un script (v0) qui n'utilisait que les mod√®les de Groq (environ 300 lignes de code) √† une grosse usine √† gaz de plus de 1000 lignes de code, d√©clar√©e "production ready" par Antigravity, mais qui ne donne toujours pas de r√©sultats √† la hauteur de mes attentes.</p>
<p>Les LLM ont beaucoup de mal √† travailler au niveau de la phrase ou du mot. Ils prennent les paragraphes entiers, peut-√™tre plusieurs √† la fois. Souvent ils les r√©√©crivent compl√®tement et vont parfois jusqu'√† les traduire alors qu'on ne leur a pas demand√© de le faire. Mon script extrait le texte du document Word, le donne au LLM et le r√©injecte ensuite dans Word, qui fait ce qu'il peut pour afficher la masse de changements en mode "suivi des modifications". Pas s√ªr que mon approche soit la bonne. J'essaie trop de forcer la machine √† effectuer des t√¢ches √† la mani√®re des humains.</p>
<p><b>10-12-2025</b></p>
<p>J'installe <a href="https://github.com/mistralai/mistral-vibe">Mistral Vibe</a> en utilisant <code>curl -LsSf https://mistral.ai/vibe/install.sh | bash</code> et voici ce qui se passe :</p>
<p><pre><code>Starting Mistral Vibe installation...
[INFO] Detected Linux platform
[INFO] uv is already installed: uv 0.9.17
[INFO] Installing mistral-vibe from GitHub repository using uv...
Resolved 54 packages in 368ms
Prepared 28 packages in 200ms
Installed 54 packages in 12ms</code></pre></p>
<p>L'installateur d√©tecte si uv est pr√©sent et boum, tout est install√© en 580 msüî•</p>
<p>J'installe √©galement Kimi CLI avec <code>uv tool install --python 3.13 kimi-cli</code> (cette fois c'est uv direct). Par contre, pour utiliser Kimi CLI il faut obligatoirement une cl√© API. Pas de free tier pour tester, donc.</p>
<p>Mistral Vibe a chang√© (v1.1.1) depuis mon premier test ce midi : la fen√™tre contextuelle est pass√©e de 100k √† 200k tokens (!) et l'outil se comporte beaucoup mieux. J'avais en effet un bug au niveau des /commandes, qui a disparu. Je le fais travailler sur mon projet "docx-copy-editor" et il me propose une quantit√© d'am√©liorations possibles, qu'il impl√©mente dans la foul√©e √† toute vitesse. J'avoue que suis impressionn√©, m√™me si apr√®s avoir quitt√© le programme, j'ai eu un avertissement <code>Segmentation fault (core dumped)</code>.</p>
<p><b>09-12-2025</b></p>
<p>Claude Code <a href="https://www.reddit.com/r/ClaudeAI/comments/1pgxckk/claude_cli_deleted_my_entire_home_directory_wiped/">aurait accidentellement d√©truit</a> tout le r√©pertoire "home" d'un Mac en ex√©cutant la commande <code>rm -rf tests/ patches/ plan/ ~/'</code> et en expliquant ensuite √† l'utilisateur que la commande ex√©cut√©e √©tait "catastrophique".</p>
<p>ChatGPT peut vraiment √™tre tr√®s p√©nible. Si on lui donne quelque chose d'un tant soit peu long √† traiter, il est visiblement programm√© pour l√¢cher la r√©ponse en petits morceaux, ce qui complique beaucoup la vie de l'utilisateur, qui sera ainsi tent√© de prendre un abonnement.</p>
<p>Je lui donne du texte √† hauteur de 1600 mots et demande de le traduire en anglais, puis dans un second temps de produire comme hier une version TMX de la traduction qu'il vient de g√©n√©rer. Cette fois il me propose le code XML en petits lots de 100 segments. Je copie-colle chaque segment dans un fichier. Au bout de 3 copier-coller, il me dit que j'ai atteint la limite de mon quota et que je dois revenir dans plusieurs heures.</p>
<p>Mais ce n'est pas tout. Il m'assure que la traduction encapsul√©e dans le code XML est exactement la m√™me que celle qu'il vient de produire. Je v√©rifie et constate que ce n'est pas le cas. Quand ChatGPT g√©n√®re le code XML, il ne parvient pas forc√©ment √† recopier fid√®lement ce qu'il vient de produire. Enfin, le code XML, qui hier √©tait toujours nickel, est maintenant pourri. Balises non ferm√©es, code langue qui varie, le bordel quoi.</p>
<p><b>08-12-2025</b></p>
<p>Je donne des extraits de texte √† traduire √† Gemini (2.5 Flash) avec un peu de contexte pour am√©liorer la pertinence de la r√©ponse. J'ai remarqu√© qu'interagir un peu avec un LLM avant de lui demander le gros de la t√¢che semble donner de meilleurs r√©sultats, et pas seulement dans le domaine de la traduction. Discuter un peu avec le mod√®le lui permet de mieux comprendre de quoi il retourne. Au bout d'un moment je lui donne un paragraphe entier, lui demande de le traduire et ‚Äìsoyons fous‚Äì de g√©n√©rer une m√©moire de traduction des deux textes (source et cible). Il ne g√©n√®re pas le fichier mais son contenu en XML. Il fait √ßa avec une rapidit√© et une facilit√© d√©concertantes. Il ne me reste plus qu'√† copier-coller le r√©sultat dans un fichier texte que je sauvegarde ensuite sous la forme <code>fichier.tmx</code>. Puis j'importe le fichier TMX dans mon logiciel de traduction et pour chaque segment (~ phrase) peux maintenant comparer la traduction existante avec la nouvelle proposition de Gemini.</p>
<p>Grosse mise √† jour de Manjaro suivi d'une tentative de red√©marrage qui √©choue. Je repasse sur Ubuntu et parvient √† r√©cup√©rer mes donn√©es dans le r√©pertoire <code>@home</code> du disque d√©di√© √† Manjaro. La solution est peut-√™tre sur <a href="https://wiki.manjaro.org/index.php?title=GRUB/Restore_the_GRUB_Bootloader">cette page</a>.</p>
<p>J'installe <a href="https://antigravity.google/">Google Antigravity</a> sur un laptop avec Ubuntu et 16 Go de RAM. Antigravity bugue et crashe ("m√©moire insuffisante"). Je d√©cide de le d√©sinstaller. Je demande √† ChatGPT comment le faire le plus proprement possible. ChatGPT n'a bien entendu jamais entendu parler du logiciel Antigravity, sorti il y a seulement 20 jours. Cependant "Google Antigravity" lui √©voque quelque chose. Dans sa r√©ponse, ChatGPT me glisse "Google Antigravity is a long-running tech in-joke", ce qui pique ma curiosit√©. Je lui demande de m'en dire plus. Et l√† il d√©balle l'histoire ci-dessous, incroyable, mais vraie !</p>
<p>"Google Antigravity n‚Äôest pas un vrai produit de Google. C‚Äôest en r√©alit√© une blague r√©currente dans la culture tech, bas√©e sur l‚Äôid√©e que Google disposerait de technologies secr√®tes incroyablement avanc√©es. L‚Äôexpression vient principalement d'une bande dessin√©e, <a href="https://xkcd.com/353/">"xkcd #353 ‚Äî Python"</a>, dans laquelle un personnage se met √† voler comme un oiseau apr√®s avoir tap√© <code>import antigravity</code> dans son terminal.</p>
<p>Cette blague est devenue culte, au point qu'il existe bel et bien un module Python nomm√© <code>antigravity</code>. La commande <code>python3 -m antigravity</code> ouvre en effet un navigateur sur la page de la BD en question. Avec le temps, la blague s'est r√©pandue dans la communaut√© des d√©veloppeurs, avec des blagues du genre "Google est tellement avanc√© qu‚Äôils ont s√ªrement invent√© une API d‚Äôantigravit√©". Certains d√©veloppeurs cr√©ent de faux d√©p√¥ts ou de faux paquets nomm√©s ‚Äúantigravity‚Äù. Le d√©p√¥t APT que vous avez install√© n'est probablement pas un produit officiel de Google".</p>
<p><b>07-12-2025</b></p>
<p>Essais de comparaison de la popularit√© de 2 outils d'agentic coding (Claude Code vs Antigravity) au travers du nombre de vid√©os YouTube sur le sujet (mots-cl√©s dans le titre de la vid√©o). Mon script yt-first-video-search d√©tecte 539 vid√©os pour "claude code" depuis le 24-02-2025 (soit depuis 286 jours) et 508 vid√©os pour "antigravity" depuis le 18-11-2025 (soit depuis 19 jours). Antigravity impressionne par la mont√©e rapide de sa popularit√©. En un temps beaucoup plus court (15 fois moins longtemps), Antigravity a suscit√© presque autant de vid√©os que Claude Code. Ceci dit, Claude Code √©tait le premier de ces outils et sa popularit√© a logiquement mis plus de temps √† s'√©tablir. Mais √ßa n'explique pas tout.</p>
<p>Olivier sugg√®re de regarder Claude Code vs Antigravity sur <a href="https://trends.google.fr/trends/explore?q=claude%20code,antigravity">Google Trends</a>. J'ajoute par curiosit√© 3 autres outils (Gemini CLI, OpenAI Codex et OpenCode) ce qui donne <a href="https://trends.google.fr/trends/explore?q=claude%20code,antigravity,openai%20codex,gemini%20cli,opencode">cette vue</a>. Je r√®gle la dur√©e sur 12 mois. On peut aussi choisir d'afficher les recherches effectu√©es directement dans YouTube. Je suis surpris par la ligne presque plate correspondant √† OpenCode. En dessous sur la carte du monde, on voit que le pays le plus actif en termes de recherche semble √™tre de loin la Chine.</p>
<p><b>06-12-2025</b></p>
<p>Chose apprise aujourd'hui : Ne PAS installer CURL avec SNAP ! Explication de Claude : "There are significant issues with installing curl via snap, primarily related to snap's security confinement. Snap's security model restricts curl to accessing only non-hidden files under the /home/$USER/ directory. This means the snap version of curl cannot write files to many common locations, causing failures with various installers and tools." Vraiment n'importe quoi snap ! üò§</p>
<p><b>03-12-2025</b></p>
<p>Une <a href="https://youtu.be/NV5uYJ3vxZg">courte vid√©o</a> sur <a href="https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor/tree/main">un outil</a> qui permet de suivre le quota d'utilisation de Claude Code. Ce qu'avance la youtubeuse semble √™tre en phase avec le contenu de la <a href="https://claudelog.com/faqs/claude-code-usage/">page officielle</a> de la documentation d'Anthropic sur le sujet.</p>
<p>Une <a href="https://www.youtube.com/watch?v=imw8MkwW9xw">autre vid√©o</a> sur deux outils qui permettent entre autre de mieux utiliser et de mieux comprendre comment Claude Code fonctionne.</p>
<p><b>02-12-2025</b></p>
<p>Suite √† l'appel du projet Llamafile "We want to hear from you!", hier soir je poste <a href="https://github.com/mozilla-ai/llamafile/discussions/809">sur leur Github</a> un petit r√©sum√© de ce que Llamafile repr√©sente pour moi. Ce matin je suis ravi de constater que Justine Tunney, l'initiatrice du projet, a lik√© mon post.üòÄ</p>
<p>J'essaie d'am√©liorer mon outil de r√©vision de documents Word avec Antigravity/Sonnet. Le syst√®me me sugg√®re une am√©lioration notable : la prise en compte du texte dans les tableaux, dans les ent√™tes et dans les pieds de page. Je donne le feu vert et Antigravity fait le boulot sans probl√®me majeur.</p>
<p>Eclaircissement au sujet de l'utilisation de uv avec un outil d'agentic coding. Je peux tout √† fait utiliser uniquement uv. Il faut juste le d√©clarer quelque part. Je pose la question √† Antigravity, qui me cr√©e sur le champ <a href="http://bytepacking.free.fr/files/run-scripts.md">un fichier en markdown</a> avec les instructions dont il a besoin pour un usage exclusif de uv. Ce fichier, r√©utilisable, va m'√™tre tr√®s utile je pense.</p>
<p><b>01-12-2025</b></p>
<p>J'enregistre une petite vid√©o sur des essais de g√©olocalisation d'images avec <a href="https://nicolas-dufour.github.io/plonk">l'outil de Nicolas Dufour</a>. Je fais des copies d'√©cran √† partir d'une <a href="https://www.youtube.com/watch?v=Ffh9OeJ7yxw">video</a> YouTube et l'outil identifie correctement le lieu comme √©tant du c√¥t√© de Kuala Lumpur, bluffant.</p>
<p>Je fais une session avec OpenCode en utilisant Big Pickle comme mod√®le. Big Pickle semble gratuit et propose une fen√™tre contextuelle de 200 K tokens et 128 K tokens en sortie. Je d√©poussi√®re un vieux script qui effectue une recherche sur Internet avec des mots-cl√©s en utilisant DuckDuckGo puis produit un r√©sum√© en interrogeant un mod√®le de Groq. OpenCode utilise uv comme je lui ai dit de le faire, cependant il privil√©gie une approche classique, cr√©e un environnement virtuel, un dossier .git, etc. J'ai l'impression que la performance se d√©grade l√©g√®rement √† mesure que le contexte se remplit mais globalement je parviens √† mes fins et r√©sous tous les probl√®mes rencontr√©s. Session tr√®s agr√©able et didactique.</p>
<p><b>30-11-2025</b></p>
<p>Dans la cat√©gorie "youtubeurs insupportables"üòÖ, Alex Finn serait certainement tr√®s bien plac√©, peut-√™tre m√™me devant NetworkChuck ou Matthew Berman. Mais je dois avouer que <a href="https://youtu.be/wxFt2d7xN44">sa derni√®re vid√©o</a> sur Claude Code m'a s√©duit. Le ton qu'il emploie, tr√®s enjou√© et spontan√©, donne une impression de sinc√©rit√©. Le fait qu'il insiste avant tout sur la simplicit√© me pla√Æt. Il d√©nonce les nombreuses autres vid√©os sur le m√™me sujet de la part de soi-disant gourous de la tech, avec leurs recettes compliqu√©es pour tirer le meilleur parti de Claude Code. Il semble dire : arr√™tez de vous prendre la t√™te, Claude Code est plus intelligent que vous ne le croyez ! Pas besoin de le tenir par la main et de lui expliquer la vie. En outre Finn d√©nonce l'attitude de certains d√©veloppeurs qui pr√©f√®rent s'amuser pendant que l'IA bosse (avec toutes sortes d'<a href="https://www.ycombinator.com/launches/OgV-chad-ide-the-first-brainrot-ide">applications d√©biles</a> de doom scrolling) au lieu de rester concentr√©s sur ce qu'il sont en train de faire.</p>
<p><b>29-11-2025</b></p>
<p>Je teste <a href="https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/blob/main/mistralai_Voxtral-Mini-3B-2507-Q6_K.gguf">une version quantis√©e</a> de 3.3 Go du mod√®le Voxtral de Mistral avec llama-cpp. J'essaie d'obtenir la transcription d'un MP3 de 43 secondes. J'utilise la commande suivante pour lancer le serveur web local :</p>
<p><pre><code>llama-server -hf bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF:Q6_K --temp 0</code></pre></p>
<p>J'importe le MP3 et l'accompagne du prompt suivant : "Transcribe precisely this audio segment".</p>
<p>Le temps pour obtenir la transcription (CPU only) est de 1:46 min (1:24 min o√π il ne se passe rien √† l'√©cran et 22 secondes o√π l'on peut voir la transcription s'afficher en streaming). Le r√©sultat est moyen :</p>
<p><span class="code-box">I use Claude a lot. As a hobbyist, I run it in a VM several times a week on side projects, often with the dangerous skip permissions flag to just vibe code whatever idea is on mind. Professionally, part of my team builds the AI/DE rules and tooling for our engineering team, and they consume several billion tokens per month for Claude. The CLI agent space is crowded. Between Claude, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But to be honest, when I talk to other developers, their choice often comes down to what feels like superficial, a lucky feature implementation, or a system prompt vibe they just prefer."</span></p>
<p>Second essai avec le m√™me mp3 et le mod√®le "whisper small" encapsul√© dans un llamafile (<a href="https://huggingface.co/Mozilla/whisperfile/blob/55cdd186651c87529f4442981fe1726cdfe0f188/whisper-small.llamafile">whisper-small.llamafile</a>) de 725 Mo que j'avais conserv√© sur un disque dur. Voici la commande :</p>
<p><pre><code>./whisper-small.llamafile -f transcoded-test-2.mp3 -pc</code></pre></p>
<p>Le r√©sultat est sans appel : la transcription est quasi parfaite et le temps d'inf√©rence est de seulement 8 secondes !</p>
<p><span class="code-box">I use Claude code. A lot. As a hobbyist, I run it in a VM several times a week on side projects, often with the dangerously skipped permissions flag to just vibe code whatever idea is on my mind. Professionally, part of my team builds the AI IDE rules and tooling for our engineering team, and they consume several billion tokens per month just for code agent. The CLI agent space is getting crowded. Between Claude code, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But to be honest, when I talk to other developers, their choice often comes down to what feels like Superficials, a lucky feature implementation, or a system prompt vibe they just prefer.</span></p>
<p>Je suis tr√®s surpris de la qualit√© du r√©sultat, de la vitesse de traitement et du fait que ce mod√®le llamafile, vieux de plus d'un an, √©pelle correctement Claude Code, Gemini CLI etc. Il est meilleur sur tous les tableaux !</p>
<p><b>28-11-2025</b></p>
<p>Je tombe sur une vid√©o qui me fait retourner voir ce que <a href="https://github.com/ggml-org/llama.cpp">llama-cpp</a> devient. Sur Github j'ai rarement vu une activit√© aussi soutenue que celle de Georgi Gerganov. A chaque fois que je regarde le "release" le plus r√©cent de llama-cpp, il est vieux de quelques heures, parfois de quelques minutes seulement.</p>
<p>Je t√©l√©charge la version <code>llama-b7189-bin-ubuntu-x64.zip</code> que je d√©compresse dans Downloads. Puis je copie le contenu du r√©pertoire <code>bin</code> dans mon <code>~/.local/bin</code> (qui contient entre autres <code>uv</code> et <code>uvx</code>). Je peux maintenant lancer des commandes pour ex√©cuter llama-cpp depuis n'importe quel r√©pertoire.</p>
<p>La nouveaut√© dont il est question dans la vid√©o et que je veux tester est la commande <code>llama-server</code> qui permet de lancer une interface web tr√®s similaire √† celle de ChatGPT. On peut la lancer de la mani√®re suivante</p>
<p><pre><code>llama-server -hf ggml-org/gemma-3-1b-it-GGUF</code></pre></p>
<p>ce qui t√©l√©charge automatiquement un mod√®le de Hugging Face et lance un serveur sur http://127.0.0.1:8080/.</p>
<p>On peut d√©cider de t√©l√©charger manuellement un autre mod√®le et le lancer avec la commande suivante</p>
<p><pre><code>llama-server -m gemma-3-4b-it-Q4_K_M.gguf</code></pre></p>
<p>La page web du chatbot se comporte de mani√®re assez standard, avec quelques fonctionnalit√©s sympathiques comme la possibilit√© d'afficher en temps r√©el la taille de la fen√™tre contextuelle, le nombre de tokens g√©n√©r√©s et la vitesse de l'inf√©rence en tokens par seconde. On peut ainsi exp√©rimenter et voir la taille de la fen√™tre contextuelle d√©cro√Ætre au fur et √† mesure que la conversation avance. On voit le nombre total de token g√©n√©r√©s apr√®s chaque requ√™te.</p>
<p>Par d√©faut je constate que j'ai une fen√™tre contextuelle de 4096 tokens, ce qui est loin des 128 K annonc√©s. Apr√®s <a href="https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune">quelques recherches</a> je me rends compte que 4096 est la valeur par d√©faut de llama-cpp, et qu'on peut l'augmenter en passant <code>--ctx-size 8192</code> ou <code>--ctx-size 16384</code> au moment de lancer le serveur :</p>
<p><pre><code>llama-server -m gemma-3-4b-it-Q4_K_M.gguf --ctx-size 8192</code></pre></p>
<p>Si par exemple on remplit compl√®tement la fen√™tre contextuelle, ce que j'ai fait volontairement pour voir, la g√©n√©ration s'arr√™te net. On peut alors arr√™ter le serveur (<code>CTRL C</code>) et le red√©marrer avec une fen√™tre contextuelle plus grande, puis relancer la derni√®re requ√™te de l'interface web et poursuivre la conversation. Enfin, on peut exporter chaque conversation dans un fichier JSON.</p>
<p><b>26-11-2025</b></p>
<p>Avec mon outil yt-trend, j'exp√©rimente l'acc√®s aux API gratuites propos√©es ici ou l√†. En l'occurence, je cible les h√©bergeurs de LLM suivants : Mistral, Groq, OpenRouter, Hugging Face et Github. Je constate que lorsqu'on envoie une requ√™te √† un LLM via une API, on peut essuyer un refus, m√™me pour un mod√®le classifi√© de gratuit. Il me semble que la taille (en tokens) de la requ√™te (voir "Requested" ci-dessous) est un √©l√©ment essentiel. Les fournisseurs mettent des limites sur le nombre de tokens en entr√©e et ce nombre peut varier selon les mod√®les d'un m√™me fournisseur. C'est un peu au petit bonheur la chance et il est difficile d'avoir un service fiable dans la dur√©e. J'essaie par exemple de faire analyser le contenu d'un fichier JSON de 93 Ko et j'obtiens :</p>
<p><pre><code>An error occurred during the API call: Error code: 429 - {'error': {'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'qwen/qwen3-235b-a22b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations', 'provider_name': 'Venice'}}</code></pre></p>
<p>Avec Groq, m√™me fichier, autre message, plus pr√©cis :</p>
<p><pre><code>An error occurred during the API call: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `12345` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 29369, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}</code></pre></p>
<p>ou bien</p>
<p><pre><code>An error occurred during the API call: Error code: 400 - {'error': {'message': 'Please reduce the length of the messages or completion.', 'type': 'invalid_request_error', 'param': 'messages'}}</code></pre></p>
<p>ou encore</p>
<p><pre><code>An error occurred during the API call: Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `12345` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Used 23115, Requested 28914. Please try again in 44.058s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}</code></pre></p>
<p>Chose int√©ressante concernant la r√©ponse ci-dessus : le mod√®le mobilis√© est "meta-llama/llama-4-scout-17b-16e-instruct" alors que j'avais s√©lectionn√© "groq/compound" dans la liste de mod√®les de l'API de Groq. Autre petit d√©tail : la taille de ma requ√™te (en tokens) varie l√©g√®rement selon les mod√®les (29369 vs 28914 par ex.).</p>
<p><b>25-11-2025</b></p>
<p>Nouvelle session avec Antigravity. Tout se passe bien mais Gemini 3.0 doit √™tre sursollicit√© et met un temps fou √† effectuer les t√¢ches qu'il a lui-m√™me d√©fini suivant mon prompt. Chose int√©ressante, il d√©cide de donner une r√©ponse √† mon probl√®me de mani√®re d√©terministe, sans faire appel √† un LLM en ligne. Il a d√©tect√© que ce que je demande est moins compliqu√© qu'il n'y para√Æt. Le r√©sultat fonctionne mais n'est pas √† la hauteur de mes attentes. Il faudra probablement refaire appel √† un LLM pour obtenir quelque chose de correct.</p>
<p>Rentr√© du travail, allong√© sur mon canap√© dans le noir, j'√©coute <a href="https://www.youtube.com/watch?v=xfm99Tb7CNo">une autre vid√©o</a> d'Armin Ronacher, le d√©veloppeur autrichien. Je me concentre sur sa voix et ce qu'il veut transmettre. Je devine que le support visuel n'est pas tr√®s important. C'est vraiment tr√®s int√©ressant d'avoir la possibilit√© d'√©couter l'avis d'un d√©veloppeur chevronn√© sur les nouveaux outils d'agentic coding, comme il les appelle. Il dit ne pas vouloir les nommer ni en choisir un, mais il transpara√Æt que Claude Code est son favori parmi la trentaine d'outils similaires disponibles au moment o√π il enregistre sa vid√©o, le 8 ao√ªt dernier, autant dire il y a une √©ternit√©. La vid√©o semble avoir √©t√© enregistr√©e en une seule prise. Il ne coupe pas les silences, les temps morts, et c'est beaucoup plus agr√©able √† √©couter que les <a href="https://www.youtube.com/watch?v=MsQACpcuTkU">mitraillettes habituelles</a>. On sent que m√™me lui a un peu de mal √† suivre le rythme. Il essaie d'expliquer pourquoi il est si difficile de s'y retrouver et d'√©valuer ces outils, qui changent tout le temps. Il a une compr√©hension tr√®s fine des choses. Il est tr√®s humain. Il dit qu'il en a assez des tweets de deux lignes agr√©ment√©s de smileys. Dans le noir, je trouve son intervention presque po√©tique. Oui, c'en est presque √©mouvant. Mais je m'√©gare, sans doute.</p>
<p><b>24-11-2025</b></p>
<p>Dans une <a href="https://www.youtube.com/watch?v=nfOVgz_omlU">vid√©o passionnante</a> datant de fin juin, Armin Ronacher, un d√©veloppeur, d√©courage fortement l'utilisation des MCP. La raison principale est que les MCP "polluent" le contexte (limit√©) du LLM. Il les trouve aussi "tr√®s ennuyeux √† utiliser" et leur pr√©f√®re l'utilisation d'un outil de ligne de commande.</p>
<p>D√©couverte de <a href="https://github.com/egbertbouman/youtube-comment-downloader">youtube-comment-downloader</a>, un outil pour t√©l√©charger les commentaires d'une vid√©o YouTube. Tr√®s pratique pour analyser le ressenti des internautes sur le contenu de la vid√©o.</p>
<p>Exemple d'utilisation avec <a href="https://docs.astral.sh/uv/">uv</a> :</p>
<p><pre><code>uvx youtube-comment-downloader --youtubeid nfOVgz_omlU --output nfOVgz_omlU.json</code></pre></p>
<p>Le fichier JSON obtenu peut ensuite √™tre analys√© d'une infinit√© de fa√ßons. Par exemple avec LLM, l'outil de Simon Willison (avec le plugin <a href="https://github.com/sebington/llm-hf">llm-hf</a>üòÑ install√©) :</p>
<p><pre><code>cat nfOVgz_omlU.json | llm -m moonshotai/Kimi-K2-Instruct "Analyse these YouTube comments and make a summary of the trends" > output.txt</code></pre></p>
<p><b>23-11-2025</b></p>
<p>Je parviens, au prix de pas mal de t√¢tonnements, √† installer et √† utiliser <a href="https://github.com/microsoft/playwright-mcp">playwright-mcp</a>. Playwright-mcp permet de controller un navigateur web au travers d'instructions r√©dig√©es en langage naturel et envoy√©es √† un LLM √† partir d'un "client" comme Claude Code ou Gemini CLI.</p>
<p>Le LLM interpr√®te ces instructions et les traduit en commandes qui vont effectuer des actions dans le navigateur. Pour que √ßa marche il faut que Node.js 18 "or newer" soit install√© sur la machine ET que <a href="https://playwright.dev/">Playwright</a> le soit aussi.</p>
<p>Il faut ensuite copier le code ci-dessous et le coller dans le fichier de configuration du client utilis√©. J'ai fait l'essai avec Gemini CLI et ai donc ajout√© ces instructions au fichier <code>settings.json</code> de Gemini.</p>
<p><pre><code>{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": [
        "@playwright/mcp@latest"
      ]
    }
  }
}</code></pre></p>
<p>Probl√®me : le fichier <code>settings.json</code> n'est pas vide. Il contient d√©j√† du code et il faut ins√©rer le code ci-dessus dans l'existant sans se tromper au niveau des accolades et des virgules, ce qui demande pas mal d'attention. Au final, dans mon cas, √ßa ressemble √† √ßa :</p>
<p><pre><code>{
  "ide": {
    "hasSeenNudge": true
  },
  "security": {
    "auth": {
      "selectedType": "oauth-personal"
    }
  },
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": [
        "@playwright/mcp@latest"
      ]
    }
  }
}</code></pre></p>
<p>Une fois ces pr√©paratifs termin√©s, on peut lancer Gemini CLI, qui d√©tecte automatiquement qu'un MCP server a √©t√© ajout√©. On peut maintenant taper des instructions comme "ouvre le navigateur et rends toi sur le site de telerama.fr", par exemple.</p>
<p>Par s√©curit√©, Playwright-mcp poss√®de son propre navigateur (google chrome) et n'interf√®re donc pas avec le ou les navigateurs d√©j√† install√©s.</p>
<p>Comme test je le fais aller sur komoot.com, je me connecte √† mon compte et me rends sur la page de mes activit√©s termin√©es. Je lui demande de s√©lectionner l'activit√© la plus r√©cente et de t√©l√©charger le fichier GPX de la sortie. Au d√©but je l'aide car il ne trouve pas le bouton "t√©l√©charger" qui est volontairement planqu√© (merci Komoot !).</p>
<p>Peu √† peu il comprend comment faire et √† la fin je lui dis simplement "recommence et t√©l√©charge deux autres fichiers" (en les nommant) ce qu'il fait. Mais tout le processus se d√©roule dans une lenteur abyssale. C'est ce qui m'a le plus surpris. Je lui ai pourtant donn√© toutes les autorisations. Un truc qui prendrait normalement quelques minutes √† un humain a pris une bonne demi-heure, sans compter le temps d'installation.</p>
<p>A la fin de la session, je lui demande de sauvegarder la proc√©dure dans un fichier .md pour ne pas √† tout devoir lui r√©-expliquer la prochaine fois. Je regarde la consommation de tokens en faisant <code>/stats</code>. Ce sont surtout les "inputs tokens" qui ont √©t√© consomm√©s : 2M de tokens pour Gemini-2.5-flash-lite et 7M pour Gemini-2.5-pro. Neuf millions de tokens en tout ! On est bien au del√† du quota quotididen d'un million, m√™me si Gemini explique que 70 % des input tokens "were served from the cache, reducing costs". Heureusement que Gemini CLI est gratuit !</p>
<p><b>22-11-2025</b></p>
<p>Je cherche √† comprendre la diff√©rence fondamentale entre le "sudo apt install" d'Ubuntu et le "yay -S" de Manjaro. <a href="https://claude.ai/share/c5300daf-b6ff-421a-995e-c0a5f7b373fb">R√©ponse tr√®s √©clairante</a> de Claude. Yay est tr√®s puissant et pratique une fois qu'on a compris de quoi il retourne.</p>
<p>Hier mon coll√®gue JB me parle d'un projet d'analyse d'images d'√©tiquettes de bouteilles de vin. Je commence une r√©flexion sur une approche possible du probl√®me. Profusion de possibilit√©s.</p>
<p>Je visionne <a href="https://youtu.be/0HqUYpGQIfs">la derni√®re vid√©o en date</a> de l'excellente cha√Æne "Emergent Garden" et tombe sur ceci : "An idiot admires complexity. A genius admires simplicity." ‚ÄîTerry Davis.</p>
<p>Je continue d'exp√©rimenter avec yt-first-video-search. Je me rends compte que la mani√®re avec laquelle la recherche s'effectue est assez opaque, et repose davantage sur l'algorithme de Youtube que sur les fonctionnalit√©s de yt-dlp. Je demande √† Antigravity/Sonnet 4.5 les endroits dans la page YouTube de la vid√©o o√π sont cherch√©s les mots-cl√©s. J'obtiens la r√©ponse suivante :</p>
<p><span class="code-box">The keywords are searched in the title, description, tags, and channel name - but NOT in comments. YouTube's algorithm prioritizes title matches, but will also return videos where keywords appear only in descriptions or tags.</span></p>
<p>J'essaie d'√©tendre les capacit√©s de l'outil √† d'autres recherches que celle d'un buzz en cours. Par exemple, j'essaie de trouver la vid√©o la plus ancienne du projet "llamafile". J'utilise "llamafile" comme unique mot-cl√© et d√©finis le nombre maximum de vid√©os √† 300. Le script renvoie 283 vid√©os et je constate que "llamafile" est seulement pr√©sent dans 97 titres. Cependant, l'outil fonctionne : √† chaque fois il permet de trouver la vid√©o la plus ancienne. Dans le cas pr√©sent il semble s'agir de <a href="https://www.youtube.com/watch?v=86lmdc0QP4c">celle-ci</a>, sortie deux jours seulement apr√®s l'<a href="https://hacks.mozilla.org/2023/11/introducing-llamafile/">annonce officielle</a> du projet llamafile, le 29 novembre 2023.</p>
<p>Au passage, je v√©rifie que la date du premier "commit" de llamafile sur Github (8 novembre 2023) est bien ant√©rieure √† la date de l'annonce officielle. Pour trouver cette info, je fais un <code>git clone</code> du repo, puis <code>cd llamafile</code> et enfin <code>git log --reverse</code> (q pour sortir).</p>
<p>Apr√®s de nombreuses modifications de yt-first-video-search, j'opte pour une s√©lection plus stricte par mots-cl√©s du titre uniquement, coupl√©e √† un nombre maximum de vid√©os d√©fini par l'utilisateur. Je laisse tomber l'id√©e du "time gap" entre 2 vid√©os, qui ne sert √† rien puisque le tri ne se fait pas en temps r√©el comme je le pensais mais uniquement apr√®s que l'ensemble des m√©tadonn√©es aient √©t√© r√©cup√©r√©es.</p>
<p>Je fais un premier essai pour le projet de JB avec un script g√©n√©r√© par Claude qui utilise l'API d'Anthropic pour analyser des images d'√©tiquettes de vin et r√©cup√®re des donn√©es structur√©es dans un fichier JSON. Je trouve <a href="https://huggingface.co/datasets/Francesco/wine-labels">un dataset</a> sur Hugging Face. Je demande ensuite √† Antigravity/Sonnet de modifier le script pour utiliser l'API de Groq, qui, via le mod√®le <code>meta-llama/llama-4-scout-17b-16e-instruct</code> peut tester gratuitement des images "par groupes de 5 maximum". J'essaie avec 6 juste pour voir et √ßa passe. Je demande aussi que l'utilisateur ait seulement √† saisir le nom du r√©pertoire o√π se trouvent les images. Voici <a href="http://bytepacking.free.fr/files/wine_labels_analysis.json">le fichier JSON</a> du r√©sultat. Ce n'est pas parfait mais pas si mal pour un d√©but.</p>
<p><b>21-10-2025</b></p>
<p>Essai d'am√©lioration de l'outil d√©crit hier (baptis√© "yt-first-video-search") avec Antigravity. L'id√©e est la suivante : utiliser <a href="https://github.com/TeamNewPipe/NewPipeExtractor.git">NewPipeExtractor</a> pour potentiellement am√©liorer yt-datetime-search. Je me dis que NewPipeExtractor, une biblioth√®que tr√®s robuste √©crite en Java, doit poss√©der des fonctionnalit√©s int√©ressantes. Je demande √† Antigravity d'analyser le code de ce d√©p√¥t et lui donne aussi 2 versions en Python de yt-datetime-search. Antigravity d√©tecte plusieurs am√©liorations possibles. Je choisis la suivante :</p>
<p><span class="code-box">L'objectif est d'am√©liorer yt-datetime-search en ajoutant un "parser" de dates relatives robuste, port√© √† partir du code source Java npext. Cela permettra au script d'analyser les cha√Ænes de caract√®res "time ago" (par exemple, "2 days ago") lorsque l'horodatage exact est absent des m√©tadonn√©es r√©cup√©r√©es par yt-dlp.</span></p>
<p>Antigravity fait un "Implementation Plan", que je valide et lui donne le feu vert. Il effectue les modifications, cr√©e un fichier de test et produit un rapport ("Walkthrough") √† la fin. Je n'ai plus qu'√† tester pour voir si √ßa marche, et √ßa marche.</p>
<p><b>20-11-2025</b></p>
<p>Je me rends compte que lorsqu'on utilise uv et notamment les "uv tools", on peut avoir 2 installations distinctes du m√™me logiciel sur sa machine. C'est par exemple le cas pour yt-dlp, que j'ai install√© comme outil dans uv tools en faisant <code>uv tool install yt-dlp</code> et que j'ai √©galement install√© via la commande plus classique <code>sudo snap install yt-dlp</code> sur Ubuntu. Probl√®me : quand je lance yt-dlp, comment savoir quelle version est utilis√©e ? R√©ponse : il faut taper <code>which yt-dlp</code>. Si cela renvoit qq ch comme <code>/usr/bin/yt-dlp</code>, c'est la version "system-wide" qui est utilis√©e par d√©faut. Si en revanche cela renvoie quelque chose comme <code>~/.local/bin/yt-dlp</code>, il est plus probable qu'il s'agisse d'une version install√©e via uv ou pip.</p>
<p>Avec Gemini CLI je cr√©e un outil bas√© sur yt-dlp qui r√©cup√®re toutes les vid√©os YouTube correspondant √† certains mots-cl√©s et les trie par ordre de "timestamp" (date et heure) de mise en ligne en v√©rifiant le temps √©coul√© entre chaque vid√©o. L'id√©e est (en th√©orie) de retrouver la premi√®re vid√©o mise en ligne pour un sujet donn√©. Le choix des mots cl√© est crucial. Par exemple avec les mots "google" et "antigravity" le script r√©cup√®re (√† 23h19) les m√©tadonn√©es de 336 vid√©os, dont <a href="https://www.youtube.com/watch?v=L8wEC6A5HQY">la plus ancienne</a> a √©t√© mise en ligne le 18-11-2025 √† 14:34:33. La vid√©o suivante r√©cup√©r√©e avec les m√™mes mots-cl√©s est plus ancienne de 851 heures (~ 35 jours), et n'appartient donc pas √† la "vague" actuelle concernant la sortie d'<a href="https://antigravity.google/">Antigravity</a> le 18 novembre.</p>
<p><b>18-11-2025</b></p>
<p>Aujourd'hui Olivier essaie de faire fonctionner le mod√®le Voxtral de Mistral en local sur sa machine. Sur la <a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507">page Hugging Face du mod√®le</a>, la doc pr√©conise d'utiliser un truc qui s'appelle vLLM. J'essaie avec Voxtral-Mini-3B-2507 (10 Go) sur Google Colab. Avec vLLM, √ßa ne marche pas, alors j'essaie avec Transformers et <a href="https://colab.research.google.com/drive/1WAvmXZXHE3Aa9quGjBHgcrCYR8ac1G1H?usp=sharing">√ßa fonctionne</a>. Je teste les √©chantillons de code "transcribe" et "multi-audio + text instruction". Je dois red√©marrer la session entre chaque essai autrement je sature la m√©moire de Colab. Je constate que le mod√®le est bien t√©l√©charg√© sur la machine (2 fichiers de 5 Go) lors du premier run, et que red√©marrer la session ne m'oblige pas √† tout re-t√©l√©charger.</p>
<p>Un peu plus tard, Olivier essaie avec le mod√®le "Voxtral-Small-24B-2507" (48 Go), et constate qu'il ne fonctionne pas avec son setup (GPU avec 24 Go de RAM). Je trouve l'info suivante : "Running Voxtral-Small-24B-2507 on GPU requires ~55 GB of GPU RAM in bf16 or fp16". Du coup Olivier d√©cide d'essayer <a href="https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF">une version quantis√©e</a> du m√™me mod√®le avec llama.cpp comme moteur d'inf√©rence et √ßa fonctionne (<code>llama-server -hf  bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF:Q6_K</code>). Mais il semblerait que la "quantisation" affecte tout de m√™me un peu les performances du mod√®le.</p>
<p>Google annonce la sortie de <a href="https://blog.google/technology/developers/gemini-3-developers/">Gemini 3</a> et d'<a href="https://antigravity.google/">Antigravity</a>. Antigravity est une sorte d'√©ni√®me m√©lange de VSCode et de Cursor. Je le teste avec deux scripts en Python que je souhaite combiner et √ßa marche tr√®s bien. Mais apparemment rien que je n'aurais pu faire dans Claude Code ou Gemini CLI. Un peu plus tard je le fais bosser avec Sonnet 4.5 sur une appli g√©n√©r√©e en TypeScript par Google AI Studio. Il la modifie et la rend compatible avec Android Studio et pr√™te √† √™tre export√©e en fichier APK installable sur un smartphone. Question mod√®les, on peut choisir entre Gemini 3, Claude Sonnet 4.5 et les mod√®les GPT-OSS d'OpenAI. Quelle journ√©e !</p>
<p><b>17-11-2025</b></p>
<p>Olivier me parle d'<a href="https://www.affinity.studio/">affinity</a> comme alternative √† InDesign pour ouvrir les fichiers IDML. Je t√©l√©charge l'appli et la teste. Elle permet sans probl√®me de visualiser in fichier IDML. Bon √† savoir.</p>
<p>D√©couverte de la <a href="https://www.youtube.com/@mattpocockuk/videos">cha√Æne YouTube</a> de Matt Pocock, un adepte de TypeScript. Ses conseils ou explications en mati√®re d'IA me semblent tr√®s claires et utiles et loin des "buzzeries" habituelles.</p>
<p>Le site https://models.dev/ liste tous les LLM et leurs "providers" avec les tarifs des API et autres informations utiles. Olivier y d√©couvre <a href="https://iflow.cn/">iflow.cn</a>, qui propose une API gratuite et de nombreux mod√®les. China taking over?</p>
<p>R√©flexion sur le module qui permet d'√©couter le texte de <a href="https://blog.sshh.io/p/how-i-use-every-claude-code-feature">cet excellent billet de blog</a> sur Claude Code. Je me demande si c'est une IA qui g√©n√®re la voix "√† la vol√©e" ou si c'est un fichier mp3 pr√©-enregistr√©. En fait il s'agit un peu des deux : c'est un mp3 (dont je finis par trouver le lien dans le code source) qui a certainement √©t√© g√©n√©r√© par une IA sur une version ant√©rieure du texte avant sa mise en ligne. En effet la version audio ne colle pas tout √† fait au texte de la page.</p>
<p><b>16-11-2025</b></p>
<p>Suite du "InDesign Challenge". Olivier me propose de tester le fichier IDML g√©n√©r√© hier. Il me renvoie un fichier PDF sans les images qui semble tenir la route. Entre temps j'ai eu une autre id√©e. Comme le r√©sultat de l'importation des fichiers JSON dans memoQ n'est pas id√©al (pr√©sence de nombreux √©l√©ments qui n'ont pas besoin d'√™tre traduits), je me dis que l'on pourrait g√©n√©rer un second fichier JSON plus simple √† partir du premier. Ce second fichier ne contiendrait que trois cl√©s : "path" suivi d'un identifiant, "paragraph_index" suivi de 0, 1, 2, etc. et "text" suivi de la cha√Æne de caract√®res √† traduire.</p>
<p>Je demande √† Claude de me construire un nouveau script pour cela. Ce nouveau script extrait uniquement l'information n√©cessaire √† la traduction et est capable de r√©-encapsuler le r√©sultat dans le fichier dont il est issu. C'est une histoire de poup√©es russes : IDML-source > JSON_1-source > JSON_2-source > traduction > JSON_2-cible > JSON_1-cible > IDML-cible. Apr√®s quelques aller-retour avec Claude tout semble fonctionner et Olivier teste IDML-cible et produit un nouveau fichier PDF valide.</p>
<p>Maintenant je me dis que je pourrais certainement supprimer une des "poup√©es" et obtenir le fichier JSON "l√©ger" directement √† partir du fichier IDML en demandant √† Claude Code ou Gemini CLI de bosser un peu sur mes 3 scripts. C'est ce que je fais avec Gemini CLI, qui fusionne 2 scripts en un seul "all-in-one".</p>
<p><b>15-11-2025</b></p>
<p>Un plugin int√©ressant de LLM est absent de <a href="https://llm.datasette.io/en/stable/plugins/directory.html">la liste officielle</a> : <code>llm-github-models</code>. Ce <a href="https://github.com/tonybaloney/llm-github-models">plugin</a> donne acc√®s √† ~62 mod√®les h√©berg√©s(?) par Github, dont plusieurs mod√®les d'OpenAI, comme par exemple <code>github/gpt-4.1</code>. Pour que √ßa fonctionne, il  faut au pr√©alable se cr√©er une cl√© sur https://github.com/settings/tokens.</p>
<p>Nouveau challenge : j'ai une BD √† traduire √† partir d'un fichier InDesign (.idml). L'importation de ce fichier dans memoQ fonctionne, mais la segmentation des phrases ne fonctionne pas. Je me retrouve avec des fragments de phrases et jamais les phrases compl√®tes.</p>
<p>J'explique le probl√®me √† Claude, qui me propose un script en Python qui convertit le fichier IDML en JSON, en faisant en sorte de "respecter" le texte. √áa semble marcher, m√™me si le texte est dupliqu√© dans 2 "cl√©s" diff√©rentes du fichier JSON. L'id√©e est de traduire le texte contenu dans le fichier JSON et de reconstruire le fichier IDML. Je fais des essais et √ßa semble marcher, mais je ne poss√®de malheureusement pas le logiciel InDesign pour v√©rifier que √ßa a VRAIMENT fonctionn√©.</p>
<p><b>14-11-2025</b></p>
<p>J'ai retrouv√© les logs de Claude Code pour la construction du plugin llm_hf (fichiers JSONL). Essentiellement 4 sessions du 2 au 4 novembre 2025. Leur conversion en HTML ne restitue malheureusement pas le rendu du terminal. Pour cela il faut copier tout le contenu brut du terminal juste avant de quitter la session et le coller dans un <a href="https://tools.simonwillison.net/terminal-to-html">convertisseur appropri√©</a>.</p>
<p>Cependant ces fichiers HTML (<a href="http://bytepacking.free.fr/files/2025-11-02_18-07-22.html">exemple</a>) contiennent √©norm√©ment d'information, beaucoup plus que ce que l'on voit √† l'√©cran lorsqu'on se sert de Claude Code. Il serait impossible de suivre en temps r√©el tout le d√©roul√© du processus. Prendre le temps d'inspecter ces "logs" convertis en HTML permet de prendre conscience de la quantit√© de tokens "ing√©r√©s" par la machine pour faire son travail.</p>
<p>Je teste Voxtral, le mod√®le Speech-to-Text de Mistral, en suivant <a href="https://simonwillison.net/tags/speech-to-text/">les instructions de Willison</a> pour voir si √ßa fonctionne toujours de la m√™me fa√ßon depuis juillet.<br>J'installe le plugin Mistral pour LLM :</p>
<p><pre><code>llm install -U llm-mistral</code></pre></p>
<p>Ensuite je tente un test avec un extrait d'une interview de Willison (fichier WAV) en laissant volontairement le fichier en local pour voir si √ßa fonctionne comme avec l'API de Groq par exemple :</p>
<p><pre><code>llm -m voxtral-small -a test.wav</code></pre></p>
<p>J'obtiens un message d'erreur :</p>
<p><pre><code>Error: This model does not support attachments of type 'audio/wav', only audio/mpeg</code></pre></p>
<p>Je convertis le fichier WAV en MP3 avec VLC et r√©essaie :</p>
<p><pre><code>llm -m voxtral-small -a test.mp3</code></pre></p>
<p>Nouvelle erreur :</p>
<p><pre><code>Error: No key found - add one using 'llm keys set mistral' or set the LLM_MISTRAL_KEY environment variable</code></pre></p>
<p>J'ai oubli√© d'entrer la cl√© API :</p>
<p><pre><code>llm keys set mistral
Enter key: *******</code></pre></p>
<p>Nouvel essai :</p>
<p><pre><code>llm -m voxtral-small -a test.mp3</code></pre></p>
<p>Nouvelle erreur :</p>
<p><pre><code>Error: Audio attachment must use a URL</code></pre></p>
<p>Ils auraient pu le dire plus t√¥t, mais bon cela confirme que l'API de Mistral ne fonctionne pas comme celle de Groq.<br>J'uploade mon fichier test.mp3 sur le serveur de ce blog et r√©essaie :</p>
<p><pre><code>llm -m voxtral-small -a http://bytepacking.free.fr/files/test.mp3</code></pre></p>
<p>Cette fois √ßa marche et j'obtiens <a href="http://bytepacking.free.fr/files/test.txt">une transcription</a> tr√®s fid√®le. Mais le fait de devoir mettre le fichier en ligne est quand m√™me tr√®s contraignant.</p>
<p>Je demande √† Gemini 2.5 Pro de transcrire le m√™me fichier (<code>Transcribe this audio file</code>) ce qu'il fait en quelques secondes. <a href="http://bytepacking.free.fr/files/test_gemini.txt">R√©sultat</a>.</p>
<p>Google Colab vs Github Codespaces</p>
<p>Le look de l'interface des notebooks de Google Colab a √©volu√© (en mieux je trouve) et ils ont ENFIN ajout√© un terminal. All√©luia. On peut par exemple installer uv en faisant :</p>
<p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></pre></p>
<p>Les Codespaces, quant √† eux sont une version de VSCode qui fonctionne dans une machine virtuelle dans le cloud. J'ai essay√© de m'en servir et je trouve l'interface assez fouillis. De plus, par rapport √† Colab, on est sur des machines beaucoup moins puissantes, sans GPU et avec moins de RAM et d'espace disque.</p>
<p>J'ai essay√© d'installer nvidia/parakeet-tdt-0.6b-v2 sur une VM, sans succ√®s (low RAM / Disc space). Sur Google Colab √ßa n'a pas march√© non plus, mais pour d'autres raisons (incompatibilit√© de librairies Python).</p>
<p><b>13-11-2025</b></p>
<p>Aujourd'hui je collecte et regroupe tous les fichiers logs des sessions effectu√©es avec Claude Code et Gemini CLI sur mes diff√©rents ordis depuis le 15 septembre. Les logs de CC sont des fichiers JSONL et ceux de Gemini CLI des fichiers JSON. Je demande √† Claude Web de me cr√©er un script pour chaque outil afin de convertir les logs en HTML, avec une mise en forme plus facile comprendre. Des outils existent en ligne mais ne me satisfont pas. La structure des logs de Claude Code semble plus complexe et foisonnante que celle de Gemini CLI. Gemini a le m√©rite de montrer les moments o√π le mod√®le "r√©fl√©chit", ce qui habituellement transparent pour l'utilisateur. Comme Gemini CLI est gratuit, je poss√®de beaucoup plus de logs "Gemini" que de logs "Claude".</p>
<p>Commande pour conna√Ætre la date d'installation de mon OS (Ubuntu) :<br><code>ls -lt /var/log/installer/ 2>/dev/null</code></p>
<p><b>11-11-2025</b></p>
<p>Retour √† Brest. Je demande √† Gemini-CLI de comparer une liste de 6 scripts de plugins officiels de LLM (openrouter, gemini, anthropic, perplexity, mistral, groq), liste √† laquelle j'ajoute mon plugin candidat "Hugging Face" (<code>llm-hf</code>). Je souhaite v√©rifier la mani√®re avec laquelle les plugins se comportent en mati√®re de connexion aux API.</p>
<p>Je me rends compte que mon plugin n'est pas le seul √† pouvoir utiliser une cl√© API stock√©e dans les variables d'environnement de l'OS alternativement au stockage de cette m√™me cl√© dans un fichier JSON de LLM, ce qui me rassure.</p>
<p>Gemini me r√©pond au sujet des API avant m√™me que je pose la question. Il a d√©tect√© que le script du plugin <code>llm-perplexity</code> est assez complexe (et donc plus sujet √† des pannes potentielles) et me propose de le simplifier, mais je lui demande de s'en tenir aux commentaires.</p>
<p><span class="code-box">A significant distinction [of the llm-perplexity plugin] is its use_openrouter option, which allows it to act as a proxy to the OpenRouter API. This feature is unique to the llm-perplexity plugin and is not shared by the other plugins. When this option is enabled, the plugin dynamically changes the API endpoint and model ID to route requests through OpenRouter, adding a layer of abstraction that is not present in the other plugins.</span></p>
<p>Concernant mon plugin :</p>
<p><span class="code-box">The llm_hf.py plugin is also unique, as it uses the OpenAI library to communicate with a Hugging Face endpoint that is compatible with the OpenAI API.</span></p>
<p>Vraiment incroyable de pouvoir comparer ainsi mon plugin candidat aux plugins officiels pour savoir si je suis √† peu pr√®s dans la norme. J'ai d√©j√† eu recours plusieurs fois √† cette m√©thode, qui semble efficace.</p>
<p>Il faut maintenant que je me penche sur les tests pre et post Github push. Un <code>uvx ruff check</code> d√©tecte 2 probl√®mes, que je corrige avec <code>uvx ruff check --fix</code>.</p>
<p><b>09-11-2025</b></p>
<p>Les nouveaux outils en ligne de commande continuent d'occuper les esprits.</p>
<p>Simon Willison trouve le moyen de "hacker" Codex CLI afin de tester GPT-5-Codex-Mini, un nouveau mod√®le d'OpenAI disponible uniquement via Codex CLI, √©crit en Rust, et non via leur API. De son propre aveu, <a href="https://simonwillison.net/2025/Nov/9/gpt-5-codex-mini/">sa man≈ìuvre</a> est un peu "gonfl√©e".</p>
<p>D√©couverte, gr√¢ce √† un commentaire de <a href="https://www.youtube.com/watch?v=97FYys-kj58">cette vid√©o</a>, de l'existence de <a href="https://docs.z.ai/devpack/overview">GLM</a>, un mod√®le chinois compatible avec les CLI tools comme Claude Code ou Gemini CLI.</p>
<p><b>08-11-2025</b></p>
<p>J'ai trouv√© un moyen de continuer de mettre √† jour ce blog √† partir de mon smartphone. J'ai install√© l'application termux, qui √©mule un environnement Linux, avec un terminal classique en bash. La commande <code>termux-setup-storage</code> permet d'avoir acc√®s au syst√®me de fichiers du smartphone. J'ai ensuite install√© uv (<code>pkg install uv</code>), qui me permet de lancer <code>logbook_ftp.py</code>, le script en Python qui g√©n√®re et met en ligne la page HTML de ce blog √† partir d'un fichier.txt en faisant <code>uv run logbook_ftp.py ai_stream.txt</code>.</p>
<p><b>07-11-2025</b></p>
<p>Ai chang√© la police de caract√®res de ce blog dans la partie du code d√©di√©e aux tablettes et smartphones : <a href="https://fonts.googleapis.com/css2?family=Roboto+Flex:wght@300;400;500;600">Roboto Flex</a>.</p>
<p>Toujours int√©ressant de voir comment les d√©veloppeurs de m√©tier utilisent les outils comme Claude Code. En voici <a href="https://www.youtube.com/watch?v=kZ-zzHVUrO4">un exemple</a>.</p>
<p>Je quitte la Bretagne pour la Normandie sous des trombes d'eau. Dans la voiture, j'√©coute avec beaucoup d'attention et de plaisir un <a href="https://www.heavybit.com/library/podcasts/generationship/ep-39-simon-willison-i-coined-prompt-injection">podcast</a> de Simon Willison o√π il est bien entendu question d'IA toutes les deux secondes.</p>
<p><b>06-11-2025</b></p>
<p>J'installe Cursor 2.0 et lui demande de me cr√©er un "docx proofreader". C'est l√† qu'on voit que les d√©mos de certains youtubeurs ne valent pas grand chose. Quand on demande quelque chose d'un tant soit peu complexe, c'est une autre paire de manches. Il faut dire que je ne commence pas par le plus facile. Je poursuis le projet avec Gemini-CLI, qui parvient √† faire fonctionner le truc. Mais le r√©sultat, tr√®s d√©cevant, comme tant d'autres, part √† la poubelle.</p>
<p>J'essaie d'am√©liorer mon plugin llm-hf et demande √† Claude Sonnet 4.5 (version web) de le comparer √† llm-groq, un plugin "production-ready" :</p>
<p><span class="code-box">Considering llm_groq.py as "production-ready", can llm_hf.py be further improved?</span></p>
<p>Claude me fait un rapport tr√®s complet dans lequel il souligne que le point d'am√©lioration le plus important est le "Model Caching", qui est absent de mon script.</p>
<p><span class="code-box">llm_groq.py caches models in a JSON file (groq_models.json), avoiding API calls on every invocation. llm_hf.py fetches from the API each time, which is slow and wasteful.</span></p>
<p>Claude me propose de produire une version am√©lior√©e, que je t√©l√©charge et qui fonctionne tout de suite. Je la met sur Github.</p>
<p>Apr√®s le temps pass√© avec les outils en ligne de commande, le retour √† l'interface web, gratuite de surcro√Æt, est tr√®s agr√©able. Impression de revenir en terrain connu. Cela pose une question br√ªlante : aurais-je pu d√©velopper le plugin SANS le recours aux CLI-tools? Probablement, mais j'aurais mis beaucoup plus de temps.</p>
<p>Olivier me parle de <a href="https://github.com/musistudio/claude-code-router">claude-code-router</a> qui permet d'utiliser CC avec n'importe quel mod√®le.</p>
<p><b>05-11-2025</b></p>
<p>Utiliser Claude Code en mode API co√ªte cher, tr√®s cher. En 7 jours j'ai consomm√© quasiment 10 dollars (pay√©s 12 √† cause de taxes ajout√©es √† chaque transaction). Sur mon relev√© bancaire, j'ai √©t√© factur√© 10,63 EUR. Cela fait une consommation moyenne quotidienne de 1,50 EUR. De plus, leur console d'administration est une vraie usine √† gaz, j'ai eu un mal de chien √† afficher ma consommation depuis le d√©but de mes paiements, le 30 octobre. La plupart des "vues" indiquent des sommes d√©risoires. Je ne sais m√™me plus comment j'ai fini par trouver <a href="https://console.anthropic.com/cost?start_date=2025-10-30">la bonne vue</a>. Visiblement le mode API n'est pas la bonne solution pour utiliser Claude Code. J'en suis √† me demander si les sommes affich√©es √† la fin de chaque session sont correctes. Je vais peut-√™tre regarder du c√¥t√© d'OpenCode avec un mod√®le pas trop cher (DeepSeek, comme le sugg√©rait Olivier), mais √ßa risque de ne pas √™tre aussi performant que CC. Notons que le gros de la consommation est le fait de "tokens entrants" (input tokens). En une semaine, j'en ai consomm√© 17,3 millions !</p>
<p>Je refais une session avec CC. Comme il me reste quelque chose comme 19 cents, je s√©lectionne Haiku comme mod√®le par d√©faut. A la fin de la session je vais voir combien de cr√©dits il me reste. La premi√®re chose que je constate c'est que je suis d√©biteur de 9 cents ! ‚òπÔ∏è Je croyais na√Øvement que le truc s'arr√™terait automatiquement √† 0,00 mais non. Autre chose : CC a quand m√™me un peu utilis√© Sonnet alors que je lui avais dit de n'utiliser que Haiku. C'est une toute petite somme ($0.0059), mais quand m√™me. Ensuite, quand on utilise CC, Anthropic cr√©e une "sous-cl√© API" √† chaque fois qu'on installe CC sur une machine diff√©rente. Sur ma console d'administration je vois que j'ai ma cl√© API principale (mauve) et 4 cl√© CC (oranges). Ce qui est bizarre, c'est que si j'additionne la consommation de chaque cl√©, j'arrive √† un total de 7,44 dollars et non pas 10,09 comme on pourrait s'y attendre ! WTF!? OK, il y a peut-√™tre un temps de latence avant d'avoir "the full picture", mais tout √ßa est quand m√™me assez opaque. üò§</p>
<p>Je poursuis avec OpenCode et leur mod√®le d√©di√© (Zen ?), le seul qui veuille bien fonctionner. Je pars de presque rien (les Skills et les d√©finitions des t√¢ches linguistiques) et n'arrive √† rien de potable. OpenCode semble √† nouveau √™tre en "YOLO mode" par defaut et je le trouve beaucoup moins intuitif que les autres (on ne peut pas faire r√©f√©rence √† un fichier en faisant @fichier par exemple).</p>
<p>Lass√© par les outils CLI, je retourne sur le Claude Web classique et parviens √† g√©n√©rer et √† peaufiner un script qui fonctionne pas si mal au bout de 5 it√©rations. L'utilisateur choisit un mod√®le de Groq et le script lance la r√©vision (copy-editing) du texte et le sauvegarde en incluant dans le nom de fichier le nom du LLM utilis√©. Il est ainsi plus facile de comparer les capacit√©s des diff√©rents mod√®les en mati√®re de correction-r√©vision. En plus √ßa marche avec Word, SANS les Claude Skills, dont je commence √† douter du bien fond√©.</p>
<p>Je teste le plugin "llm-huggingface-plugin" dont je parlais hier et constate qu'il ne fonctionne pas correctement. Le plugin ne r√©cup√®re aucune liste de mod√®les, la fonctionalit√© "register_commands" dans "hooks" est absente, et quand je tente de lancer <code>llm -m hf/meta-llama/Llama-3.2-3B-Instruct "Write a haiku about coding"</code>, j'obtiens un message d'erreur. Il y a de l'espoir !</p>
<p><b>04-11-2025</b></p>
<p>J'ai rendu le d√©p√¥t Github de mon plugin "llm-hf" public : https://github.com/sebington/llm-hf. Je n'arr√™te pas de modifier le fichier README.md, il y a toujours quelque chose √† changer.</p>
<p>Coup de th√©√¢tre. En me familiarisant avec l'interface de Github et en explorant la partie "Pull Requests" du projet <a href="https://llm.datasette.io/">LLM</a>, je me rends compte que quelqu'un a d√©j√† d√©velopp√© un <a href="https://github.com/SecKatie/llm-huggingface-plugin">plugin similaire</a> il y a 3 mois. Cependant je constate que le projet semble un peu oubli√© et que pour l'instant Simon n'a pas jug√© utile de l'ajouter √† la liste officielle des plugins de LLM. Cela n'enl√®ve rien √† mes efforts et me conforte dans l‚Äôid√©e que le fait d'avoir un objectif bien d√©fini est une des seules fa√ßons de progresser.</p>
<p>Suite √† une remarque d'Olivier, j'ajoute la fonctionnalit√© "register_commands" dans "hooks". Pour ce faire je demande √† Claude Code d'examiner le script d'un plugin similaire (llm-groq) et d'en extraire l'information pertinente. Cette fois je ne d√©pose pas le script dans le dossier du projet, je me contente de lui donner l'URL et CC va lui-m√™me lire le script en ligne.</p>
<p>Maintenant, quand on tape <code>llm plugins</code>, on obtient (entre autres) :</p>
<p><pre><code>[
  {
    "name": "llm-hf",
    "hooks": [
      "register_commands",
      "register_models"
    ],
    "version": "0.1.0"
  }
]</code></pre></p>
<p>Et quand on tape <code>llm hf</code>, on obtient d√©sormais :</p>
<p><pre><code>Usage: llm hf [OPTIONS] COMMAND [ARGS]...</code></pre></p>
<p><pre><code>  Commands relating to the llm-hf plugin</code></pre></p>
<p><pre><code>Options:
  -h, --help  Show this message and exit.</code></pre></p>
<p><pre><code>Commands:
  models   List all available Hugging Face models
  refresh  Refresh the list of available Hugging Face models</code></pre></p>
<p>Claude met syst√©matiquement √† jour <code>CLAUDE.md</code> et je lui demande de modifier <code>README.md</code> pour que la doc le soit √©galement.</p>
<p>En revanche, grosse prise de t√™te avec la mise √† jour des fichiers sur Github. Je ne ma√Ætrise pas assez Git. Je fais tout en d√©pit du bon sens, parfois en ligne de commande, parfois avec VSCode, parfois en modifiant les fichiers directement sur Github. C'est le bordel et je perds un temps fou avec √ßa, ce qui est nul. Je sais que CC poss√®de une fonctionnalit√© pour g√©rer Git mais je n'ai pas encore essay√© de m'en servir. Ce serait peut-√™tre plus fluide.</p>
<p>Last but not least, je constate que le mod√®le <code>swiss-ai/Apertus-8B-Instruct-2509</code> ne fonctionne pas.</p>
<p><b>03-11-2025</b></p>
<p>Je dois modifier mon plugin (renomm√© "llm-hf") pour le faire fonctionner comme les autres plugins de <a href="https://llm.datasette.io/">LLM</a>. Il y avait notamment un probl√®me avec la gestion de la cl√© API (ou du "User Access Token") de Hugging Face. Stock√© dans les variables d'environnement tout fonctionne bien. Mais les plugins de LLM proposent √©galement un autre moyen de stocker la cl√©, notamment quand l'utilisateur tape <code>llm keys set nom_du_plugin</code>. Dans ce cas la cl√© est stock√©e dans un fichier JSON. Ce probl√®me est maintenant r√©gl√© et tout semble bien fonctionner.</p>
<p>Je me familiarise avec la gestion d'un d√©p√¥t Github. J'apprends et mets en place les choses petit √† petit. Par exemple : modification des infos sur le fichier <code>README.md</code> avec VSCode ou suppression des fichiers ajout√©s √† <code>.gitignore</code> avec :</p>
<p><pre><code>git rm -r --cached .
git add .
git commit -m "Remove ignored files and update .gitignore"
git push</code></pre></p>
<p><b>02-11-2025</b></p>
<p>Simon Willison recommande <a href="https://blog.sshh.io/p/how-i-use-every-claude-code-feature">cet article</a> sur les multiples fonctionnalit√©s de Claude Code. Le truc se veut assez complet, donc √† garder sous le coude comme r√©f√©rence. Pratique.</p>
<p>Le MCP (Model Context Protocol) n'est peut-√™tre pas (encore) la panac√©e. Dans <a href="https://www.youtube.com/watch?v=Cw2txKfAqK4">cette vid√©o</a>, l'intervenant souligne que bien que le MCP vise √† permettre √† l‚ÄôIA d‚Äôex√©cuter des t√¢ches complexes, son efficacit√© r√©elle reste limit√©e par les contraintes fondamentales des LLM, notamment la taille de leur fen√™tre contextuelle. √Ä mesure que les informations fournies au LLM augmentent (lecture de documents, traitement de donn√©es, ex√©cution de r√©sultats d‚Äôoutils), la fen√™tre contextuelle du mod√®le se remplit rapidement, ce qui entra√Æne une d√©gradation des performances et un risque accru d‚Äôerreurs ou d'hallucinations. L'intervenant conclut en soulignant l‚Äôimportance √©mergente du "context engineering" comme solution potentielle pour am√©liorer les syst√®mes agentiques et aider le MCP √† tenir ses promesses.</p>
<p>J'essaie de r√©parer mon appli "proofreader" (version Groq) avec Gemini-CLI mais il ne s'en sort pas, alors je change de strat√©gie. Je lui demande de conserver toutes les fonctionnalit√©s mais de tout "porter" en Python. Il s'ex√©cute, et apr√®s quelques aller-retour, √ßa finit par fonctionner √† peu pr√®s. En tous cas c'est beaucoup plus rapide qu'avec la version en React / TypeScript. Les changements sont bien visibles sur le fichier Word. Projet √† poursuivre et √† am√©liorer. Je fais un push vers Github avec VSCode.</p>
<p>Je retourne au projet consistant √† cr√©er un plugin "Hugging Face" pour l'outil <a href="https://llm.datasette.io/">LLM</a> de Willison. J'avais commenc√© une √©bauche avec Gemini-CLI mais je crois que je lui avais donn√© trop d'informations. J'avais en effet r√©cup√©r√© plusieurs "vrais" plugins sur Github et les avais inclus dans le projet comme exemples. Je lui avais aussi donn√© toute la doc de "LLM" sous la forme d'un gros PDF de 140 pages. Bonjour la consommation de tokens. Je r√©duis donc drastiquement la voilure et vire quasiment tout pour ne garder essentiellement que 3 fichiers :<br>1. le contenu de <a href="https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html">Developing a model plugin</a> de la doc de LLM ;<br>2. le contenu de <a href="https://huggingface.co/docs/inference-providers/index">Inference Providers</a> de Hugging Face (que j'ai sauvegard√© en markdown directement depuis la version en ligne avec le bouton "copy page" en haut √† droit, super pratique) ;<br>3. le fichier GEMINI.md</p>
<p>Je modifie GEMINI.md, dans lequel je r√©explique clairement l'objectif du projet. Puis je le renomme en CLAUDE.md. J'installe Claude Code et le lance. Session incroyable pendant laquelle je sens les "super-pouvoirs" dont parlait NetworkChuck ! ü§© Un peu fascin√©, je regarde CC bosser et le guide quand il le faut. Au bout de 30 minutes j'ai un plugin fonctionnel. Je le teste et constate que la liste des mod√®les renvoy√©e n'est pas la bonne (trop courte et ne proposant que des "vieux" mod√®les). Je retrouve une fonction sens√©e r√©cup√©rer la "bonne" liste de mod√®les HF (il y en a normalement 118) et la donne √† CC, qui repart au quart de tour, int√®gre ce nouveau code, effectue des tests et met tout le projet √† jour. Vingt minutes suppl√©mentaires et c'est boucl√©. J'obtiens un plugin install√© et test√©, avec toute la doc mise √† jour, pratiquement pr√™t √† √™tre d√©ploy√© sur Github. Un truc de ouf. Et cette fois √ßa ne me co√ªte que 43 cents ($0.4334).</p>
<p><b>01-11-2025</b></p>
<p>Je visionne <a href="https://www.youtube.com/watch?v=MsQACpcuTkU">cette vid√©o</a>. Le type est vraiment le youtubeur professionnel typique, √† la fois insupportable et captivant. Sa vid√©o au montage aff√ªt√© comme un rasoir est un feu d'artifice des possibilit√©s offertes par les nouveaux outils "CLI" comme Claude Code ou Gemini CLI. Mais on apprend des choses. Il montre par exemple comment cr√©er des agents avec CC et qu'il est possible de faire bosser plusieurs IA sur un m√™me projet, en m√™me temps. Seul souci avec ce genre de d√©monstration : on ne sait pas de quel abonnement dispose ce youtubeur, qui est forc√©ment un peu un influenceur. Il b√©n√©ficie peut-√™tre de cr√©dits gratuits de la part d'OpenAI ou d'Anthropic, vu la publicit√© qu'il leur fait.</p>
<p>Ai mis en ligne une copie de ce blog sur https://sebington.github.io/ avec l'outil Source Control de VSCode. L'ennui avec Github, c'est qu'√† chaque fois il faut attendre un peu pour que les changements soient pris en compte, alors qu'en FTP classique, c'est instantan√©. Pour l'instant http://bytepacking.free.fr/ reste le plus fr√©quemment mis √† jour. J'ai vu qu'il est normalement possible de s√©curiser le site, mais la proc√©dure est assez compliqu√©e. Si free pouvait se bouger un peu sur ce sujet ce serait cool.</p>
<p>Je continue de travailler avec Gemini-CLI et am√©liore ma compr√©hension de l'outil. Ai retravaill√© mon projet d'analyse de cha√Ænes Youtube (renomm√© yt-trend). Les deux scripts ont √©t√© am√©lior√©s : "retrieve.py" est beaucoup plus rapide et "analyze.py" peut maintenant se connecter √† une liste impressionante de mod√®les par le biais de 5 fournisseurs (Mistral, Groq, OpenRouter, Hugging Face et Claude). Pour OpenRouter, seuls les mod√®les gratuits sont affich√©s. J'essaie depuis peu de "pousser" ce qui fonctionne sur Github (en d√©p√¥t priv√©) pour m'y retrouver. J'adore Gemini-CLI, qui est vraiment super fluide. Je le pr√©f√®re √† Claude Code pour l'instant.</p>
<p><b>31-10-2025</b></p>
<p>Je suis un √¢ne. Ce matin, la t√™te (relativement) repos√©e, je lis sur mon t√©l√©phone <a href="https://simonwillison.net/2025/Oct/16/claude-skills/">cet excellent billet</a> de Simon Willison sur les "Claude Skills", dont j'avais report√© la lecture pour je ne sais quelle raison, et me rends compte qu'il r√©pond pr√©cis√©ment √† tous mes questionnements. Il dit notamment :</p>
<p>"Something else I love about the design of skills is there is nothing at all preventing them from being used with other models. You can grab a skills folder right now, point Codex CLI or Gemini CLI at it and say ‚Äúread pdf/SKILL.md and then create me a PDF describing this project‚Äù and it will work, despite those tools and models having no baked in knowledge of the skills system.",</p>
<p>ce qui me rassure, puisque c'est ce que je fais avec Gemini CLI depuis la sortie des Skills.</p>
<p>Nouvelle fonctionnalit√© sur le script de g√©n√©ration de cette page : la possibilit√© de cr√©er un hyperlien en utilisant la syntaxe markdown, c'est √† dire [lien]+(URL) (sans le signe +). La nouvelle fonction cr√©√©e par Claude fait une fois de plus appel √† un regex, qui rep√®re la structure "crochets + parenth√®ses" et la transforme en lien hypertexte. L'utilisation de regex assure entre autre une g√©n√©ration ultra-rapide de la page.</p>
<p>Tentative de r√©vision d'un texte en fran√ßais bourr√© de maladresses et de fautes avec <a href="https://www.claude.com/product/claude-code">Claude Code</a>. Je commence avec Sonnet, puis passe √† Haiku pour √©conomiser mes cr√©dits. Dans un dossier je pr√©pare un fichier README.md avec toutes les infos utiles pour CC (t√¢ches, outils, environnement, etc.). Je mets aussi le contenu du dossier "docx/" des Claude Skills avec des instructions d'usage. Le r√©sultat est d√©cevant. Le texte n'a pas assez √©t√© retravaill√© et de grossi√®res erreurs persistent. Mon appli sur Google AI Studio fonctionne beaucoup mieux.</p>
<p><pre><code>Total cost:            $0.4781
Total duration (API):  3m 46s
Total duration (wall): 10m 14s
Total code changes:    360 lines added, 17 lines removed
Usage by model:
claude-haiku:  852 input, 8.9k output, 2.1m cache read, 62.0k cache write ($0.3280)
claude-sonnet:  818 input, 3.1k output, 120.8k cache read, 17.2k cache write ($0.1500)</code></pre></p>
<p>M√™me tentative avec <a href="https://geminicli.com/">Gemini-CLI</a> qui d√©cide de me l√¢cher juste apr√®s avoir analys√© le projet. Message : <code>API Error: code: 429, Resource exhausted. Please try again later.</code></p>
<p>Du coup je modifie mon appli dans Google AI Studio. L'utilisateur peut maintenant choisir entre 3 t√¢ches linguistiques : "revision", "copy-editing" et proofreading". Le r√©sultat n'est pas mal, Gemini propose 41 am√©liorations qui se justifient. En revanche il laisse une belle faute de grammaire : "...la ville a v√©cue une p√©riode sombre..." et des double ou triple espaces blancs. A faire : modifier cette nouvelle version de l'appli pour qu'elle fonctionne en local et qu'elle se connecte √† un LLM puissant et rapide (Groq?).</p>
<p>NB : Ces efforts ont pour but de manipuler des textes en conservant leur format d'origine, l'ind√©boulonnable MS Word. Concernant les aspects purement linguistiques, un petit passage par ChatGPT avec le texte brut (.txt) donne de bien meilleurs r√©sultats que tous les essais qui pr√©c√®dent. C'est bien ce qui est rageant dans le m√©tier de traducteur : l'encapsulation du texte nuit √† son traitement.</p>
<p>Je soumets le m√™me texte (.docx) aux versions web gratuites de ChatGPT et de Claude pour correction-r√©vision. Tous deux r√©cup√®rent et corrigent le texte, mais aucun ne me livre un document Word : "lien mort" pour ChatGPT et "impossibilit√© technique" chez Claude. Olivier soumet le m√™me texte √† la version "Pro" de Claude et finit par obtenir un document Word r√©vis√©, mais fragile techniquement et linguistiquement.</p>
<p>Concernant les deux documents r√©vis√©s issus des version gratuites de ChatGPT et de Claude, on peut les comparer √† posteriori √† l'original et pour chacun g√©n√©rer une version du document o√π l'on voit les modifications effectu√©es. A premi√®re vue ChatGPT semble avoir fait le meilleur travail, mais Claude n'est pas tr√®s loin derri√®re.</p>
<p>Statistiques<br>Claude : 209 r√©visions (86 insertions, 89 suppressions, 34 mises en forme)<br>ChatGPT : 308 r√©visions (131 insertions, 127 suppressions, 50 mises en forme)</p>
<p><b>30-10-2025</b></p>
<p>Je me rends compte que je confondais node.js, nvm et npm. üòè ChatGPT me fait un petit r√©sum√© explicatif :</p>
<p><pre><code>- Node.js is the core runtime that runs JavaScript on your system.
- NPM is automatically installed with Node.js and manages the dependencies your project needs.
- NVM sits above both ‚Äî it manages which version of Node.js (and its bundled npm) you are using.</code></pre></p>
<p>Site de r√©f√©rence : https://nodejs.org/en/download.</p>
<p>Surprises, paradoxes et d√©ceptions :</p>
<p>1. Je fais une session compl√®te d'environ 15 min avec <a href="https://opencode.ai/">opencode</a> et un mod√®le gratuit d'OpenRouter (MiniMax M2). Le r√©sultat n'est pas foufou. A noter : opencode semble √™tre en mode YOLO par d√©faut. Il prend toutes les d√©cisions d'ex√©cuter le code tout seul sans me demander mon avis !üòÆ</p>
<p>2. J'encha√Æne une session similaire avec Gemini-CLI et √† peine a-t-il commenc√© qu'il cale :</p>
<p><pre><code>‚ö° Automatically switching from gemini-2.5-pro to gemini-2.5-flash for faster responses for the remainder of this session.
‚ö° Possible reasons for this are that you have received multiple consecutive capacity errors or you have reached your daily gemini-2.5-pro quota limit
‚ö° To increase your limits, upgrade to a Gemini Code Assist Standard or Enterprise plan with higher limits at https://goo.gle/set-up-gemini-code-assist
‚ö° Or you can utilize a Gemini API Key. See: https://goo.gle/gemini-cli-docs-auth#gemini-api-key
‚ö° You can switch authentication methods by typing /auth
‚úï [API Error: Please submit a new query to continue with the Flash model.]</code></pre></p>
<p>alors que je ne l'ai pas du tout utilis√© aujourd'hui. O√π sont mes tokens gratuits ?? ü§î</p>
<p>3. J'ach√®te 5 dollars (factur√©s 6) de cr√©dits sur Claude Console pour pouvoir utiliser Claude Code (CC). Apr√®s avoir saisi mon num√©ro de carte bleue et mon adresse et tandis que je r√©fl√©chis encore √† la somme que je veux mettre, le processus en ligne "d√©cide" soudain de valider tout seul la transaction alors que je n'ai cliqu√© sur aucun bouton "Valider", WTF!! M√©thode pour le moins "cavali√®re" de la part d'Anthropic je trouve.</p>
<p>Je lance ensuite ma premi√®re session avec CC. Elle se passe pas trop mal mais le r√©sultat n'est pas au rendez-vous et je constate qu'elle m'a co√ªt√© $1.33 ! Je v√©rifie le mod√®le par d√©faut : il s'agit bien de Sonnet 4.5 comme je m'y attendais. Je suis n√©anmoins surpris apr√®s avoir entendu maintes et maintes fois Simon Willison d√©voiler les sommes ridicules d√©pens√©es sur la g√©n√©ration de ses p√©licans √† bicyclette. Cela confirme ce dont personne ne semble parler dans les nombreuses vid√©os sur YT concernant ces outils : ils sont un moyen de nous faire consommer du token √† la pelle. Le g√©n√©reux free-tiers de Gemini-CLI n'est pas si mal apr√®s tout.</p>
<p>R√©flexion sur Claude Code et Gemini CLI (r√©sum√© d'un √©change entre Olivier et moi)</p>
<p>Info importante concernant l'abonnement "PRO" de Claude : il ne permet pas d'utiliser l'API de Claude, contrairement l'abonnement "MAX". On ne peut donc pas lancer de scripts autonomes, mais on peut toujours faire appel √† d'autres LLM pour cela.</p>
<p>Concernant les "Skills" pas mal de choses sont encore brumeuses dans mon esprit. Je me demande si CC y a acc√®s par d√©faut ou s'il faut les lui donner en usage local, comme je le fais avec Gemini-CLI. Officiellement, pour b√©n√©ficier des "Skills", il faut les installer via un plugin dispo sur "anthropics/skills marketplace". Ensuite Claude les charge automatiquement quand il en a besoin. On peut aussi les installer "manuellement" en les ajoutant √† ~/.claude/skills (pas tr√®s clair). Donc les lui donner en local reste possible, √† condition de les avoir sous la main ou de savoir o√π les trouver.</p>
<p>Ceci dit les "Skills" permettent de faire toutes sortes de manipulations de fichiers (Word ou PDF par ex.) qui √©taient impensables il y a 6 mois et qui sont super utiles dans mon boulot. Je me demande si l'utilisation que j'en ai faite dans Gemini-CLI est analogue √† leur utilisation "native" dans Claude Web ou CC. En tous cas quand je regarde le d√©tail des actions de Gemini-CLI lors d'une session, manifestement il s'en sert.</p>
<p>Olivier donne une pr√©cision importante : ce qui caract√©rise les Skills c'est la gestion du contexte : Claude lit le d√©but de chaque fichier SKILL.md  (champs "name" et "description") et ne lira la suite que si cela est n√©cessaire pour la t√¢che en cours. Cela permet d'√©conomiser des tokens. Tandis qu'il est probable que Gemini-CLI int√®gre 100 % des infos contenues dans les Claude Skills dans son contexte, ce qui consomme davantage de tokens.</p>
<p>Je viens de faire une nouvelle session avec CC et j'ai maintenant d√©pens√© 2 dollars sur 5 ! En gros on est √† un dollar par session. Je vais essayer d'utiliser ce qui reste sur des t√¢ches plus √©conomes en API, en lan√ßant des scripts autonomes par exemple.</p>
<p><b>29-10-2025</b></p>
<p>"Rainy day, dream away, let the sun take a holiday..." (Jimi Hendrix)</p>
<p>Je continue de tester Gemini-CLI et lui demande non pas de "construire un outil qui va faire", mais de "faire lui m√™me" (je ne sais pas si √ßa a du sens), en l‚Äôoccurrence r√©viser un texte au format Word (.docx) et produire une version r√©vis√©e du document en mode "suivi des modifications". Je lui donne acc√®s aux outils "Claude Skills". J'initialise le dossier avec uv pour qu'il puisse ex√©cuter du Python. Et apr√®s l'avoir fait un <code>/init</code>, lui donne le prompt suivant :</p>
<p><span class="code-box">Do a professional copy editing of @doc_en.docx. You can decide to use the tools provided in @tools/ or not. Output the result in a .docx file in tracked changes. The project has been uv initialized, you can use 'uv add ...' or 'uv run ...' if you need to execute Python. Reminder: Copy editing is a detailed linguistic and stylistic review focusing on grammar, syntax, punctuation, spelling, and consistency of tone, terminology, and formatting. It ensures clarity, precision, and adherence to stylistic or editorial standards without altering the substance of the content.</span></p>
<p>Les r√©sultats ne sont pas tr√®s concluants. Gemini veut se servir de pandoc mais il bute sur l'installation malgr√© mon guidage. J'aurais d√ª lire le fichier GEMINI.md apr√®s sa cr√©ation et installer pandoc au pr√©alable. Il parvient tout de m√™me √† travailler sur le document mais soudain s'emballe quand il se lance dans une r√©vision sans fin du xml interne au doc. Je l'arr√™te avant qu'il ne grille tous ses tokens et finis par le faire travailler sur le m√™me document en TXT, ce qu'il fait, mais le r√©sultat est d√©cevant. Mon appli cr√©√©e avec Google AI Studio fonctionne beaucoup mieux, sans √™tre extraordinaire. A suivre.</p>
<p>Mon coll√®gue JB me demande un service. Il voudrait pouvoir identifier la langue d'une collection de cha√Ænes de caract√®res dans une base de donn√©es. Il voudrait utiliser l'API de DeepL, lui envoyer la cha√Æne de caract√®res et r√©cup√©rer le code RFC 5646 de la langue ('en', 'fr', etc.). Apr√®s quelques tests, je me souviens que DeepL ne propose pas le breton dans sa biblioth√®que de langues. Or JB a absolument besoin de pouvoir identifier du breton.</p>
<p>Je propose alors d'utiliser un LLM au lieu de DeepL. La demande de JB est un cas parfait de "Structured Outputs". On fait une requ√™te bien pr√©cise au LLM et on filtre sa r√©ponse pour ne r√©cup√©rer que l'info dont on a besoin. Je consulte la doc de Groq et construis l'outil en quelques requ√™tes √† Claude. √áa marche tout de suite. Le script boucle sur les donn√©es d'un fichier .tsv, envoie une cha√Æne de caract√®res √† <code>llama-3.3-70b-versatile</code> sur l'API de Groq avec un prompt bien pr√©cis, r√©cup√®re la langue et son code et stocke le tout dans un fichier JSON.</p>
<p><pre><code>  {
    "row": 3,
    "text": "[The Eisteddfod. A Great reunion]",
    "language": "English",
    "code": "en"
  },
  {
    "row": 4,
    "text": "[Lettre de Paul Diverr√®s √† Yves Berthou]",
    "language": "French",
    "code": "fr"
  }, etc.</code></pre></p>
<p>Apr√®s quelques essais nous changeons de mod√®le pour <code>moonshotai/kimi-k2-instruct-0905</code>, qui semble donner de meilleurs r√©sultats. JB me demande aussi de simplifier et r√©duire le script √† sa seule fonction : cha√Æne de caract√®res en entr√©e et code RFC 5646 en sortie, ce que Claude fait en quelques secondes. Je suis tr√®s fatigu√© et quasiment dans un √©tat second, mais tr√®s content d'avoir pu travailler sur ce probl√®me et d'avoir trouv√© une solution viable. Je ne sais pas si JB, un peu r√©ticent aux IA, va finir par utiliser cette solution mais au moins j'aurais mis en ≈ìuvre mes maigres comp√©tences sur un "vrai projet", et cela m'emplit de joie.üòÑ<br>Script : https://gist.github.com/sebington/382e2c6ebd43a28453c3c2541d4b241f</p>
<p>Je continue d'utiliser Linux Manjaro et parviens, √† grand renfort de Claude, √† localiser Freetube et √† l'installer avec yay (<code>yay -S freetube</code>). Je constate toutefois que la recherche de "freetube" avec pacman (<code>pacman -Ss freetube</code>) ne renvoit rien. Claude me propose plusieurs solutions, par exemple de faire (<code>pacman -Ss | grep -i freetube</code>) ou m√™me d'installer fzf (fuzzy search) pour le coupler √† pacman, mais √ßa ne donne rien. C'est finalement la recherche directement avec yay (<code>yay freetube</code>), qui me permet de localiser freetube. Bref je me suis probablement trop focalis√© sur pacman alors que yay semble √™tre plus versatile et intuitif.</p>
<p>D√©couverte (re-d√©couverte en fait) d'un merveilleux site qui propose toutes sorte de g√©n√©rateurs de bruits reproduisant au besoin toutes les ambiances souhait√©es : https://mynoise.net/. Excellent.</p>
<p><b>28-10-2025</b></p>
<p>J'ai ajout√© un formatage sp√©cial pour les prompts dans le code de g√©n√©ration de cette page (rectangle aux bords arrondis). Je teste diff√©rentes polices et couleurs.</p>
<p>J'essaie de reproduire et de lancer un script en Python qui construit et teste un r√©seau de neurones (NN) tr√®s simple comportant une seule couche de 32 neurones. Ce NN g√©n√®re des donn√©es al√©atoires avec du bruit, puis analyse ces donn√©es pour retrouver la fonction "cach√©e" derri√®re. Source : https://www.youtube.com/watch?v=rdFjMb6dTJQ. Je pars de la vid√©o et fais des captures d'√©cran du code, que je colle dans Claude et dans Mistral avec le prompt : <code>OCR this image</code>. Je r√©cup√®re peu √† peu tout le code que je colle dans un fichier.</p>
<p>Je demande √† Ruff de v√©rifier le script reconstitu√© (<code>uvx ruff check</code>). Ruff d√©tecte un petit probl√®me (E702 Multiple statements on one line (semicolon)), mais cela ne devrait pas emp√™cher le script de fonctionner. J'ajoute un "uv inline script" en haut avec les biblioth√®ques pertinentes et lance le truc. UV met environ 15 secondes √† t√©l√©charger et √† installer les paquets. √âtonnamment, Torch ne fait pas 1 Go comme d'habitude, mais plut√¥t autour de 100 Mo. Je me demande si uv d√©tecte le fait que je n'ai pas de GPU et t√©l√©charge une version "light" (cpu-only) de Torch. Pure conjecture de ma part.</p>
<p>Ensuite tout va tr√®s vite (quelques secondes). Le script se lance et le NN s'entra√Æne sur 500 it√©rations ("epochs"). La sortie est exactement comme ce qui est montr√© dans la vid√©o. Le NN pr√©dit la courbe de x au carr√© et on voit les donn√©es d'entra√Ænement en bleu et de test en marron. J'adore ce genre de petite exp√©rience o√π l'on peut tester et mieux comprendre un des principes fondamentaux des r√©seaux de neurones et de l'intelligence artificielle. Script : https://gist.github.com/sebington/0ec87907ad0a079e6f59d941336d547c.</p>
<p>Ce soir, fatigu√©, je farfouille dans mes fichiers √† la recherche du plus vieux morceau de code python que j'aie tap√© sur un clavier. Je retrouve un script datant du 14 octobre 2020 o√π j'ai recopi√© le code de la vid√©o suivante : https://www.youtube.com/watch?v=InUqeaOSPpA (Pytorch Torchtext Tutorial 2: Built in Datasets with Example). Qu'est-ce qui m'a fait choisir cette vid√©o ? Pourquoi ne pas avoir commenc√© par le "Tutorial 1" ? Je crois que je trouvais ce gars qui tapait du code tout en semblant savoir ce qu'il faisait hyper cool. D√®s le d√©but je cherchais des choses en lien avec le langage et les mots : j'allais bient√¥t d√©couvrir le NLP (Natural Language Processing) et toutes ses applications. Les LLM d'aujourd'hui viennent de l√†, du langage. ¬´ Au commencement √©tait le Verbe, et le Verbe √©tait avec Dieu, et le Verbe √©tait Dieu. ¬ª ‚Äî Premier verset de l'√âvangile selon Jean.</p>
<p><b>27-10-2025</b></p>
<p>Je poursuis ma traduction juridico-administrative, que je dois livrer demain. J'avais entendu dire que ChatGPT √©tait d'une grande aide avec ce genre de texte. Je confirme ! Il conna√Æt le droit international dans toutes ses nuances comme sa poche. C'est incroyable l'aide qu'il apporte.</p>
<p>Vu sur HackerNews, un site int√©ressant pour qui aime les cartes et les donn√©es GPS : https://geoutil.com/.</p>
<p>Concernant mon probl√®me d'URL matching avec regex, j'interroge ChatGPT avec le prompt suivant :</p>
<p><span class="code-box">Create a regex that will only match a URL, even if: 1. the URL is enclosed in brackets; 2. the URL is followed by a full stop; 3. the URL finishes with a slash "/"; 4. any combination of the above</span></p>
<p>ChatGPT me sort un regex assez long qui ne fonctionne pas mieux que le reste. Bon il faut avouer que mon prompt est un peu tordu dans sa formulation.</p>
<p>Olivier sugg√®re <a href="https://pregex.readthedocs.io/en/latest/introduction.html">pregrex</a>, mais l'approche est assez ardue et le projet n'a pas √©t√© mis √† jour depuis le 8 octobre 2022 (soit un mois avant la sortie de ChatGPT). J'essaie le regex qui est propos√© dans la partie "Usage Example" de leur pr√©sentation, qui est sens√© matcher une URL, mais c'est le m√™me topo.</p>
<p>Je me rends sur https://lmarena.ai/?mode=direct et choisis <code>claude-opus-4-1-20250805-thinking-16k</code> avec un nouveau prompt :</p>
<p><span class="code-box">Write a regex that will match any URL in a text, whether it is enclosed in brackets, finishes with a slash or a digit or is followed by a full stop.</span></p>
<p>J'obtiens 2 regex vraiment longs, sortes d'usines √† gaz qui fonctionnent, mais butent sur les m√™mes √©cueils (les points, les virgules).</p>
<p>Dernier essai sur le m√™me site avec <code>gpt-5-high</code> (m√™me prompt).</p>
<p>J'obtiens 2 regex un peu moins longs.</p>
<p>Je teste le premier et l√† bingo, √ßa marche ! Lanc√© dans Sublime Text, le truc matche toutes les URL de mon carnet de notes sans exceptions. Je crie victoire mais d√©chante aussi vite quand j'essaie d'int√©grer le regex dans le code du script en Python qui g√©n√®re cette page. Pour l'instant √ßa ne marche pas. Je me rends compte que le fait m√™me d'afficher les diff√©rents regex essay√©s dans mes notes en TXT interf√®re avec la g√©n√©ration de la page (qui fait beaucoup appel √† regex). √áa cr√©e une sorte contamination. J'aurais d√ª y penser.</p>
<p><b>26-10-2025</b></p>
<p>Ce matin, je fais 42 km de VTT dans le vent et les embruns du c√¥t√© de Ploudalm√©zeau. Salutaire pour me reposer les yeux et le cerveau.</p>
<p>Je travaille sur une traduction tout le reste du temps. Je r√©utilise mon outil d'alignement cr√©√© l'autre jour et me rends compte qu'il n'est pas assez convivial. Je demande √† Claude de le modifier en laissant l'utilisateur choisir les langues et les fichiers et en le rendant un peu plus attractif visuellement avec <code>rich</code>. Le r√©sultat est ici : https://github.com/sebington/txt-to-tmx-aligner.</p>
<p>Je continue de perfectionner le script en Python qui g√©n√®re cette page HTML √† partir de mes notes au format TXT. J'ajoute une fonction pour mettre la page en ligne en FTP. Je me rends aussi compte que certains liens g√©n√©r√©s par le script ne fonctionnent pas s'ils sont entre parenth√®ses ou suivis d'un point, ou s'ils finissent par "/".</p>
<p>La fonction de conversion d'URL en liens hypertextes fait appel √† une expression r√©guli√®re :</p>
<p><pre><code>def linkify_urls(text):
    url_pattern = r'(https?://[^\s<>]+)'
    return re.sub(url_pattern, r'<a href="\1">\1</a>', text)</code></pre></p>
<p>Claude en a produit une version plus sophistiqu√©e mais pas encore parfaite :</p>
<p><pre><code>def linkify_urls(text):
    url_pattern = r'(https?://[^\s<>]+?)(?=[.,;:!?)"\']?(?:\s|$|<))'
    return re.sub(url_pattern, r'<a href="\1">\1</a>', text)</code></pre></p>
<p>Certains liens entre parenth√®ses fonctionnent tandis que d'autres non. Il va falloir investiguer.</p>
<p>A noter que mon script g√©n√®re la page compl√®te √† chaque fois en une fraction de seconde. Cela me permet de corriger ou d'am√©liorer mes notes et de disposer d'un rendu instantan√© de l'ensemble.</p>
<p><b>25-10-2025</b></p>
<p>Je r√©pare √† nouveau mon Google AI Studio Proofreader. Je l'avais d√©j√† fait hier et pensais qu'autosave √©tait activ√© par d√©faut comme dans le reste des applis web de Google. J'√©tais dans l'erreur : il y a un bouton pour sauvegarder les changements apport√©s √† l'appli, ce qui semble logique : pourquoi sauvegarder une appli si elle bugue ?</p>
<p>Je t√©l√©charge l'appli (fichier zipp√©) et fais bosser Gemini-CLI dessus. Je voudrais :<br>1. que l'appli fonctionne en local sur mon ordi<br>2. qu'elle se connecte √† un autre mod√®le que Gemini via une API de mon choix (Groq par exemple)</p>
<p>Apr√®s quelques moments de d√©bogage √ßa marche. Il faut que node (npm) soit install√©. Je peux maintenant lancer l'appli en faisant <code>npm run dev</code>, ce qui d√©marre un serveur local accessible via http://localhost:3000/. L'appli se lance dans Firefox et fonctionne correctement avec un mod√®le de Groq. Le mod√®le utilis√© est hardcod√© dans un fichier nomm√© <code>groqService.ts</code>. Il s'agit de <code>llama-3.3-70b-versatile</code>. üòÑ</p>
<p><b>24-10-2025</b></p>
<p>Je demande √† Gemini de r√©parer mon appli sur Google AI Studio dont l'interface graphique ne s'affiche pas et fais un essai avec un fichier complet. Mon proofreader d√©tecte 130 changements, que je dois passer en revue un par un avant de l'accepter ou de le refuser. L'outil d√©tecte un certain nombre de faux positifs quand il s'agit des majuscules ou minuscules. Je le modifie pour am√©liorer ce point et demande l'ajout d'un bouton pour accepter tous les changements d'un coup. √áa fonctionne. Le fichier en sortie est bien en mode suivi des modifications avec tous les changements clairement visibles.</p>
<p>D√©couverte du projet marimo : https://github.com/marimo-team/marimo. C'est une sorte de jupyter notebooks on steroids super prometteur. Par exemple, la sortie d'un bloc de code se met √† jour en temps r√©el si on modifie le code. Et c'est du pur Python !</p>
<p>Premi√®re tentative de git push en ligne de commande (avec une image)</p>
<p><pre><code>$ git clone https://github.com/repo_name/repo_name.github.io.git
$ cd repo_name.github.io/
$ git add images/nn.png
$ git commit -m "Add image to images folder"
$ git push origin main [provide Username and Password]</code></pre></p>
<p>Pour renommer un fichier</p>
<p><pre><code>$ git mv index.html index_old.html
$ git commit -m "Rename index.html"
$ git push origin main</code></pre></p>
<p>Pour ne pas avoir √† s'authentifier √† chaque commit</p>
<p><pre><code>$ git config --global credential.helper cache
# or store permanently (less secure)
$ git config --global credential.helper store</code></pre></p>
<p><b>23-10-2025</b></p>
<p>Simon Willison, probablement √† la demande d'Anthropic, a supprim√© de son Github les fichiers Claude Skills qu'il avait d√©couverts et t√©l√©charg√©s avant l'annonce officielle d'Anthropic concernant ce nouveau service de Claude : https://simonwillison.net/2025/Oct/10/claude-skills/.</p>
<p>Nouvelle session Gemini CLI. Je lui demande d'utiliser les Claude Skills pour r√©viser un document Word et le livrer en mode suivi des modifications. J'oublie toujours d'activer le projet avec uv avant de commencer !</p>
<p>Prompt:<br><span class="code-box">Do a professional revision (grammar, spelling, punctuation etc.) of @test.docx and output a .docx file in tracking mode where all the changes made are visible.</span></p>
<p>En parall√®le je demande √† Claude Sonnet 4.5 de faire la m√™me chose dans l'interface web classique.</p>
<p>Gemini bosse comme un dingue et grille son quota de tokens avant d'avoir termin√©.</p>
<p>Claude cr√©e une interface web inutile et ne parvient pas √† sortir un document Word (erreur). Je le relance et il finit par me donner le r√©sultat sous forme d'un fichier TXT avec les corrections (surtout l'orthographe) demand√©es.</p>
<p>Nouvelle tentative, dans Google AI Studio cette fois. Je cr√©e une appli qui fonctionne du premier coup, prend un .docx, le r√©vise, me montre les changements apport√©s et produit un document Word en mode suivi des modification. Le d√©lire. Mais est-ce que √ßa marcherait avec un gros fichier ? Je retourne sur l'appli 3h plus tard et bizarrement le code est toujours l√† mais l'interface graphique ne se charge pas. Ecran noir.</p>
<p>Retour sur Manjaro Gnome. Je r√®gle tous les probl√®mes rencontr√©s hier et savoure cette nouvelle distro qui est vraiment sympa.</p>
<p>Je suis un tuto de Simon Willison sur YT [5TdIxxBPUSI]. Pas tr√®s progressif et un peu fouilli, mais instructif. Je cr√©e un dossier 'sqlite' et lance un jupyter notebook en faisant <code>uv run --with jupyter jupyter lab</code> o√π je tape les exemples de code. Tout fonctionne bien et je peux sauvegarder les sorties.</p>
<p>L'infatiguable Willison vient de mettre une nouvelle vid√©o YT en ligne [GQvMLLrFPVI] dans laquelle il utilise Claude Code Web pour se fabriquer un outil √† partir de 2 autres outils cr√©√©s pr√©c√©demment. On aper√ßoit au passage l'impressionnante collection d'outils dont il dispose et qui sont disponibles sur son github : https://tools.simonwillison.net.</p>
<p><b>22-10-2025</b></p>
<p>Avec Claude, je continue de modifier mon script de g√©n√©ration de HTML √† partir de mes notes au format TXT (voir 19/10). √áa commence √† devenir pas mal.</p>
<p>Je t√©l√©charge Linux Mint Cinnamon, l'installe sur une cl√© USB bootable avec Rufus et l'utilise pendant 30 minutes. Je trouve bien mes marques. Le terminal est imparam√©trable une fois de plus et j'installe le terminal √©mul√© "deepin" √† la place. Plut√¥t pas mal.</p>
<p><b>21-10-2025</b></p>
<p>J'installe Manjaro Gnome pour voir. Pas mal de positif (look, rapidit√©, efficacit√©) mais certaines choses bizarres comme par exemple le fait que je n'arrive pas √† lancer un outil install√© dans uv et que je ne peux pas compl√®tement d√©sinstaller zsh, qui semble √©troitement int√©gr√© √† l'OS (d√©pendances). Le terminal pr√©-install√© n'est pas param√©trable et j'en installe une autre version. En revanche, Nextcloud, Signal et OBS Studio fonctionnent √† merveille.</p>
<p>Retour dans Ubuntu</p>
<p>Commande pour n'afficher que les mod√®les gratuits d'openrouter dans LLM:</p>
<p><pre><code>llm models | grep ':free$'</code></pre></p>
<p>J'installe https://datasette.io/ avec uv</p>
<p><pre><code>uv tool install datasette
llm models | grep ':free$'
llm -m openrouter/google/gemma-3-27b-it:free "What is Datasette?"
datasette "$(llm logs path)"</code></pre></p>
<p>cela donne l'acc√®s √† une interface web</p>
<p><pre><code>INFO: Started server process [13667]
INFO: Waiting for application startup.
INFO: Application startup complete.
INFO: Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)</code></pre></p>
<p>o√π on peut requ√™ter la base de donn√©es de tout l'historique de LLM (prompts, mod√®les, r√©ponses, etc.)</p>
<p><b>20-10-2025</b></p>
<p>Je visionne une vid√©o YT [C44iCr6czAo] sur les d√©rives s√©curitaires de Windows. √áa me donne envie de basculer sur Linux et d'y rester. Je t√©l√©charge 2 versions de Manjaro (KDE Plasma et Gnome) ainsi que CachyOS, que je teste tour √† tour. Moi qui m'√©tais promis de me reposer les yeux ce soir, c'est rat√©.</p>
<p><b>19-10-2025</b></p>
<p>Je demande √† Claude de me cr√©er un script en Python (logbook-converter) qui convertit ce carnet de notes (au format .txt) en fichier HTML avec les prompts et les extraits de code reconnus et formatt√©s. √Ä suivre.</p>
<p>Je n'en finis pas de peaufiner le projet YouTube metadata (1. retrieve, 2. analyze). Le fichier source est une liste de chaines YT au format CSV (export de FreeTube), le fichier interm√©diaire est un fichier JSON (channel_videos_2025.json) et la sortie finale peut se faire soit √† l'√©cran (v2), soit √©cran + markdown (v2_md). Je finis par faire une vid√©o.</p>
<p>Je continue d'essayer de construire un plugin Hugging Face pour Willison's <a href="https://llm.datasette.io/">LLM</a> avec Gemini-CLI. C'est de la folie pure de s'attaquer √† un truc pareil. Je vais n√©anmoins beaucoup plus loin qu'hier. Le plugin s'installe et r√©cup√®re une liste de mod√®les. Cependant impossible de lancer une requ√™te (<code>Client error '404 Not Found'</code>).</p>
<p><b>18-10-2025</b></p>
<p>J'active Copilot dans VScode et le teste timidement. Copilot n'a pas compris que j'utilise uv et tente d'installer des librairies Python avec pip sur Ubuntu. Je l'arr√™te et y reviendrai d√®s que possible.</p>
<p>Suite √† la d√©couverte du nouveau HuggingChat il y a 2 jours, je me dis qu'il est probable que HF essaie de proposer quelque chose comme OpenRouter : une seule API donne acc√®s √† de nombreux mod√®les. Je pose la question √† ChatGPT qui finit par me le confirmer apr√®s √™tre all√© lire la doc sur le site de Hugging Face.</p>
<p><span class="code-box">Yes, Hugging Face is now offering a multi-provider inference capability via its Inference Providers framework that approximates the ‚Äúone API, many models/providers‚Äù goal. The OpenAI-compatible API support is a particularly strong move, enabling you to reuse existing client libraries against HF‚Äôs routing system.</span></p>
<p>Convaincu par cette explication, je me dis qu'en th√©orie, je pourrais cr√©er un plugin pour Simon Willison's <a href="https://llm.datasette.io/">LLM</a> qui donnerait acc√®s aux mod√®les de HF. Mais je me dis aussi que c'est peut-√™tre un peu ambitieux de commencer par √ßa. Essayons d√©j√† de voir si l'API de HF fonctionne comme pr√©vu.</p>
<p>Je me rends sur https://huggingface.co/docs/inference-providers/ et finit par trouver un bout de code pour tester l'API. Il faut cr√©er une cl√© sp√©ciale (allowing inference providers). Le script fonctionne.</p>
<p>Je demande alors √† Gemini-CLI d'incorporer HF aux 3 autres providers de mon script 2_analyze_gemini_v1.py (projet YouTube).</p>
<p>Prompt :<br><span class="code-box">Use @test_hf.py to add Hugging Face as a new model provider in @2_analyze_gemini_v2.py.</span></p>
<p>Gemini-CLI le fait en 2 temps 3 mouvements. C'est totalement jouissif. Je lui demande la possibilit√© de pouvoir revenir √† la liste des "providers" si je ne choisis pas de mod√®le tout de suite. On peut ainsi comparer le nombre de mod√®les propos√©s par chaque provider : Mistral = 68, Groq = 19, OpenRouter = 340 et Hugging Face = 115.</p>
<p>Je fais une vid√©o sous Linux avec OBS Studio qui fonctionne enfin !</p>
<p>Je tente tout de m√™me de cr√©er le plugin HF avec Gemini-CLI, mais il ne veut m√™me pas s'installer dans LLM.</p>
<p><b>16-10-2025</b></p>
<p>Je tente d'utiliser nextcloud en ligne de commande avec WSL pour tester la vitesse de synchronisation des fichiers:</p>
<p><pre><code>mkdir nextcloud
sudo add-apt-repository ppa:nextcloud-devs/client
sudo apt update
sudo apt install nextcloud-client</code></pre></p>
<p>Mais c'est tr√®s gal√®re : au lieu de sp√©cifier quels dossiers on veut synchroniser, il faut exclure tous les dossiers que l'on ne veut PAS synchroniser</p>
<p><pre><code>--exclude "SEB" --exclude "TAO" etc.</code></pre></p>
<p>Du coup je d√©sinstalle tout.</p>
<p>Je g√©n√®re de nouveaux scripts dans mon projet YouTube metadata :<br>- yt_metadata_csv.py (titre et upload date √† partir de plusieurs chaines YT)<br>- retrieve_multiple_yt_video_titles.py (titre et upload date √† partir de plusieurs videos)</p>
<p>Ce second script exporte le r√©sultat du scraping de toutes mes URL de videos YT dans un fichier CSV (URL, Title, Upload date) que j'importe ensuite dans Google Drive. A noter que ce script n'utilise pas yt-dlp mais fait appel √† requests et beautifulsoup4 et semble √™tre plus rapide.</p>
<p>Le nouveau HuggingChat (https://huggingface.co/chat/) est une tuerie (116 mod√®les propos√©s).</p>
<p>Simon Willison reparle des Claude Skills sur son blog. Je suis certain que ce que j'ai fait avec Claude Skills et Gemini-CLI le 12/10 n'avait jamais √©t√© fait auparavant.</p>
<p><b>15-10-2025</b></p>
<p>L'alignement manuel des textes de ma coll√®gue est d√©cid√©ment impossible avec memoQ. Obs√©d√© par le c√¥t√© r√©calcitrant de la chose, je demande √† Claude de me cr√©er un outil d'alignement.</p>
<p><span class="code-box">I have two .txt files source (fr) and target (en) that I would like to align in order to obtain a translation memory (.tmx). Create a Python script that will scan the two .txt files line by line (this is important), retrieve all sentences and expressions pairs (source and target language) and store them methodically and chronologically in a dictionary, to be later exported as a .tmx file. Keep everything, even if source text is the same as target text but remove duplicate entries. Discard section numbering (e.g. 12.2.3.1) at the start of a line. Discard lines containing just one letter or only numbers. Keep all meaningful chains of characters. Segment at sentence level whenever possible. Attached are two extracts (source and target).</span></p>
<p>√áa marche ! One shot !üòÉ Le script g√©n√®re un fichier TMX valide que j'importe dans memoQ sans probl√®me. Dingue.</p>
<p>https://repomix.com/ is a powerful tool that packs your entire repository into a single, AI-friendly file.<br>https://www.kimi.com/en/ : sounds like another great LLM like Qwen and DeepSeek</p>
<p><b>14-10-2025</b></p>
<p>Je fais une vid√©o sur la tentative d'alignement de fichiers .txt avec memoQ et sur la nouvelle approche avec Gemini CLI du 12/10.</p>
<p>Il faut que je m'int√©resse aux MCP server.<br>Une liste impressionnante : https://github.com/modelcontextprotocol/servers<br>Essayer celui-ci peut-√™tre : https://github.com/vivekVells/mcp-pandoc</p>
<p><b>13-10-2025</b></p>
<p>Ce soir, je regarde deux vid√©os passionnantes de la cha√Æne Underscore.</p>
<p>Dans <a href="https://www.youtube.com/watch?v=biZX5cnQ_UU">la premi√®re</a>, Gr√©goire Mialon et Cl√©mentine Fourrier de Hugging Face expliquent comment les LLM sont √©valu√©s. Je visite ensuite le site de <a href="https://clefourrier.github.io/">Cl√©mentine Fourrier</a> dont j'adore le c√¥t√© minimaliste avec en bas <a href="https://jeffhuang.com/designed_to_last/">ce lien</a> sur une mani√®re minimaliste et p√©renne de concevoir un blog.</p>
<p>La <a href="https://www.youtube.com/watch?v=s5oHvfFUsbE">seconde vid√©o</a> est consacr√©e √† un outil tr√®s original de g√©olocalisation d'images d√©velopp√© par Nicolas Dufour (voir https://nicolas-dufour.github.io/plonk).</p>
<p>Je fais des essais de mise en ligne de ce carnet sur http://bytepacking.free.fr/. Du bon vieux html. Pour mettre la page en ligne, j'utilise un outil primitif fourni par free : http://ftpperso.free.fr/index.pl.</p>
<p>Liens du jour :<br>Pense-b√™te uv (Olivier) : https://mathspp.com/blog/uv-cheatsheet<br>Sortie de NanoChat d'Andrej Karphaty : https://github.com/karpathy/nanochat<br>Sauvegarder une session Gemini-CLI en markdown doit √™tre possible : https://gist.github.com/simonw/12c7b072e8e21ef1e040fb3b69c1da28</p>
<p><b>12-10-2025</b></p>
<p>Je me penche sur les outils de Claude (Claude Skills) pour manipuler les fichiers et fais un essai avec le r√©pertoire d√©di√© √† Word (.docx). La "boite √† outil" √©tant assez complexe, je d√©cide de faire bosser Gemini CLI dessus (avec uv pour ex√©cuter les scripts en Python). Apr√®s tout ces outils ont √©t√© con√ßus pour √™tre pilot√©s par un LLM. L'objectif est de remplacer du texte FR par du texte EN dans un document Word (.docx) en le copiant d'un fichier √† l'autre, sans alt√©rer la structure (images) du fichier cible.</p>
<p>Je fais 3 tentatives. Une session sur Windows, mal configur√©e, produit un fichier illisible. Mais les deux autres (sur Ubuntu) fonctionnent. Avec 2 fichiers tr√®s simples constitu√©s de 4 paragraphes de texte et de 2 images en FR (et la m√™me chose en EN), je demande √† Gemini CLI de trouver un moyen de remplacer le texte du fichier en fran√ßais par le texte du fichier en anglais, sans alt√©rer la mise en page et les images.</p>
<p><span class="code-box">Using the available tools in this project, devise a strategy to replace the text from @input-fr.docx by the text from @input-en.docx, keeping the layout and integrity of the target document.</span></p>
<p>Il s'ex√©cute et produit un fichier cible o√π le texte en fran√ßais a √©t√© remplac√© par le texte en anglais. Mais comment √™tre absolument certain qu'il n'a pas simplement copi√© le fichier source dans son int√©gralit√© ? Je fais une deuxi√®me tentative plus ambitieuse : je ne laisse dans le r√©pertoire de travail que le fichier Word FR, auquel j'ajoute le texte en anglais au format TXT. Je demande √† Gemini CLI la m√™me chose, c'est √† dire de remplacer le texte en FR par le texte en EN, mais cette fois √† partir du fichier TXT.</p>
<p><span class="code-box">@input-fr.docx is a .docx of a short text in French with photos. @en-text.txt is a replacement text in English for the docx file. Use the tools provided here to edit @input-fr.docx and replace the text it contains by the text in @en-text.txt while keeping the photos.</span></p>
<p>Sans surprise, il n'y arrive pas. Je lui demande alors de faire une comparaison LINGUISTIQUE des contenus avant commencer l'op√©ration et √ßa marche !</p>
<p><span class="code-box">Use a linguistic approach and run a similarity search to check that the meaning of each paragraph in English of @en-text.txt corresponds approximately to the paragraph in French. If so then copy it over the French in the .docx file.</span></p>
<p>Il compare les textes, √©tablit des correspondances entre les paragraphes et remplace le texte FR par le texte EN au bons endroits, sans toucher aux images ! üòÉ Mais ce n'est qu'un d√©but et les fichiers utilis√©s sont hyper simples.</p>
<p>[EDIT] : dans les manipulations d√©crites ici, il n'est pas question de cr√©er des scripts autonomes r√©utilisables plus tard, mais bien de laisser Gemini-CLI orchestrer les op√©rations et r√©pondre aux demandes de l'utilisateur en temps r√©el gr√¢ce √† l'√©cosyst√®me du projet reposant sur la partie docx des Claude Skills.</p>
<p>A faire : complexifier les fichiers .docx (ajouter par ex. num√©rotation, gras, italiques)</p>
<p><b>11-10-2025</b></p>
<p>Je visionne √† nouveau <a href="https://www.youtube.com/watch?v=0OUODPRgPl8">10+ tools to use ai in the terminal</a> et prends des notes. Le gars aurait mieux fait d'intituler sa vid√©o "Demo of Simon Willison's LLM" ! Sur le m√™me sujet, il y a aussi <a href="https://www.youtube.com/watch?v=UZ-9U1W0e4o">Become a command-line superhero with Simon Willison's LLM tool</a>, au titre plus appropri√©.</p>
<p>Je d√©couvre un post de Simon Willison concernant les "Claude Skills", des outils de manipulation de fichiers de Claude que Simon a "hack√©" √† demi-mots : "This is a really sophisticated set of tools for document manipulation, and I love that Anthropic have made those visible - presumably deliberately - to users of Claude who know how to ask for them." (https://simonwillison.net/2025/Oct/10/claude-skills/).</p>
<p><b>09-10-2025</b></p>
<p>Le probl√®me de l'alignement automatique des textes pour le projet de ma coll√®gue semble insoluble. M√™me si les r√©sultats d'hier soir sont encourageants. Je tente de passer tout en .txt et compare dans notepad++. M√™me la num√©rotation des sections n'est pas fiable !üò´</p>
<p><b>08-10-2025</b></p>
<p>Tentative de g√©n√©ration d'un outil d'alignement s√©mantique multilingue de textes avec ChatGPT, Claude (OpenRouter/Mistral) puis Gemini CLI. L'id√©e est de faire appel √† un mod√®le de g√©n√©ration d'embeddings et √† un autre LLM classique pour aligner des paires de phrases dans deux documents parall√®les, l'un √©tant la traduction de l'autre.</p>
<p>De plus en plus je rencontre les limites des acc√®s API Free Tier. Est-ce voulu ? Bizarrerie : l'acc√®s OpenRouter fonctionne pour mon script "analyze YouTube metadata" mais pas avec "aligner OpenRouter". Je pose la question √† ChatGPT qui me donne une explication vaseuse et un patch qui ne marche pas.</p>
<p>Claude me donne la bonne explication un peu plus tard : l'acc√®s √† OpenRouter va vite dans le rouge parce que mon script, par d√©finition, fait de multiples requ√™tes au mod√®le pour tenter d'√©tablir des ponts entre chaque phrase de ma paire de documents ! L'explication de ChatGPT √©tait compl√®tement √† c√¥t√© de la plaque.</p>
<p>Je refile le b√©b√© √† Gemini CLI et le fait bosser sur plusieurs d√©clinaisons du script (il am√©liore la fonction embeddings) avec OpenRouter seul, puis avec Ollama (embeddings) et OpenRouter (LLM), puis comme le free tier d'OpenRouter est grill√©, avec Ollama seul (embeddinggemma et gemma3) et √ßa marche enfin sans erreurs ! Le r√©sultat de l'alignement est stock√© dans un fichier JSON. Le script est quand m√™me assez long : 500+ lignes de code, et je ne sais pas ce que vaut l'alignement.<br>------<br>Je change mon mot de passe Google sur mon PC via le navigateur et je re√ßois une notification sur mon smartphone Xiaomi alors que j'avais supprim√© le compte Google du t√©l√©phone. √áa laisse songeur.</p>
<p><b>07-10-2025</b></p>
<p>Je fais le gros m√©nage dans mes scripts (YT metadata). Instructif. Inutile de garder des vieux scripts. Perte de temps.</p>
<p><b>06-10-2025</b></p>
<p>Vid√©o pour Olivier (YT metadata) tr√®s mal pr√©par√©e et d√©cevante. Mais √ßa me fait avancer.</p>
<p><b>05-10-2025</b></p>
<p>Un effet particuli√®rement d√©l√©t√®re de l'arriv√©e de l'IA dans les entreprises de services, c'est l'assomption par le client que la prestation est d√©sormais ex√©cutable en 2-3 clics de souris, que le devis qui lui est envoy√© est surcot√© et n'est pas le reflet du travail r√©ellement effectu√©.</p>
<p>autoencoder mnist plan<br>1. after training do not run the test set<br>2. ask user to input a square image of a handwritten digit<br>3. normalize the image (size, color, bit depth) so that is complies with the train set<br>4. submit the image to the model<br>5. display the model's best guess</p>
<p>Ai retest√© <a href="https://github.com/mozilla-ai/llamafile">llamafile</a> sous Windows (qwen3 et gemma3) et je trouve la g√©n√©ration plus rapide.</p>
<p>Ai test√© <a href="https://jules.google/">Google Jules</a> sur autoencoder h√©berg√© sur Github. Bluffant.</p>
<p>Ai install√© Gemini CLI sur Powershell, puis modifi√© autoencoder avec. Super bluffant.</p>
<p><b>04-10-2025</b></p>
<p>Installation de la version beta 25.10 d'Ubuntu. Elle propose un nouveau terminal par d√©faut (ptyxis) difficile √† param√©trer. Je finis par r√©installer le gnome-terminal.</p>
<p>J'installe Nextcloud et constate que la mise √† jour des fichiers est √† nouveau rapide. La DSI a peut-√™tre r√©solu le probl√®me de lenteur. Ce faisant j'oublie de sauvegarder les logs de Gemini CLI ! üò´</p>
<p>Je visite le site de <a href="https://bellard.org">Fabrice Bellard</a> et c'est parti pour un petit voyage technologique passionnant. Je t√©l√©charge son projet /tsac/ et le teste avec un mp3 de piano (renomm√© toto.mp3) dans powershell :</p>
<p><pre><code>./tsac c toto.mp3 toto.tsac</code></pre></p>
<p>Le fichier passe de 4707 Ko √† 101 Ko !!</p>
<p>Je peut ensuite reconvertir ce fichier hyper-compress√© en WAV :</p>
<p><pre><code>./tsac d toto.tsac toto.wav</code></pre></p>
<p>Le fichier .wav fait 26 Mo. Je ne d√©tecte pas de perte de qualit√© sonore.</p>
<p>Je demande des explications √† Claude concernant le r√¥le du Transformer (voir "Ultra-low bitrate audio compression") et il me r√©pond :</p>
<p><span class="code-box">The Transformer model acts as a learned compressor on top of the DAC encoding. It takes the discrete codes produced by the DAC encoder and finds patterns and redundancies in these codes. The DAC does the heavy lifting of converting audio to a compact form, but the Transformer notices that even within this compact form, there are patterns (temporal dependencies, repetitions, predictable sequences) that can be exploited for additional compression.</span></p>
<p>EDIT : Sur ce sujet, voir aussi <a href="https://openzl.org/">https://openzl.org/</a>.</p>
<p>Autre projet de Fabrice Bellard : <a href="https://textsynth.com/">https://textsynth.com/</a>, un serveur de LLM !</p>
<p>On peut maintenant <a href="https://claudio.uk/posts/epub-to-audiobook.html">convertir un livre entier en audio</a> avec kokoro.</p>
<p><b>03-10-2025</b></p>
<p>Grosse fatigue. Je continue de pr√©pare la pr√©sentation Youtube M√©tadata mais la gestion des multiples versions des scripts m'√©puise. Il faut changer de m√©thode. Je tente une vid√©o mais m'emp√™tre dans des explications sans fin.</p>
<p><b>02-10-2025</b></p>
<p>Nouveau script (Claude) pour visualiser les conversations ChatGPT avec leur date de cr√©ation convertie en date standard.</p>
<p>Quand on y r√©fl√©chit, le chemin qu'on prend pour arriver √† un r√©sultat peut √™tre tr√®s alambiqu√©, il y a toutes les fausses pistes, les prompts qui ne donnent rien, les scripts ou les prompts que l'on r√©utilise, que l'on passe √† une IA, puis √† une autre, sans r√©sultat tout de suite utilisable, ou parfois juste un peu √† cot√© de la plaque.</p>
<p>D√©couverte (gr√¢ce √† mon coll√®gue JB) de St√©phane Mallat. Coup de foudre. J'√©coute tout ce que je peux trouver sur lui : https://www.radiofrance.fr/franceculture/podcasts/la-science-cqfd/stephane-mallat-la-palme-d-ondes-1077850.</p>
<p><b>01-10-2025</b></p>
<p>J'essaie de retracer le workflow de mon projet "YouTube metadata retrieval and analysis". Je cherche les premi√®res conversations en esp√©rant que ce soit bien sur Claude ou ChatGPT. √áa semble √™tre le cas. Tr√®s difficile de s'y retrouver.</p>
<p>Pour ChatGPT, j'exporte mes donn√©es et ouvre ensuite chat.html, dans lequel je peux faire une recherche par mots-cl√©s. Normalement sur la version en ligne je devrais avoir un outil de recherche comme dans Claude, mais il n'appara√Æt pas sur ma version de ChatGPT, ce qui est bizarre. En plus de chat.html, ChatGTP propose un autre fichier (conversations.json) dans lequel les dates sont au "Unix epoch format (seconds since January 1, 1970, UTC)".</p>
<p>Pour Claude version web, je peux utiliser leur outil de recherche, mais √ßa cherche uniquement les TITRES de conversations, pas leur contenu. Je fais donc √©galement un export de mes donn√©es et j'obtiens entre autres un fichier conversations.json que j'ouvre avec Sublime Text et que je peux parser manuellement. Ce n'est pas tr√®s pratique alors je demande √† Claude de me faire un script qui g√©n√®re une page HTML comme pour ChatGPT. √áa marche nickel apr√®s quelques it√©rations.</p>
<p>J'arrive √† reconstituer un historique rudimentaire :<br>2024-10-30 ChatGPT : YouTube metadata (last modif. 2025-01-26)<br>2024-12-09 Claude : Aggregate YouTube Channel Titles from JSON<br>2024-12-11 Claude : YouTube Channel Video Scraper<br>2025-09-28 ChatGPT : Combine scripts and classify trends<br>2025-09-28 Claude : YouTube video download error<br>2025-09-30 Gemini CLI session</p>
<p><b>30-09-2025</b></p>
<p>How to download and open a .parquet file in ipython repl using uv (ubuntu):</p>
<p><pre><code>uv init parquet
cd parquet
uv add ipython pandas pyarrow
wget https://huggingface.co/datasets/ylecun/mnist/resolve/main/mnist/test-00000-of-00001.parquet -O mnist-test.parquet
ipython
import pandas as pd
df = pd.read_parquet('mnist-test.parquet')
print(df.head())
print(df.columns)
print(df.describe())</code></pre></p>
<p>Je demande √† Claude de cr√©er un convertisseur .epub vers .azw3, ce qu'il fait (epub_to_azw3.py). Le script a besoin que Calibre soit install√© pour fonctionner (d√©cevant).</p>
<p>Je visionne avec int√©r√™t une vid√©o YT de Jodie Burchell, "Build a Semantic Book Recommender" [Q7mS1VHm3Yw]. Voix tr√®s agr√©able et explications limpides.</p>
<p>J'installe Opencode (<code>curl -fsSL https://opencode.ai/install | bash</code>). Petit essai rapide, mais √† priori Opencode est beaucoup moins intuitif et efficace que Gemini CLI.</p>
<p>Nouvelle session incroyable sur Gemini CLI pour mon projet "YouTube metadata retrieval and analysis". Apr√®s 2h de 'pilotage', j'obtiens 2 scripts qui tiennent la route. Incroyable ce que √ßa marche bien. Le second script permet entre autres de lister tous les mod√®les propos√©s par l'API d'un fournisseur. Chez Mistral j'ai cru voir 2 mod√®les faisant de l'OCR. J'utilise aussi OpenRouter qui propose plusieurs mod√®les gratuits parmi les dizaines propos√©s via leur API.</p>
<p><b>29-09-2025</b></p>
<p>Papillonnage</p>
<p>Le taux de transfert en √©criture sur ma carte microSD branch√©e avec un lecteur de carte USB3 est assez m√©diocre (env. 60 Mo/sec). Pas de changement si je formate la carte en NTFS au lieu de ExFAT. Le taux de transfert avec un disque SSD et son adaptateur SATA-USB3 est bien meilleur : env. 350 Mo/sec en lecture comme en √©criture.</p>
<p>Un traducteur explique <a href="https://news.ycombinator.com/item?id=42897856">comment il utilise des LLM</a> pour travailler.</p>
<p>Sortie de Claude Sonnet 4.5 : <a href="https://www.anthropic.com/news/claude-sonnet-4-5">https://www.anthropic.com/news/claude-sonnet-4-5</a>.</p>
<p>Je mets √† jour LLM (<code>uv tool upgrade llm</code>) et constate que tous les plugins ont disparu ! Sur le sujet voir : <a href="https://github.com/simonw/llm/issues/575">https://github.com/simonw/llm/issues/575</a>. Il semblerait que Simon n'ait pas corrig√© le probl√®me.</p>
<p>Je modifie un script "autoencoder with MNIST dataset" avec Claude Sonnet 4.5 et √ßa fonctionne.</p>
<p><b>28-09-2025</b></p>
<p>Projet "YouTube metadata analysis". J'essaie de combiner plusieurs scripts en un seul, avec un fichier .csv en entr√©e contenant une liste de cha√Ænes YouTube (ChatGPT : "Combine scripts and classify trends") mais r√©sultats inconsistants d'un run √† l'autre. Faut-il avoir recours au RAG ?</p>
<p><b>27-09-2025</b></p>
<p>Visionnage de vid√©o YT :<br>- <a href="https://www.youtube.com/watch?v=0OUODPRgPl8">10+ tools to use ai in the terminal</a><br>- <a href="https://www.youtube.com/watch?v=rGz0r8VaYCc">LLMs on the Edge of the Scaling Laws</a> (Jodie Burchell)<br>- <a href="https://www.youtube.com/watch?v=iRqpsCHqLUI">LLMs for Devs: Model Selection, Hallucinations, Agents, AGI</a> (Jodie Burchell)</p>
<p><a href="https://crawshaw.io/blog/programming-with-llms">Article tr√®s int√©ressant</a> de David Crawshaw sur la mani√®re de programmer avec des LLM.</p>
<p><b>26-09-2025</b></p>
<p>Refresher sur llamacpp, qui ne veut plus se lancer depuis son r√©pertoire en faisant 'llama-cli -m modele.gguf' comme avant. Solution :</p>
<p><pre><code># Download the latest llamacpp precompiled binaries
# Unzip and stick everything in (for example) $HOME/llamacpp/
# Add the build dir to your PATH
echo 'export PATH="$HOME/llamacpp/:$PATH"' >> ~/.bashrc
source ~/.bashrc
# Launch a model
llama-cli -m path_to/model.gguf</code></pre></p>
<p>EDIT : manip ci-dessus inutile si on fait <code>./llama-cli -m modele.gguf</code> dans le r√©pertoire o√π llamacpp est install√© (j'avais oubli√©)</p>
<p>Retour √† <a href="https://www.youtube.com/watch?v=QUXQNi6jQ30">la vid√©o</a> de Willison sur son outil LLM. Vers 28'35", je fais une copie d'√©cran de son exemple de script en bash qui r√©sume les th√®mes des commentaires d'un sujet de Hacker News et je demande √† Claude de transcrire le texte de l'image, ce qu'il fait sans aucune erreur. Je colle le code dans un fichier (hn_summarizer.sh), auquel je donne les droits d'ex√©cution (<code>cmod +x hn_summarizer.sh</code>), puis je remplace 'haiku' par '<code>groq/openai/gpt-oss-20b</code>', mon mod√®le par d√©faut dans llm.</p>
<p>Ensuite je choisis un sujet au hasard dans Hacker News et clique sur le lien pour afficher les commentaires (ex. https://news.ycombinator.com/item?id=45384653). Je copie le num√©ro d'id (45384653) et lance le script de Willison en faisant <code>./hn_summarizer.sh 45384653</code>. J'obtiens un r√©sum√© des th√®mes abord√©s. Avec la version gratuite de mon mod√®le (fen√™tre contextuelle limit√©e) je ne peux pas choisir de sujets qui ont trop de commentaires (30 maxi je dirais). Mais je peux changer de mod√®le et mettre par exemple <code>openrouter/meta-llama/llama-3.3-70b-instruct:free</code>.</p>
<p>Post de Simon sur le sujet : https://til.simonwillison.net/llms/claude-hacker-news-themes</p>
<p><b>25-09-2025</b></p>
<p>Essais de conversion et de reconstruction d'un document PDF de 60 pages en Word. Ma coll√®gue Catherine convertit le doc avec Acrobat Pro mais nous savons d'exp√©rience que la mise en page est tr√®s "fragile", surtout si on doit modifier le document. J'essaie avec diff√©rentes IA (Gemini, Mistral, Claude, ChatGPT) de passer du texte brut copi√© √† partir du PDF √† du markdown propre et structur√©. C'est ChatGPT qui donne le meilleur r√©sultat, au moins pour le d√©but du document.</p>
<p>Prompt :<br><span class="code-box">This text is a raw copy from a PDF document. Reformat it as readable markdown. Do not alter its content. Do not summarize. Do not translate.</span></p>
<p>Je demande ensuite √† Claude de cr√©er un script qui convertit le markdown g√©n√©r√© par ChatGPT en Word et √ßa marche. Mon intuition me dit que le fichier Word ainsi g√©n√©r√© est beaucoup plus robuste que le fichier issu de la conversion avec Acrobat. Cependant cette m√©thode induit encore de trop nombreuses approximations pour √™tre r√©ellement fiable.</p>
<p>Je me replonge dans Willison's <a href="https://llm.datasette.io/">LLM</a>. J'installe plusieurs plugins : llm-cmd, llm-command-r, llm-ollama, llm-groq, llm-openrouter, llm-mistral. Je cr√©e un compte sur <a href="https://openrouter.ai/">OpenRouter</a>. Je visionne <a href="https://www.youtube.com/watch?v=aQuuJuCa0VM">la vid√©o de Mark Needham sur LLM</a>.</p>
<p>Commandes int√©ressantes :</p>
<p><pre><code>llm 'Who is Simon Willison?' > output.txt</code></pre></p>
<p>(√©crit le r√©sultat de la requ√™te dans le fichier output.txt)</p>
<p><pre><code>cat /etc/*-release | llm -s "Tell me about my operating system"</code></pre></p>
<p>(pipe le r√©sultat de la commande Bash, y ajoute un prompt et lance la requ√™te)</p>
<p><b>24-09-2025</b></p>
<p>Tentation de refiler aux IA (sans trop y croire) des trucs jug√©s difficiles ou infaisables. Je donne √† Gemini (interface web) un texte brut sans mise en forme issu d'un copier-coller sauvage d'un PDF avec le prompt suivant :</p>
<p><span class="code-box">Produis une version fid√®le et restructur√©e en markdown de ce document en prenant soin de faire appara√Ætre les titres de section et les paragraphes. Ne le traduis pas.</span></p>
<p>Gemini s'ex√©cute, puis je ne sais pas ce qui se passe, il bugue et je perds tout.</p>
<p>Je t√©l√©charge une appli web "gpx-map-filter" g√©n√©r√©e il y a quelques jours avec Google AI Studio. Cette appli me permet de retrouver les fichiers GPX correspondant √† une zone pr√©d√©finie sur une carte ("bounding box"). Je demande √† Gemini CLI de faire fonctionner l'appli en local. Gemini CLI me dit qu'il faut cr√©er une cl√© API sur Google Cloud. Je lui r√©ponds que je n'en ai pas. Il me donne alors une solution sans avoir recours √† une cl√© API ! Il faut installer node et lancer un serveur en tapant <code>npn run dev</code>. √áa fonctionne.</p>
<p><b>20-09-2025</b></p>
<p>Fichiers GPX : le retour de ChatGPT. Apr√®s m'√™tre perdu avec Claude et ses scripts interminables, une simple requ√™te √† ChatGPT produit un script d'une soixantaine de lignes qui fait le taf sans essayer de centraliser tous les points des GPX dans un dataframe, ce que je m'obstine √† faire depuis des mois. On peut vraiment √™tre aveugl√© par ce qu'on croit √™tre une bonne id√©e.</p>
<p>Prompt :<br><span class="code-box">I want to clean up many gpx files from various devices and format. I have made a python script that parses gpx files, stores all latitude, longitude, elevation and timestamps points into a dataframe, removes duplicate entries and for each date (yyymmdd) outputs clean gpx files. This kinda works for some files, however many exported files have points in the wrong chronological order, making them useless. Can you think of a better approach to solve this problem?</span></p>
<p>Une fois de plus je constate que complexit√© ne rime pas forc√©ment avec efficacit√©.</p>
<p><b>18-09-2025</b></p>
<p>Je reconstitue partiellement la d√©marche du projet "docx-images" avec les 3 sessions de Gemini CLI que je regroupe dans un seul fichier texte.</p>
<p>Je me replonge dans l'id√©e de produire des fichiers GPX "propres". Je perfectionne un outil en ligne de commande g√©n√©r√© sur Claude. Le script finit par √™tre assez long (1000+ lignes de code) et n'est pas foufou. Certaines fonctions du menu ne fonctionnent pas correctement. Je g√©n√®re d'autres scripts avec Claude jusqu'√† en perdre la t√™te. C'est le probl√®me de ces outils. Il faut √™tre hyper-organis√© sinon on peut se noyer dans les scripts.</p>
<p><b>16-09-2025</b></p>
<p>Je poursuis mon projet de retirer/remettre les images avec des documents Word. J'utilise le script d'hier soir (qui fonctionne avec des documents .odt) comme base pour Gemini CLI et lui demande d'en faire une version qui fonctionne avec des fichiers Word. Cette fois √ßa marche apr√®s seulement 2-3 it√©rations (docx_image_manager.py). Mais le plus fort c'est que j'essaie ensuite avec le fichier d'un client contenant 423 images et √ßa fonctionne ! Je n'en reviens pas. Il faudrait que j'arrive √† retracer toute la d√©marche. Je crois qu'au d√©but je suis parti d'un script g√©n√©r√© par Claude.</p>
<p><b>15-09-2025</b></p>
<p>J'installe Gemini CLI dans WSL <code>npm install -g @google/gemini-cli</code>. Pour se loguer avec son compte Google dans WSL, je dois taper <code>gemini nobrowser=TRUE</code>.</p>
<p>Je fais travailler Gemini CLI sur un probl√®me assez ardu. Cr√©er un script qui prend un fichier DOCX avec des images en entr√©e, retire toutes les images et g√©n√®re un fichier sans images (qui ainsi sera beaucoup plus l√©ger et pourra √™tre import√© dans notre CAT tool). Une fois le fichier traduit un second script remet toutes les images √† leur juste place. Pour l'instant mon script n¬∞1 parvient √† retirer les images, mais le n¬∞2 n'arrive pas √† les remettre. Gemini CLI a boss√© comme un dingue pour y arriver, sans succ√®s. C'√©tait fascinant comme session. Je lui ai dit d'utiliser uv (<code>uv run script.py</code>) et il l'a fait. Il s'est arr√™t√© 2-3 fois et je lui ai dit "continue" et il a continu√© !</p>
<p>Ma premi√®re approche consiste √† lui demander de cr√©er des "placeholders" num√©rot√©s "image1.jpg", "image2.jpg", etc., ce qu'il fait, mais je me rends compte que ce n'est pas la bonne m√©thode. Je lui demande alors d'extraire le squelette XML du docx. Il parvient √† retirer les images sans toucher au code XML. √áa donne un fichier Word sans images avec des sortes de cadres vides √† la place, comme quand il manque une image sur un site internet. Je crois que passer par le XML qui est constitutif des fichiers .docx est la bonne voie, mais Gemini a vraiment beaucoup de mal √† g√©rer le truc. Impression que la mani√®re dont un fichier Word est encod√© est un vrai cauchemar, m√™me pour une IA !</p>
<p>Mon beau-fr√®re Olivier sugg√®re d'essayer la m√™me manip avec le format Libre Office (.odt). Le soir je reprends tout depuis le d√©but avec un fichier ODT contenant du texte et des images. Je demande √† Gemini CLI de cr√©er 2 scripts : un qui enl√®ve les images et g√©n√®re le doc sans images et un second script qui prend le doc sans images et les remet. J'atteins bient√¥t la limite du mod√®le pro et n'ai d'autre choix que de continuer avec gemini-2.5-flash mais √ßa finit par fonctionner. Chose surprenante, les scripts g√©n√©r√©s ne sont pas si longs que √ßa (beaucoup moins que pour Word). Je regroupe les 2 scripts en un seul avec un menu basique : 1. retirer les images, 2. remettre les images. Gemini-2.5-flash semble bien marcher pour ce genre de petites modifs (odt_image_manager.py).</p>
<p>Je me rends compte que les outils comme Gemini CLI et Claude Code sont l'aboutissement de toutes les avanc√©es dont on entend parler depuis la sortie de ChatGPT : chain-of-thought prompting, RAG, fonction calling, tool use, augmentation de la taille de la fen√™tre contextelle, agents autonomes, MCP... Tout √ßa enfin mis en oeuvre de mani√®re coh√©rente. Je demande √† ChatGPT s'il est d'accord avec mon constat et il me r√©pond comme d'habitude en me caressant dans le sens du poil :</p>
<p><span class="code-box">Tu touches un point tr√®s juste üëç : des outils comme Gemini CLI et Claude Code incarnent une sorte de synth√®se appliqu√©e de toutes les briques d‚Äôinnovation accumul√©es dans le domaine des LLM depuis fin 2022. Au d√©but, on avait "juste" ChatGPT comme assistant conversationnel. Puis, √©tape par √©tape, des concepts et technologies se sont empil√©s pour transformer les LLM en v√©ritables syst√®mes intelligents, interactifs et sp√©cialis√©s.</span></p>
<p><b>14-09-2025</b></p>
<p>Oneliner pour lancer un Jupyter notebook sans se prendre la t√™te</p>
<p>Installer <a href="https://docs.astral.sh/uv/getting-started/installation/">uv</a>, puis faire :</p>
<p><pre><code>uv run --with jupyter jupyter lab</code></pre></p>
<p>M√™me chose avec ipython :</p>
<p><pre><code>uv run --with ipython ipython</code></pre></p>
<p>------<br>Je regarde ce que devient <a href="https://llm.datasette.io">LLM</a>, l'outil de Simon Willison. Je ne peux m'emp√™cher de penser que Claude Code et Gemini CLI s'en sont grandement inspir√©s.<br>------<br><a href="https://pyodide.org/en/stable/console.html">Pyodide</a> est un outil √©patant qui permet d'ex√©cuter du code Python dans un navigateur.</p>
<p><b>13-09-2025</b></p>
<p>Je m'amuse avec <a href="https://github.com/google-gemini/gemini-cli">Gemini CLI</a> depuis 2 jours. L'outil est vraiment impressionnant. Ce qui est dingue c'est que Gemini CLI en mode gratuit permet de vraiment travailler (60 requests/min and 1,000 requests/day). Je lui donne l'acc√®s √† un r√©pertoire contenant plusieurs scripts en Python plus ou moins r√©cents permettant de manipuler les fichiers GPX et lui demande de les analyser. Il identifie tout de suite les plus aboutis et peut les am√©liorer tout en les testant avec de vraies donn√©es. On peut lui demander n'importe quoi, il code, il r√©fl√©chit, il ex√©cute en Bash, la totale. Et il poss√®de une fen√™tre contextuelle d'un million de tokens. Selon Gemini, le script le plus abouti de mon r√©pertoire est un script √©crit par Claude.</p>
<p><b>02-09-2025</b></p>
<p>Groq toujours au top pour transcrire les fichiers audio, √† condition de r√©duire leur taille au pr√©alable avec ffmpeg (fichier < 40 Mo). Pour obtenir une transcription diaris√©e (identification des interlocuteurs), je passe la transcription brute obtenue avec Groq √† Claude avec le prompt suivant :</p>
<p><span class="code-box">Le texte ci-joint est la transcription brute d'un entretien entre 2 interlocuteurs. Peux-tu am√©liorer cette transcription en veillant √† s√©parer les prises de paroles et en prenant soin de retirer tous les tics de langage, r√©p√©titions inutiles, etc., tout restant fid√®le aux propos tenus? Le texte final devra √™tre fluide et agr√©able √† lire.</span></p>
<p>Le r√©sultat est bluffant. Le mod√®le parvient √† identifier les changements d'intervenants et √† d√©couper leur prises de paroles respectives √† partir de cette simple transcription √©crite sans rep√®res d'aucune sorte.</p>
<p><b>01-09-2025</b></p>
<p>Retour au boulot et reconnexion progressive √† l'IA apr√®s la pause estivale. Nous avons re√ßu une demande de transcription d'enregistrements et je me replonge sans transition dans Whisper et autres joyeuset√©s en mati√®re de speech-to-text. Je teste Voxtral (voxtral-mini-latest), le mod√®le speech-to-text de Mistral. √áa marche, mais la transcription est tronqu√©e car le mod√®le est payant. Groq semble toujours fonctionner. Ils ont retir√© un mod√®le (distil-whisper) de leur liste.</p>
<p>ü§ñ</p>

</main>

</body>
</html>