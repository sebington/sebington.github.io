<!doctype html>
<html lang="fr">
<head>
<meta charset="utf-8">
<title>Paradis artificiels - Carnet de notes d'un traducteur passionn√© d'IA</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="icon" type="image/png" href="stars.png">
<style>

/* Import Inter + Roboto Flex */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Roboto+Flex:wght@300;400;500;600&display=swap');

:root {
	--back-color: rgb(255, 247, 211);
	--text-color: rgb(12, 12, 12);
	--url-color:rgb(120, 120, 120);
}

/* minimalist reset */
* {
	padding: 0;
	margin: 0;
	font-size: 1em;
}

body {
	background-color: var(--back-color);
	color: var(--text-color);
	font-family: Tahoma, Verdana, Arial, sans-serif;
}

main {
	margin: 0 auto;
	padding: 30px 140px 0 140px;
	max-width: 600px;
	min-height: 100vh;
}

h1 {
	font-size: 1.5em;
	font-weight: 700;
	display: inline;
	margin-right: 5px;
}
h2 {
	font-weight: 400;
	margin-top: 0px;
}
h3 {
	font-weight: 400;
	margin-bottom: 30px;
}

footer {
	text-align: right;
	margin-top: 80px;
	padding-bottom: 20px;
}

p {
	line-height: 1.5em;
	margin-top: 20px;
	display: block;
}

a {
	color: rgb(10, 57, 199); /*inherit;*/
	word-wrap: break-word;
	overflow-wrap: break-word;
    text-decoration: underline;
}
a:hover, a:focus {
    text-decoration: underline;
	/*color: var(--url-color);*/
}
code {
  background-color: rgb(255, 228, 171);
  border-radius: 5px;
  font-size: 1.2em;
}
.code-box {
  display: inline-block;
  padding: 4px 10px;
  background-color: rgba(255, 228, 171, 1.0);
  border-radius: 8px;
  font-family: Monospace;
  font-size: 1.2em;
}
.highlight-box {
  display: inline-block;
  padding: 4px 10px;
  background-color: rgba(12, 12, 12, 0.08);
  border-radius: 8px;
  font-family: Georgia, 'Times New Roman', serif;
  font-style: italic;
}
pre {
	overflow-x: auto;
	white-space: pre-wrap;
	word-wrap: break-word;
}

ol {
    counter-reset: item;
}
ol>li {
    counter-increment: item;
    list-style: none inside;
    margin: 20px 0;
    overflow: hidden;
    line-height: 1.5em;
}
ol>li:before {
    content: counter(item) ;
    margin-right: 10px;
    padding: 8px;
    border-radius: 50%;
    width: 25px;
    background: var(--text-color);
    color: var(--back-color);
    text-align: center;
    float: left;
    font-weight: 700;
}

@media (max-width: 600px) {
  main {
  	max-width: 100%;
    padding: 20px 15px;
    font-family: 'Roboto Flex', sans-serif;
    font-size: 1.1em;
  }
  h2 {
    font-size: 0.9em;
    margin-top: 5px;
  }
}

</style>
</head>
<body>

<main>
<h1>Paradis artificiels ‚ú®</h1>
<h2><i>Carnet de notes d'un traducteur passionn√© d'IA</i></h2>
<p style="color: var(--url-color); font-size: 0.9em; margin-top: 10px;">NB : les notes ci-dessous sont tap√©es "√† la main" SANS correcteur orthographique.</p>
<p style="color: var(--url-color); font-size: 0.9em; margin-top: 10px;">Derni√®re modification : 02/12/2025 √† 10:35:28</p>

<p><b>02-12-2025</b></p>
<p>Suite √† l'appel du projet Llamafile pour un "retour utilisateur", je poste <a href="https://github.com/mozilla-ai/llamafile/discussions/809">sur leur Github</a> un petit r√©sum√© de ce que Llamafile repr√©sente pour moi. Ce matin je suis ravi de constater que Justine Tunney, l'initiatrice du projet, a lik√© mon post.üòÄ</p>
<p><b>01-12-2025</b></p>
<p>J'enregistre une petite vid√©o sur des essais de g√©olocalisation d'images avec <a href="https://nicolas-dufour.github.io/plonk">l'outil de Nicolas Dufour</a>. Je fais des copies d'√©cran √† partir d'une video YouTube et l'outil identifie correctement le lieu comme √©tant du c√¥t√© de Kuala Lumpur, bluffant.</p>
<p>Je fais une session avec OpenCode en utilisant Big Pickle comme mod√®le. Big Pickle semble gratuit et propose une fen√™tre contextuelle de 200 K tokens et 128 K tokens en sortie. Je d√©poussi√®re un vieux script qui effectue une recherche sur Internet avec des mots-cl√©s en utilisant DuckDuckGo puis produit un r√©sum√© en interrogeant un mod√®le de Groq. OpenCode utilise uv comme je lui ai dit de le faire, cependant il privil√©gie une approche classique, cr√©e un environnement virtuel, un dossier .git, etc. J'ai l'impression que la performance se d√©grade l√©g√®rement √† mesure que le contexte se remplit mais globalement je parviens √† mes fins et r√©souds tous les probl√®mes rencontr√©s. Session tr√®s agr√©able et didactique.</p>
<p><b>30-11-2025</b></p>
<p>Dans la cat√©gorie "youtubeurs insupportables"üòÖ, Alex Finn serait certainement tr√®s bien plac√©, peut-√™tre m√™me devant NetworkChuck ou Matthew Berman. Mais je dois avouer que <a href="https://youtu.be/wxFt2d7xN44">sa derni√®re vid√©o</a> sur Claude Code m'a s√©duit. Le ton qu'il emploie, tr√®s enjou√© et spontan√©, donne une impression de sinc√©rit√©. Le fait qu'il insiste avant tout sur la simplicit√© me plait. Il d√©nonce les nombreuses autres vid√©os sur le m√™me sujet de la part de soi-disant gourous de la tech, avec leurs recettes compliqu√©es pour tirer le meilleur parti de Claude Code. Il semble dire : arr√™tez de vous prendre la t√™te, Claude Code est plus intelligent que vous ne le croyez ! Pas besoin de le tenir par la main et de lui expliquer la vie. En outre Finn d√©nonce l'attitude de certains d√©veloppeurs qui pr√©f√®rent s'amuser pendant que l'IA bosse (avec toutes sortes d'<a href="https://www.ycombinator.com/launches/OgV-chad-ide-the-first-brainrot-ide">applications d√©biles</a> de "doom scrolling") au lieu de rester concentr√©s sur ce qu'il sont en train de faire.</p>
<p><b>29-11-2025</b></p>
<p>Je teste <a href="https://huggingface.co/bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF/blob/main/mistralai_Voxtral-Mini-3B-2507-Q6_K.gguf">une version quantis√©e</a> de 3.3 Go du mod√®le Voxtral de Mistral avec llama-cpp. J'essaie d'obtenir la transcription d'un MP3 de 43 secondes. J'utilise la commande suivante pour lancer le serveur web local :</p>
<p><pre><code>llama-server -hf bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF:Q6_K --temp 0</code></pre></p>
<p>J'importe le MP3 et l'accompagne du prompt suivant : "Transcribe precisely this audio segment".</p>
<p>Le temps pour obtenir la transcription (CPU only) est de 1:46 min (1:24 min o√π il ne se passe rien √† l'√©cran et 22 secondes o√π l'on peut voir la transcription s'afficher en streaming). Le r√©sultat est moyen :</p>
<p><span class="code-box">I use Claude a lot. As a hobbyist, I run it in a VM several times a week on side projects, often with the dangerous skip permissions flag to just vibe code whatever idea is on mind. Professionally, part of my team builds the AI/DE rules and tooling for our engineering team, and they consume several billion tokens per month for Claude. The CLI agent space is crowded. Between Claude, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But to be honest, when I talk to other developers, their choice often comes down to what feels like superficial, a lucky feature implementation, or a system prompt vibe they just prefer."</span></p>
<p>Second essai avec le m√™me mp3 et le mod√®le "whisper small" encapsul√© dans un llamafile (<a href="https://huggingface.co/Mozilla/whisperfile/blob/55cdd186651c87529f4442981fe1726cdfe0f188/whisper-small.llamafile">whisper-small.llamafile</a>) de 725 Mo que j'avais conserv√© sur un disque dur. Voici la commande :</p>
<p><pre><code>./whisper-small.llamafile -f transcoded-test-2.mp3 -pc</code></pre></p>
<p>Le r√©sultat est sans appel : la transcription est quasi parfaite et le temps d'inf√©rence est de seulement 8 secondes !</p>
<p><span class="code-box">I use Claude code. A lot. As a hobbyist, I run it in a VM several times a week on side projects, often with the dangerously skipped permissions flag to just vibe code whatever idea is on my mind. Professionally, part of my team builds the AI IDE rules and tooling for our engineering team, and they consume several billion tokens per month just for code agent. The CLI agent space is getting crowded. Between Claude code, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But to be honest, when I talk to other developers, their choice often comes down to what feels like Superficials, a lucky feature implementation, or a system prompt vibe they just prefer.</span></p>
<p>Je suis tr√®s surpris de la qualit√© du r√©sultat, de la vitesse de traitement et du fait que ce mod√®le llamafile, vieux de plus d'un an, √©pelle correctement Claude Code, Gemini CLI etc. Il est meilleur sur tous les tableaux !</p>
<p><b>28-11-2025</b></p>
<p>Amazon d√©cide finalement de me livrer 2 articles sur 4 aujourd'hui üòê</p>
<p>Je tombe sur une vid√©o qui me fait retourner voir ce que <a href="https://github.com/ggml-org/llama.cpp">llama-cpp</a> devient. Sur Github j'ai rarement vu une activit√© aussi soutenue que celle de Georgi Gerganov. A chaque fois que je regarde le "release" le plus r√©cent de llama-cpp, il est vieux de quelques heures, parfois de quelques minutes seulement.</p>
<p>Je t√©l√©charge la version <code>llama-b7189-bin-ubuntu-x64.zip</code> que je d√©compresse dans Downloads. Puis je copie le contenu du r√©pertoire <code>bin</code> dans mon <code>~/.local/bin</code> (qui contient entre autres <code>uv</code> et <code>uvx</code>). Je peux maintenant lancer des commandes pour ex√©cuter llama-cpp depuis n'importe quel r√©pertoire.</p>
<p>La nouveaut√© dont il est question dans la vid√©o et que je veux tester est la commande <code>llama-server</code> qui permet de lancer une interface web tr√®s similaire √† celle de ChatGPT. On peut la lancer de la mani√®re suivante</p>
<p><pre><code>llama-server -hf ggml-org/gemma-3-1b-it-GGUF</code></pre></p>
<p>ce qui t√©l√©charge automatiquement un mod√®le de Hugging Face et lance un serveur sur <a href="http://127.0.0.1:8080/">http://127.0.0.1:8080/</a>.</p>
<p>On peut d√©cider de t√©l√©charger manuellement un autre mod√®le et le lancer avec la commande suivante</p>
<p><pre><code>llama-server -m gemma-3-4b-it-Q4_K_M.gguf</code></pre></p>
<p>La page web du chatbot se comporte de mani√®re assez standard, avec quelques "features" sympatiques comme la possibilit√© d'afficher en temps r√©el la taille de la fen√™tre contextuelle, le nombre de tokens g√©n√©r√©s et la vitesse de l'inf√©rence en tokens par seconde. On peut ainsi exp√©rimenter et voir la taille de la fen√™tre contextuelle d√©cro√Ætre au fur et √† mesure que la conversation avance. On voit le nombre total de token g√©n√©r√©s apr√®s chaque requ√™te.</p>
<p>Par d√©faut je constate que j'ai une fen√™tre contextuelle de 4096 tokens, ce qui est loin des 128 K annonc√©s. Apr√®s <a href="https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune">quelques recherches</a> je me rends compte que 4096 est la valeur par d√©faut de llama-cpp, et qu'on peut l'augmenter en passant <code>--ctx-size 8192</code> ou <code>--ctx-size 16384</code> au moment de lancer le serveur :</p>
<p><pre><code>llama-server -m gemma-3-4b-it-Q4_K_M.gguf --ctx-size 8192</code></pre></p>
<p>Si par exemple on remplit compl√®tement la fen√™tre contextuelle, ce que j'ai fait volontairement pour voir, la g√©n√©ration s'arr√™te net. On peut alors arr√™ter le serveur (<code>CTRL C</code>) et le red√©marrer avec une fen√™tre contextuelle plus grande, puis relancer la derni√®re requ√™te de l'interface web et poursuivre la conversation. Enfin, on peut exporter chaque conversation dans un fichier JSON.</p>
<p><b>26-11-2025</b></p>
<p>Avec mon outil yt-trend, j'exp√©rimente l'acc√®s aux API gratuites propos√©es ici ou l√†. En l'occurence, je cible les h√©bergeurs de LLM suivants : Mistral, Groq, OpenRouter, Hugging Face et Github. Je constate que lorsqu'on envoie une requ√™te √† un LLM via une API, on peut essuyer un refus, m√™me pour un mod√®le classifi√© de gratuit. Il me semble que la taille (en tokens) de la requ√™te (voir "Requested" ci-dessous) est un √©l√©ment essentiel. Les fournisseurs mettent des limites sur le nombre de tokens en entr√©e et ce nombre peut varier selon les mod√®les d'un m√™me fournisseur. C'est un peu au petit bonheur la chance et il est difficile d'avoir un service fiable dans la dur√©e. J'essaie par exemple de faire analyser le contenu d'un fichier JSON de 93 Ko et j'obtiens :</p>
<p><pre><code>An error occurred during the API call: Error code: 429 - {'error': {'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'qwen/qwen3-235b-a22b:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations', 'provider_name': 'Venice'}}</code></pre></p>
<p>Avec Groq, m√™me fichier, autre message, plus pr√©cis :</p>
<p><pre><code>An error occurred during the API call: Error code: 413 - {'error': {'message': 'Request too large for model `llama-3.3-70b-versatile` in organization `12345` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 29369, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}</code></pre></p>
<p>ou bien</p>
<p><pre><code>An error occurred during the API call: Error code: 400 - {'error': {'message': 'Please reduce the length of the messages or completion.', 'type': 'invalid_request_error', 'param': 'messages'}}</code></pre></p>
<p>ou encore</p>
<p><pre><code>An error occurred during the API call: Error code: 429 - {'error': {'message': 'Rate limit reached for model `meta-llama/llama-4-scout-17b-16e-instruct` in organization `12345` service tier `on_demand` on tokens per minute (TPM): Limit 30000, Used 23115, Requested 28914. Please try again in 44.058s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}</code></pre></p>
<p>Chose int√©ressante concernant la r√©ponse ci-dessus : le mod√®le mobilis√© est "meta-llama/llama-4-scout-17b-16e-instruct" alors que j'avais s√©lectionn√© "groq/compound" dans la liste de mod√®les de l'API de Groq. Autre petit d√©tail : la taille de ma requ√™te (en tokens) varie l√©g√®rement selon les mod√®les (29369 vs 28914 par ex.).</p>
<p><b>25-11-2025</b></p>
<p>Nouvelle session avec Antigravity. Tout se passe bien mais Gemini 3.0 doit √™tre sursollicit√© et met un temps fou √† effectuer les t√¢ches qu'il a lui-m√™me d√©fini suivant mon prompt. Chose int√©ressante, il d√©cide de donner une r√©ponse √† mon probl√®me de mani√®re d√©terministique, sans faire appel √† un LLM en ligne. Il a d√©tect√© que ce que je demande est moins compliqu√© qu'il n'y para√Æt. Le r√©sultat fonctionne mais n'est pas √† la hauteur de mes attentes. Il faudra probablement refaire appel √† un LLM pour obtenir quelque chose de correct.<br>------<br>Je passe une commande sur Amazon (4 articles). Mon interaction avec l'algorithme amazonien pourrait √™tre transpos√© en la sayn√®te suivante.</p>
<p>Amazon : Je peux te livrer gratuitement tous tes articles jeudi 27/11 si tu souscris √† Prime. Tu b√©n√©ficieras de ce service gratuitement pendant un mois et tu peux annuler √† tout moment.<br>Moi : Hmmm.<br>Amazon : Sinon, je peux te livrer ce weekend, mais un colis samedi et un autre dimanche.<br>Moi : Oh la barbe. Deux livraison en deux jours. Et le weekend de surcro√Æt.<br>Amazon : Je peux te livrer jeudi m√™me si tu ne souscris pas √† Prime, mais cela te co√ªtera 6,50 euros. Bah oui.<br>Moi : Je devrais peut-√™tre choisir l'essai gratuit de Prime, annulable √† tout moment. Il faut prendre des risques, vivre dangereusement quoi !<br>Amazon : Bah oui ! Tout le monde fait √ßa main'nan.<br>Moi : Oui mais je risque d'oublier d'annuler. Tant pis, je choisis la double livraison. Ce sera un "weekend Amazon". Je valide ma commande.<br>Amazon : Nan, mais c'√©tait pour de faux ! J'peux tout t'livrer en une fois. Dimanche.<br>Moi : (√† moi m√™me) Heureusement que je n'ai pas c√©d√© aux sir√®nes amazoniennes.<br>------<br>Rentr√© du travail, allong√© sur mon canap√© dans le noir, j'√©coute <a href="https://www.youtube.com/watch?v=xfm99Tb7CNo">une autre vid√©o</a> d'Armin Ronacher, le d√©veloppeur autrichien n√© au moment o√π je mettais les pieds √† Londres pour la premi√®re fois. Je me concentre sur sa voix et ce qu'il veut transmettre. Je devine que le support visuel n'est pas tr√®s important. C'est vraiment tr√®s int√©ressant d'avoir la possibilit√© d'√©couter l'avis ‚Äîla confidence presque‚Äî d'un d√©veloppeur professionnel sur les nouveaux outils d'agentic coding, comme il les appelle. Il dit ne pas vouloir les nommer ni en choisir un, mais il transpara√Æt que Claude Code est son favori parmi la trentaine d'outils similaires disponibles au moment o√π il enregistre sa vid√©o, le 8 ao√ªt dernier, autant dire il y a une √©ternit√© en temps IA. La vid√©o semble avoir √©t√© enregistr√©e en une seule prise. Il ne coupe pas les silences, les temps morts, et c'est beaucoup plus agr√©able √† √©couter que les mitraillettes habituelles (<a href="https://www.youtube.com/watch?v=nqFlQJI6OrE">suivez mon regard</a>). On sent que m√™me lui a un peu de mal √† suivre. Il essaie d'expliquer pourquoi il est si difficile de s'y retrouver et d'√©valuer ces outils, qui changent tout le temps. Tr√®s difficile √† cause des multiples param√®tres qui font que la mayonnaise prend ou pas. Il a une compr√©hension tr√®s fine des choses. Il est tr√®s humain. Il dit qu'il en a assez des tweets de deux lignes agr√©ment√©s de smileys. Dans le noir, je trouve son intervention presque po√©tique. Oui, c'en est presque √©mouvant. Mais je m'√©gare, sans doute.</p>
<p><b>24-11-2025</b></p>
<p>Dans une <a href="https://www.youtube.com/watch?v=nfOVgz_omlU">vid√©o</a> passionnante datant de fin juin, Armin Ronacher, un d√©veloppeur, d√©courage fortement l'utilisation des MCP. La raison principale est que les MCP "polluent" le contexte (limit√©) du LLM. Il les trouve aussi "tr√®s ennuyeux √† utiliser" et leur pr√©f√®re l'utilisation d'un outil de ligne de commande.<br>------<br>D√©couverte de <a href="https://github.com/egbertbouman/youtube-comment-downloader">youtube-comment-downloader</a>, un outil pour t√©l√©charger les commentaires d'une vid√©o YouTube. Tr√®s pratique pour analyser le ressenti des internautes sur le contenu de la vid√©o.</p>
<p>Exemple d'utilisation avec <a href="https://docs.astral.sh/uv/">uv</a> :</p>
<p><pre><code>uvx youtube-comment-downloader --youtubeid nfOVgz_omlU --output nfOVgz_omlU.json</code></pre></p>
<p>Le fichier JSON obtenu peut ensuite √™tre analys√© d'une infinit√© de fa√ßons. Par exemple avec LLM, l'outil de Simon Willison (avec le plugin <a href="https://github.com/sebington/llm-hf">llm-hf</a>üòÑ install√©) :</p>
<p><pre><code>cat nfOVgz_omlU.json | llm -m moonshotai/Kimi-K2-Instruct "Analyse these YouTube comments and make a summary of the trends" > output.txt</code></pre></p>
<p><b>23-11-2025</b></p>
<p>Je parviens, au prix de pas mal de t√¢tonnements, √† installer et √† utiliser <a href="https://github.com/microsoft/playwright-mcp">playwright-mcp</a>. Playwright-mcp permet de controller un navigateur web au travers d'instructions r√©dig√©es en langage naturel et envoy√©es √† un LLM √† partir d'un "client" comme Claude Code ou Gemini CLI. Le LLM interpr√®te ces instructions et les traduit en commandes qui vont effectuer des actions dans le navigateur. Pour que √ßa marche il faut que Node.js 18 "or newer" soit install√© sur la machine ET que <a href="https://playwright.dev/">Playwright</a> le soit aussi, chose que j'ignorais. Au d√©but, bien √©videmment √ßa ne marchait pas puisque Playwright n'√©tait pas install√©. Je l'ai install√© "par accident" en essayant d'installer un second MCP (firefox-mcp-server) qui n'a jamais fonctionn√© mais a eu le m√©rite de me faire installer Playwright.</p>
<p>Bon, c'est pas fini. Il faut ensuite copier le code ci-dessous et le coller dans le fichier de configuration du client utilis√©. J'ai fait l'essai avec Gemini CLI et ai donc ajout√© ces instructions au fichier <code>settings.json</code> de Gemini.</p>
<p><pre><code>{
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": [
        "@playwright/mcp@latest"
      ]
    }
  }
}</code></pre></p>
<p>Probl√®me : le fichier <code>settings.json</code> n'est pas vide. Il contient d√©j√† du code et il faut ins√©rer le code ci-dessus dans l'existant sans se tromper au niveau des accolades et des virgules, ce qui demande pas mal d'attention. Au final, dans mon cas, √ßa ressemble √† √ßa :</p>
<p><pre><code>{
  "ide": {
    "hasSeenNudge": true
  },
  "security": {
    "auth": {
      "selectedType": "oauth-personal"
    }
  },
  "mcpServers": {
    "playwright": {
      "command": "npx",
      "args": [
        "@playwright/mcp@latest"
      ]
    }
  }
}</code></pre></p>
<p>Une fois ces pr√©paratifs termin√©s, on peut lancer Gemini CLI, qui d√©tecte automatiquement qu'un MCP server a √©t√© ajout√©. On peut maintenant taper des instructions comme "ouvre le navigateur et rends toi sur le site de telerama.fr", par exemple.</p>
<p>Par s√©curit√©, Playwright-mcp poss√®de son propre navigateur (google chrome) et n'interf√®re donc pas avec le ou les navigateurs d√©j√† install√©s.</p>
<p>Comme test je le fais aller sur komoot.com, je me connecte √† mon compte et me rends sur la page de mes activit√©s termin√©es. Je lui demande de s√©lectionner l'activit√© la plus r√©cente et de t√©l√©charger le fichier GPX de la sortie. Au d√©but je l'aide car il ne trouve pas le bouton "t√©l√©charger" qui est volontairement planqu√© (merci Komoot).</p>
<p>Peu √† peu il comprend comment faire et √† la fin je lui dis simplement "recommence et t√©l√©charge deux autres fichiers" (en les nommant) ce qu'il fait. Mais tout le processus se d√©roule dans une lenteur abyssale. C'est ce qui m'a le plus surpris. Je lui ai pourtant donn√© toutes les autorisations. Un truc qui prendrait normalement quelques minutes √† un humain a pris une bonne demi-heure, sans compter le temps d'installation.</p>
<p>A la fin de la session, je lui demande de sauvegarder la proc√©dure dans un fichier .md pour ne pas √† tout devoir lui r√©-expliquer la prochaine fois. Je regarde la consommation de tokens en faisant <code>/stats</code>. Ce sont surtout les "inputs tokens" qui ont √©t√© consomm√©s : 2M de tokens pour Gemini-2.5-flash-lite et 7M pour Gemini-2.5-pro. Neuf millions de tokens en tout ! On est bien au del√† du quota quotididen d'un million, m√™me si Gemini explique que 70 % des input tokens "were served from the cache, reducing costs". Heureusement que Gemini CLI est gratuit !</p>
<p><b>22-11-2025</b></p>
<p>Ce matin je cherche √† comprendre la diff√©rence fondamentale entre le "sudo apt install" d'Ubuntu et le "yay -S" de Manjaro. <a href="https://claude.ai/share/c5300daf-b6ff-421a-995e-c0a5f7b373fb">R√©ponse tr√®s √©clairante</a> de Claude (ne pas lire "it's" mais "its"). Yay est tr√®s puissant et pratique une fois qu'on a compris de quoi il retourne.</p>
<p>Hier mon coll√®gue JB me parle d'un projet d'analyse d'images d'√©tiquettes de bouteilles de vin. Je commence une r√©flexion sur une approche possible du probl√®me. Profusion de possibilit√©s.</p>
<p>Je visionne <a href="https://youtu.be/0HqUYpGQIfs">la derni√®re vid√©o en date</a> de l'excellente cha√Æne "Emergent Garden" et tombe sur ceci : "An idiot admires complexity. A genius admires simplicity." ‚ÄîTerry Davis.<br>------<br>Je continue d'exp√©rimenter avec yt-datetime-search. Je me rends compte que la mani√®re avec laquelle la recherche s'effectue est assez opaque, et repose davantage sur l'algorithme de Youtube que sur les fonctionalit√©s de yt-dlp. Je demande √† Antigravity/Sonnet 4.5 les endroits dans la page YouTube de la vid√©o o√π sont cherch√©s les mots-cl√©s. J'obtiens la r√©ponse suivante :</p>
<p><span class="highlight-box">The keywords are searched in the title, description, tags, and channel name - but NOT in comments. YouTube's algorithm prioritizes title matches, but will also return videos where keywords appear only in descriptions or tags.</span></p>
<p>J'essaie d'√©tendre les capacit√©s de l'outil √† d'autres recherches que celle d'un buzz en cours. Par exemple, j'essaie de trouver la vid√©o la plus ancienne du projet "llamafile". J'utilise "llamafile" comme unique mot-cl√© et d√©finis le nombre maximum de vid√©os √† 300. Le script renvoie 283 vid√©os et je constate que "llamafile" est seulement pr√©sent dans 97 titres. Cependant, l'outil fonctionne : √† chaque fois il permet de trouver la vid√©o la plus ancienne. Dans le cas pr√©sent il semble s'agir de <a href="https://www.youtube.com/watch?v=86lmdc0QP4c">celle-ci</a>, sortie deux jours seulement apr√®s l'<a href="https://hacks.mozilla.org/2023/11/introducing-llamafile/">annonce officielle</a> du projet llamafile, le 29 novembre 2023.</p>
<p>Au passage, je v√©rifie que la date du premier "commit" de llamafile sur Github (8 novembre 2023) est bien ant√©rieure √† la date de l'annonce officielle. Pour trouver cette info, je fais un <code>git clone</code> du repo, puis <code>cd llamafile</code> et enfin <code>git log --reverse</code> (q pour sortir).</p>
<p>Apr√®s de nombreuses modifications de yt-datetime-search, j'opte pour une s√©lection plus stricte par mots-cl√©s du titre uniquement, coupl√©e √† un nombre maximum de vid√©os d√©fini par l'utilisateur. Je laisse tomber l'id√©e du "time gap" entre 2 vid√©os, qui ne sert √† rien puisque le tri ne se fait pas en temps r√©el comme je le pensais mais uniquement apr√®s que l'ensemble des m√©tadonn√©es aient √©t√© r√©cup√©r√©es.<br>------<br>Je fais un premier essai pour le projet de JB avec un script g√©n√©r√© par Claude qui utilise l'API d'Anthropic pour analyser des images d'√©tiquettes de vin et r√©cup√®re des donn√©es structur√©es dans un fichier JSON. Je trouve <a href="https://huggingface.co/datasets/Francesco/wine-labels">un dataset</a> sur Hugging Face. Je demande ensuite √† Antigravity/Sonnet de modifier le script pour utiliser l'API de Groq, qui, via le mod√®le <code>meta-llama/llama-4-scout-17b-16e-instruct</code> peut tester gratuitement des images "par groupes de 5 maximum". J'essaie avec 6 juste pour voir et √ßa passe. Je demande aussi que l'utilisateur ait seulement √† saisir le nom du r√©pertoire o√π se trouvent les images. Voici <a href="http://bytepacking.free.fr/wine_labels_analysis.json">le fichier JSON</a> du r√©sultat. Ce n'est pas parfait mais pas si mal pour un d√©but.</p>
<p><b>21-10-2025</b></p>
<p>Essai d'am√©lioration de l'outil d√©crit hier (baptis√© "yt-datetime-search") avec Antigravity. L'id√©e est la suivante : utiliser <a href="https://github.com/TeamNewPipe/NewPipeExtractor.git">NewPipeExtractor</a> pour potentiellement am√©liorer yt-datetime-search. Je me dis que NewPipeExtractor, une biblioth√®que tr√®s robuste √©crite en Java, doit poss√©der des fonctionnalit√©s int√©ressantes. Je demande √† Antigravity d'analyser le code de ce d√©p√¥t et lui donne aussi 2 versions en Python de yt-datetime-search. Antigravity d√©tecte plusieurs am√©liorations possibles. Je choisis la suivante :</p>
<p><span class="highlight-box">L'objectif est d'am√©liorer yt-datetime-search en ajoutant un "parser" de dates relatives robuste, port√© √† partir du code source Java npext. Cela permettra au script d'analyser les cha√Ænes de caract√®res "time ago" (par exemple, "2 days ago") lorsque l'horodatage exact est absent des m√©tadonn√©es r√©cup√©r√©es par yt-dlp.</span></p>
<p>Antigravity fait un "Implementation Plan", que je valide et lui donne le feu vert. Il effectue les modifications, cr√©e un fichier de test et produit un rapport ("Walkthrough") √† la fin.</p>
<p>Je n'ai plus qu'√† tester pour voir si √ßa marche, et √ßa marche. A noter qu'en g√©n√©ral je trouve Antigravity plus v√©loce que Gemini CLI, qui √©tait particuli√®rement lent hier soir et avait parfois tendance √† p√©daler dans la semoule.</p>
<p><b>20-11-2025</b></p>
<p>Claude est dans les choux ce matin (lentissime), ce qui est rare. J'utilise Mistral √† la place.</p>
<p>Je me rends compte que lorsqu'on utilise uv et notamment les "uv tools", on peut avoir 2 installations distinctes du m√™me logiciel sur sa machine. C'est par exemple le cas pour yt-dlp, que j'ai install√© comme outil dans uv tools en faisant <code>uv tool install yt-dlp</code> et que j'ai √©galement install√© via la commande plus classique <code>sudo snap install yt-dlp</code> sur Ubuntu. Probl√®me : quand je lance yt-dlp, comment savoir quelle version est utilis√©e ? R√©ponse : il faut taper <code>which yt-dlp</code>. Si cela renvoit qq ch comme <code>/usr/bin/yt-dlp</code>, c'est la version "system-wide" qui est utilis√©e par d√©faut. Si en revanche cela renvoit qq ch comme <code>~/.local/bin/yt-dlp</code>, il est plus probable qu'il s'agisse d'une version install√©e via uv ou pip.</p>
<p>Ce soir avec Gemini CLI je cr√©e un outil bas√© sur yt-dlp qui r√©cup√®re toutes les vid√©os YouTube correspondant √† certains mots-cl√©s et les trie par ordre de "timestamp" (date et heure) de mise en ligne en v√©rifiant le temps √©coul√© entre chaque vid√©o. L'id√©e est (en th√©orie) de retrouver la premi√®re vid√©o mise en ligne pour un sujet donn√©. Le choix des mots cl√© est crucial. Par exemple avec les mots "google" et "antigravity" le script r√©cup√®re (√† 23h19) les m√©tadonn√©es de 336 vid√©os, dont <a href="https://www.youtube.com/watch?v=L8wEC6A5HQY">la plus ancienne</a> a √©t√© mise en ligne le 18-11-2025 √† 14:34:33. La vid√©o suivante r√©cup√©r√©e avec les m√™mes mots-cl√©s est plus ancienne de 851 heures (~ 35 jours), et n'appartient donc pas √† la "vague" actuelle concernant la sortie d'<a href="https://antigravity.google/">Antigravity</a> le 18 novembre.</p>
<p><b>18-11-2025</b></p>
<p>Aujourd'hui Olivier essaie de faire fonctionner le mod√®le Voxtral de Mistral en local sur sa machine. Sur la <a href="https://huggingface.co/mistralai/Voxtral-Mini-3B-2507">page Hugging Face du mod√®le</a>, la doc pr√©conise d'utiliser un truc qui s'appelle vLLM. J'essaie avec Voxtral-Mini-3B-2507 (10 Go) sur Google Colab. Avec vLLM, √ßa ne marche pas, alors j'essaie avec Transformers et <a href="https://colab.research.google.com/drive/1WAvmXZXHE3Aa9quGjBHgcrCYR8ac1G1H?usp=sharing">√ßa fonctionne</a>. Je teste les √©chantillons de code "transcribe" et "multi-audio + text instruction". Je dois red√©marrer la session entre chaque essai autrement je sature la m√©moire de Colab. Je constate que le mod√®le est bien t√©l√©charg√© sur la machine (2 fichiers de 5 Go) lors du premier run, et que red√©marrer la session ne m'oblige pas √† tout re-t√©l√©charger.</p>
<p>Un peu plus tard, Olivier essaie avec le mod√®le "Voxtral-Small-24B-2507" (48 Go), et constate qu'il ne fonctionne pas avec son setup (GPU avec 24 Go de RAM). Je trouve l'info suivante : "Running Voxtral-Small-24B-2507 on GPU requires ~55 GB of GPU RAM in bf16 or fp16". Du coup Olivier d√©cide d'essayer <a href="https://huggingface.co/bartowski/mistralai_Voxtral-Small-24B-2507-GGUF">une version quantiz√©e</a> du m√™me mod√®le avec llama.cpp comme moteur d'inf√©rence et √ßa fonctionne (<code>llama-server -hf  bartowski/mistralai_Voxtral-Mini-3B-2507-GGUF:Q6_K</code>). Mais il semblerait que la quantization affecte tout de m√™me un peu les performances du mod√®le.</p>
<p>Ce soir, no rest for the wicked. Google annonce la sortie de <a href="https://blog.google/technology/developers/gemini-3-developers/">Gemini 3</a> et d'<a href="https://antigravity.google/">Antigravity</a>. Antigravity est une sorte d'√©ni√®me m√©lange de VSCode et de Cursor. Je le teste avec deux scripts en Python que je souhaite combiner et √ßa marche tr√®s bien. Mais apparemment rien que je n'aurais pu faire dans Claude Code ou Gemini CLI. Un peu plus tard je le fais bosser avec Sonnet 4.5 sur une appli g√©n√©r√©e en TypeScript par Google AI Studio. Il la modifie et la rend compatible avec Android Studio et pr√™te √† √™tre export√©e en fichier APK installable sur un smartphone. Question mod√®les, on peut choisir entre Gemini 3, Claude Sonnet 4.5 et les mod√®les GPT-OSS d'OpenAI. Quelle journ√©e !</p>
<p><b>17-11-2025</b></p>
<p>Olivier me parle d'<a href="https://www.affinity.studio/">affinity</a> comme alternative √† InDesign pour ouvrir les fichiers IDML. Je t√©l√©charge l'appli et la teste. Elle permet sans probl√®me de visualiser in fichier IDML. Bon √† savoir.</p>
<p>D√©couverte de la <a href="https://www.youtube.com/@mattpocockuk/videos">cha√Æne YouTube</a> de Matt Pocock, un adepte de TypeScript. Ses conseils ou explications en mati√®re d'IA me semblent tr√®s claires et utiles et loin des "buzzeries" habituelles.</p>
<p>Le site <a href="https://models.dev/">https://models.dev/</a> liste tous les LLM et leurs "providers" avec les tarifs des API et autres informations utiles. Olivier y d√©couvre <a href="https://iflow.cn/">iflow.cn</a>, qui propose une API gratuite et de nombreux mod√®les. China taking over?</p>
<p>R√©flexion sur le module qui permet d'√©couter le texte de <a href="https://blog.sshh.io/p/how-i-use-every-claude-code-feature">cet excellent billet de blog</a> sur Claude Code. Je me demande si c'est une IA qui g√©n√®re la voix "√† la vol√©e" ou si c'est un fichier mp3 pr√©-enregistr√©. En fait il s'agit un peu des deux : c'est un mp3 (dont je finis par trouver le lien dans le code source) qui a certainement √©t√© g√©n√©r√© par une IA sur une version ant√©rieure du texte avant sa mise en ligne. En effet la version audio ne colle pas tout √† fait au texte de la page.</p>
<p><b>16-11-2025</b></p>
<p>Suite du "InDesign Challenge". Olivier me propose de tester le fichier IDML g√©n√©r√© hier. Il me renvoie un fichier PDF sans les images qui semble tenir la route. Entre temps j'ai eu une autre id√©e. Comme le r√©sultat de l'importation des fichiers JSON dans memoQ n'est pas id√©al (pr√©sence de nombreux √©l√©ments qui n'ont pas besoin d'√™tre traduits), je me dis que l'on pourrait g√©n√©rer un second fichier JSON plus simple √† partir du premier. Ce second fichier ne contiendrait que trois cl√©s : "path" suivi d'un identifiant, "paragraph_index" suvi de 0, 1, 2, etc. et "text" suivi de la cha√Æne de caract√®res √† traduire.</p>
<p>Je demande √† Claude de me construire un nouveau script pour cela. Ce nouveau script extrait uniquement l'information n√©cessaire √† la traduction et est capable de r√©encapsuler le r√©sultat dans le fichier dont il est issu. C'est une histoire de poup√©es russes : IDML-source > JSON_1-source > JSON_2-source > traduction > JSON_2-cible > JSON_1-cible > IDML-cible. Apr√®s quelques aller-retour avec Claude tout semble fonctionner et Olivier teste IDML-cible et produit un nouveau fichier PDF valide.</p>
<p>Maintenant je me dis que je pourrais certainement supprimer une des "poup√©es" et obtenir le fichier JSON "l√©ger" directement √† partir du fichier IDML en demandant √† Claude Code ou Gemini CLI de bosser un peu sur mes 3 scripts. C'est ce que je fais avec Gemini CLI, qui fusionne 2 scripts en un seul "all-in-one".</p>
<p><b>15-11-2025</b></p>
<p>Un plugin int√©ressant de LLM est absent de <a href="https://llm.datasette.io/en/stable/plugins/directory.html">la liste officielle</a> : <code>llm-github-models</code>. Ce <a href="https://github.com/tonybaloney/llm-github-models">plugin</a> donne acc√®s √† ~62 mod√®les h√©berg√©s(?) par Github, dont plusieurs mod√®les d'OpenAI, comme par exemple <code>github/gpt-4.1</code>. Pour que √ßa fonctionne, il  faut au pr√©alable se cr√©er une cl√© sur <a href="https://github.com/settings/tokens">https://github.com/settings/tokens</a>.<br>------<br>Nouveau challenge : j'ai une BD √† traduire √† partir d'un fichier InDesign (.idml). L'importation de ce fichier dans memoQ fonctionne, mais la segmentation des phrases ne fonctionne pas. Je me retrouve avec des fragments de phrases et jamais les phrases compl√®tes.</p>
<p>J'explique le probl√®me √† Claude, qui me propose un script en Python qui convertit le fichier IDML en JSON, en faisant en sorte de "respecter" le texte. √áa semble marcher, m√™me si le texte est dupliqu√© dans 2 "cl√©s" diff√©rentes du fichier JSON. L'id√©e est de traduire le texte contenu dans le fichier JSON et de reconstruire le fichier IDML. Je fais des essais et √ßa semble marcher, mais je ne poss√®de malheureusement pas le logiciel InDesign pour v√©rifier que √ßa a VRAIMENT fonctionn√©.</p>
<p><b>14-11-2025</b></p>
<p>J'ai retrouv√© les logs de Claude Code pour la construction du plugin llm_hf (fichiers JSONL). Essentiellement 4 sessions du 2 au 4 novembre 2025. Leur conversion en HTML ne restitue malheureusement pas le rendu du terminal. Pour cela il faut copier tout le contenu brut du terminal juste avant de quitter la session et le coller dans un <a href="https://tools.simonwillison.net/terminal-to-html">convertisseur appropri√©</a>.</p>
<p>Cependant ces fichiers HTML (<a href="http://bytepacking.free.fr/2025-11-02_18-07-22.html">exemple</a>) contiennent √©norm√©ment d'information, beaucoup plus que ce que l'on voit √† l'√©cran lorsqu'on se sert de Claude Code. Il serait impossible de suivre en temps r√©el tout le d√©roul√© du processus. Prendre le temps d'inspecter ces "logs" convertis en HTML permet de prendre conscience de la quantit√© de tokens "ing√©r√©s" par la machine pour faire son travail.<br>------<br>Je teste Voxtral, le mod√®le Speech-to-Text de Mistral, en suivant <a href="https://simonwillison.net/tags/speech-to-text/">les instructions de Willison</a> pour voir si √ßa fonctionne toujours de la m√™me fa√ßon depuis juillet.<br>J'installe le plugin Mistral pour LLM :</p>
<p><pre><code>llm install -U llm-mistral</code></pre></p>
<p>Ensuite je tente un test avec un extrait d'une interview de Willison (fichier WAV) en laissant volontairement le fichier en local pour voir si √ßa fonctionne comme avec l'API de Groq par exemple :</p>
<p><pre><code>llm -m voxtral-small -a test.wav</code></pre></p>
<p>J'obtiens un message d'erreur :</p>
<p><pre><code>Error: This model does not support attachments of type 'audio/wav', only audio/mpeg</code></pre></p>
<p>Je convertis le fichier WAV en MP3 avec VLC et r√©essaie :</p>
<p><pre><code>llm -m voxtral-small -a test.mp3</code></pre></p>
<p>Nouvelle erreur :</p>
<p><pre><code>Error: No key found - add one using 'llm keys set mistral' or set the LLM_MISTRAL_KEY environment variable</code></pre></p>
<p>J'ai oubli√© d'entrer la cl√© API :</p>
<p><pre><code>llm keys set mistral
Enter key: *******</code></pre></p>
<p>Nouvel essai :</p>
<p><pre><code>llm -m voxtral-small -a test.mp3</code></pre></p>
<p>Nouvelle erreur :</p>
<p><pre><code>Error: Audio attachment must use a URL</code></pre></p>
<p>Ils auraient pu le dire plus t√¥t, mais bon cela confirme que l'API de Mistral ne fonctionne pas comme celle de Groq.<br>J'upload mon fichier test.mp3 sur le serveur de ce blog et r√©essaie :</p>
<p><pre><code>llm -m voxtral-small -a http://bytepacking.free.fr/test.mp3</code></pre></p>
<p>Cette fois √ßa marche et j'obtiens <a href="http://bytepacking.free.fr/test.txt">une transcription</a> tr√®s fid√®le. Le fait de devoir mettre le fichier en ligne est quand m√™me tr√®s contraignant.<br>------<br>Je demande √† Gemini 2.5 Pro de transcrire le m√™me fichier (<code>Transcribe this audio file</code>) ce qu'il fait en quelques secondes. <a href="http://bytepacking.free.fr/test_gemini.txt">R√©sultat</a>.<br>------<br>Google Colab vs Github Codespaces</p>
<p>Le look de l'interface des notebooks de Google Colab a √©volu√© (en mieux je trouve) et ils ont ENFIN ajout√© un terminal. All√©luia. On peut par exemple installer uv en faisant :</p>
<p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh</code></pre></p>
<p>Les Codespaces, quant √† eux sont une version de VSCode qui fonctionne dans une machine virtuelle dans le cloud. J'ai essay√© de m'en servir et je trouve l'interface assez "clunky". De plus, par rapport √† Colab, on est sur des machines beaucoup moins puissantes, sans GPU et avec moins de RAM et d'espace disque.</p>
<p>J'ai essay√© d'installer nvidia/parakeet-tdt-0.6b-v2 sur une VM, sans succ√®s (low RAM / Disc space). Sur Google Colab √ßa n'a pas march√© non plus, mais pour d'autres raisons (incompatibilit√© de librairies Python).</p>
<p><b>13-11-2025</b></p>
<p>Aujourd'hui je collecte et regroupe tous les fichiers logs des sessions effectu√©es avec Claude Code et Gemini CLI sur mes diff√©rents ordis depuis le 15 septembre. Les logs de CC sont des fichiers JSONL et ceux de Gemini CLI des fichiers JSON. Je demande √† Claude Web de me cr√©er un script pour chaque outil afin de convertir les logs en HTML, avec une mise en forme plus facile comprendre. Des outils existent en ligne mais ne me satisfont pas. La structure des logs de Claude Code semble plus complexe et foisonnante que celle de Gemini CLI. Gemini a le m√©rite de montrer les moments o√π le mod√®le "r√©fl√©chit", ce qui habituellement transparent pour l'utilisateur. Comme Gemini CLI est gratuit, je poss√®de beaucoup plus de logs "Gemini" que de logs "Claude".<br>------<br>Commande pour conna√Ætre la date d'installation de mon OS (Ubuntu) :<br><code>ls -lt /var/log/installer/ 2>/dev/null</code></p>
<p><b>11-11-2025</b></p>
<p>Retour √† Brest. Je demande √† Gemini-CLI de comparer une liste de 6 scripts de plugins officiels de LLM (openrouter, gemini, anthropic, perplexity, mistral, groq), liste √† laquelle j'ajoute mon plugin candidat "Hugging Face" (<code>llm-hf</code>). Je souhaite v√©rifier la mani√®re avec laquelle les plugins se comportent en mati√®re de connexion aux API.</p>
<p>Je me rends compte que mon plugin n'est pas le seul √† pouvoir utiliser une cl√© API stock√©e dans les variables d'environnement de l'OS alternativement au stockage de cette m√™me cl√© dans un fichier JSON de LLM, ce qui me rassure.</p>
<p>Gemini me r√©pond au sujet des API avant m√™me que je pose la question. Il a d√©tect√© que le script du plugin <code>llm-perplexity</code> est assez complexe (et donc plus sujet √† des pannes potentielles) et me propose de le simplifier, mais je lui demande de s'en tenir aux commentaires.</p>
<p><span class="code-box">A significant distinction [of the llm-perplexity plugin] is its use_openrouter option, which allows it to act as a proxy to the OpenRouter API. This feature is unique to the llm-perplexity plugin and is not shared by the other plugins. When this option is enabled, the plugin dynamically changes the API endpoint and model ID to route requests through OpenRouter, adding a layer of abstraction that is not present in the other plugins.</span></p>
<p>Concernant mon plugin :</p>
<p><span class="code-box">The llm_hf.py plugin is also unique, as it uses the OpenAI library to communicate with a Hugging Face endpoint that is compatible with the OpenAI API.</span></p>
<p>Vraiment incroyable de pouvoir comparer ainsi mon plugin candidat aux plugins officiels pour savoir si je suis √† peu pr√®s dans la norme. J'ai d√©j√† eu recours plusieurs fois √† cette m√©thode, qui semble efficace.</p>
<p>Il faut maintenant que je me penche sur les tests pre et post Github push. Un <code>uvx ruff check</code> d√©tecte 2 probl√®mes, que je corrige avec <code>uvx ruff check --fix</code>.</p>
<p><b>09-11-2025</b></p>
<p>Les nouveaux outils en ligne de commande continuent d'occuper les esprits.</p>
<p>Simon Willison trouve le moyen de "hacker" Codex CLI afin de tester GPT-5-Codex-Mini, un nouveau mod√®le d'OpenAI disponible uniquement via Codex CLI, √©crit en Rust, et non via leur API. De son propre aveu, <a href="https://simonwillison.net/2025/Nov/9/gpt-5-codex-mini/">sa man≈ìuvre</a> est un peu "gonfl√©e".<br>------<br>D√©couverte, gr√¢ce √† un commentaire de <a href="https://www.youtube.com/watch?v=97FYys-kj58">cette vid√©o</a>, de l'existence de <a href="https://docs.z.ai/devpack/overview">GLM</a>, un mod√®le chinois compatible avec les CLI tools comme Claude Code ou Gemini CLI.</p>
<p><b>08-11-2025</b></p>
<p>J'ai trouv√© un moyen de continuer de mettre √† jour ce blog √† partir de mon smartphone. J'ai install√© l'application termux, qui √©mule un environnement Linux, avec un terminal classique en bash. La commande <code>termux-setup-storage</code> permet d'avoir acc√®s au syst√®me de fichiers du smartphone. J'ai ensuite install√© uv (<code>pkg install uv</code>), qui me permet de lancer <code>logbook_ftp.py</code>, le script en Python qui g√©n√®re et met en ligne la page HTML de ce blog √† partir d'un fichier.txt en faisant <code>uv run logbook_ftp.py ai_stream.txt</code>.</p>
<p><b>07-11-2025</b></p>
<p>Ai chang√© la police de caract√®res de ce blog dans la partie du code d√©di√©e aux tablettes et smartphones : <a href="https://fonts.googleapis.com/css2?family=Roboto+Flex:wght@300;400;500;600">Roboto Flex</a>.</p>
<p>Toujours int√©ressant de voir comment les d√©veloppeurs de m√©tier utilisent les outils comme Claude Code. En voici <a href="https://www.youtube.com/watch?v=kZ-zzHVUrO4">un exemple</a>.</p>
<p>Je quitte la Bretagne pour la Normandie sous des trombes d'eau. Dans la voiture, j'√©coute avec beaucoup d'attention et de plaisir un <a href="https://www.heavybit.com/library/podcasts/generationship/ep-39-simon-willison-i-coined-prompt-injection">podcast</a> de Simon Willison o√π il est bien entendu question d'IA toutes les deux secondes.</p>
<p><b>06-11-2025</b></p>
<p>Ce soir j'installe Cursor 2.0, qui fait un peu le buzz sur le web et lui demande de me cr√©er un "docx proofreader". C'est l√† qu'on voit que les d√©mos de certains youtubeurs ne valent pas grand chose. Quand on demande quelque chose d'un tant soit peu complexe, c'est une autre paire de manches. Il faut dire que je ne commence pas par le plus facile. Je poursuis le projet avec Gemini-CLI, qui parvient √† faire fonctionner le truc. Mais le r√©sultat, tr√®s d√©cevant, comme tant d'autres, part √† la poubelle.<br>------<br>J'essaie d'am√©liorer mon plugin llm-hf et demande √† Claude Sonnet 4.5 (version web) de le comparer √† llm-groq, un plugin "production-ready" :</p>
<p><span class="code-box">Considering llm_groq.py as "production-ready", can llm_hf.py be further improved?</span></p>
<p>Claude me fait un rapport tr√®s complet dans lequel il souligne que le point d'am√©lioration le plus important est le "Model Caching", qui est absent de mon script.</p>
<p><span class="code-box">llm_groq.py caches models in a JSON file (groq_models.json), avoiding API calls on every invocation. llm_hf.py fetches from the API each time, which is slow and wasteful.</span></p>
<p>Claude me propose de produire une version am√©lior√©e, que je t√©l√©charge et qui fonctionne tout de suite. Je la met sur Github.</p>
<p>Apr√®s le temps pass√© avec les outils en ligne de commande, le retour √† l'interface web, gratuite de surcro√Æt, est tr√®s agr√©able. Impression de revenir en terrain connu. Cela pose une question br√ªlante : aurais-je pu d√©velopper le plugin SANS le recours aux CLI-tools? Probablement, mais j'aurais mis beaucoup plus de temps.<br>------<br>Olivier me parle de <a href="https://github.com/musistudio/claude-code-router">claude-code-router</a> qui permet d'utiliser CC avec n'importe quel mod√®le.</p>
<p><b>05-11-2025</b></p>
<p>Utiliser Claude Code en mode API co√ªte cher, tr√®s cher. En 7 jours j'ai consomm√© quasiment 10 dollars (pay√©s 12 √† cause de taxes ajout√©es √† chaque transaction). Sur mon relev√© bancaire, j'ai √©t√© factur√© 10,63 EUR. Cela fait une consommation moyenne quotidienne de 1,50 EUR. De plus, leur console d'administration est une vraie usine √† gaz, j'ai eu un mal de chien √† afficher ma consommation depuis le d√©but de mes paiements, le 30 octobre. La plupart des "vues" indiquent des sommes d√©risoires. Je ne sais m√™me plus comment j'ai fini par trouver <a href="https://console.anthropic.com/cost?start_date=2025-10-30">la bonne vue</a>. Visiblement le mode API n'est pas la bonne solution pour utiliser Claude Code. J'en suis √† me demander si les sommes affich√©es √† la fin de chaque session sont correctes. Je vais peut-√™tre regarder du c√¥t√© d'OpenCode avec un mod√®le pas trop cher (DeepSeek, comme le sugg√©rait Olivier), mais √ßa risque de ne pas √™tre aussi performant que CC. Notons que le gros de la consommation est le fait de "tokens entrants" (input tokens). En une semaine, j'en ai consomm√© 17,3 millions !</p>
<p>Ce soir je refais une session avec CC. Comme il me reste quelque chose comme 19 cents, je s√©lectionne Haiku comme mod√®le par d√©faut. A la fin de la session je vais voir combien de cr√©dits il me reste. La premi√®re chose que je constate c'est que je suis d√©biteur de 9 cents ! ‚òπÔ∏è Je croyais na√Øvement que le truc s'arr√™terait automatiquement √† 0,00 mais non. Autre chose : CC a quand m√™me un peu utilis√© Sonnet alors que je lui avais dit de n'utiliser que Haiku. C'est une toute petite somme ($0.0059), mais quand m√™me. Ensuite, quand on utilise CC, Anthropic cr√©e une "sous-cl√© API" √† chaque fois qu'on installe CC sur une machine diff√©rente. Sur ma console d'administration je vois que j'ai ma cl√© API principale (mauve) et 4 cl√© CC (oranges). Ce qui est bizarre, c'est que si j'additionne la consommation de chaque cl√©, j'arrive √† un total de 7,44 dollars et non pas 10,09 comme on pourrait s'y attendre ! WTF!? OK, il y a peut-√™tre un temps de latence avant d'avoir the full picture, mais tout √ßa est quand m√™me assez opaque. üò§</p>
<p>Je poursuis avec OpenCode et leur mod√®le d√©di√© (Zen ?), le seul qui veuille bien fonctionner. Je pars de presque rien (les Skills et les d√©finitions des t√¢ches linguistiques) et n'arrive √† rien de potable. OpenCode semble √† nouveau √™tre en "YOLO mode" par defaut et je le trouve beaucoup moins intuitif que les autres (on ne peut pas faire r√©f√©rence √† un fichier en faisant @fichier par exemple).</p>
<p>Lass√© par les outils CLI, je retourne sur le Claude Web classique et parviens √† g√©n√©rer et √† peaufiner un script qui fonctionne pas si mal au bout de 5 it√©rations. L'utilisateur choisit un mod√®le de Groq et le script lance la r√©vision (copy-editing) du texte et le sauvegarde en incluant dans le nom de fichier le nom du LLM utilis√©. Il est ainsi plus facile de comparer les capacit√©s des diff√©rents mod√®les en mati√®re de correction-r√©vision. En plus √ßa marche avec Word, SANS les Claude Skills, dont je commence √† douter du bien fond√©.</p>
<p>Je teste le plugin "llm-huggingface-plugin" dont je parlais hier et constate qu'il ne fonctionne pas correctement. Le plugin ne r√©cup√®re aucune liste de mod√®les, la fonctionalit√© "register_commands" dans "hooks" est absente, et quand je tente de lancer <code>llm -m hf/meta-llama/Llama-3.2-3B-Instruct "Write a haiku about coding"</code>, j'obtiens un message d'erreur. Il y a de l'espoir !</p>
<p><b>04-11-2025</b></p>
<p>J'ai rendu le d√©p√¥t Github de mon plugin "llm-hf" public : <a href="https://github.com/sebington/llm-hf">https://github.com/sebington/llm-hf</a>. Je n'arr√™te pas de modifier le fichier README.md, il y a toujours quelque chose √† changer.</p>
<p>Cet apr√®s-midi, coup de th√©√¢tre. En me familiarisant avec l'interface de Github et en explorant la partie "Pull Requests" du projet <a href="https://llm.datasette.io/">LLM</a>, je me rends compte que quelqu'un a d√©j√† d√©velopp√© un <a href="https://github.com/SecKatie/llm-huggingface-plugin">plugin similaire</a> il y a 3 mois. Cependant je constate que le projet semble un peu oubli√© et que pour l'instant Simon n'a pas jug√© utile de l'ajouter √† la liste officielle des plugins de LLM. Cela n'enl√®ve rien √† mes efforts et me conforte dans l‚Äôid√©e que le fait d'avoir un objectif bien d√©fini est une des seules fa√ßons de progresser.</p>
<p>Suite √† une remarque d'Olivier, j'ajoute la fonctionalit√© "register_commands" dans "hooks". Pour ce faire je demande √† Claude Code d'examiner le script d'un plugin similaire (llm-groq) et d'en extraire l'information pertinente. Cette fois je ne d√©pose pas le script dans le dossier du projet, je me contente de lui donner l'URL et CC va lui-m√™me lire le script en ligne.</p>
<p>Maintenant, quand on tape <code>llm plugins</code>, on obtient (entre autres) :</p>
<p><pre><code>[
  {
    "name": "llm-hf",
    "hooks": [
      "register_commands",
      "register_models"
    ],
    "version": "0.1.0"
  }
]</code></pre></p>
<p>Et quand on tape <code>llm hf</code>, on obtient d√©sormais :</p>
<p><pre><code>Usage: llm hf [OPTIONS] COMMAND [ARGS]...</code></pre></p>
<p><pre><code>  Commands relating to the llm-hf plugin</code></pre></p>
<p><pre><code>Options:
  -h, --help  Show this message and exit.</code></pre></p>
<p><pre><code>Commands:
  models   List all available Hugging Face models
  refresh  Refresh the list of available Hugging Face models</code></pre></p>
<p>Claude met syst√©matiquement √† jour <code>CLAUDE.md</code> et je lui demande de modifier <code>README.md</code> pour que la doc le soit √©galement.</p>
<p>En revanche, grosse prise de t√™te avec la mise √† jour des fichiers sur Github. Je ne ma√Ætrise pas assez Git. Je fais tout en d√©pit du bon sens, parfois en ligne de commande, parfois avec VSCode, parfois en modifiant les fichiers directement sur Github. C'est le bordel et je perds un temps fou avec √ßa, ce qui est nul. Je sais que CC poss√®de une fonctionnalit√© pour g√©rer Git mais je n'ai pas encore essay√© de m'en servir. Ce serait peut-√™tre plus fluide.</p>
<p>Last but not least, je constate que le mod√®le <code>swiss-ai/Apertus-8B-Instruct-2509</code> ne fonctionne pas.</p>
<p><b>03-11-2025</b></p>
<p>Je dois modifier mon plugin (renomm√© "llm-hf") pour le faire fonctionner comme les autres plugins de <a href="https://llm.datasette.io/">LLM</a>. Il y avait notamment un probl√®me avec la gestion de la cl√© API (ou du "User Access Token") de Hugging Face. Stock√© dans les variables d'environnement tout fonctionne bien. Mais les plugins de LLM proposent √©galement un autre moyen de stocker la cl√©, notamment quand l'utilisateur tape <code>llm keys set nom_du_plugin</code>. Dans ce cas la cl√© est stock√©e dans un fichier JSON. Ce probl√®me est maintenant r√©gl√© et tout semble bien fonctionner.</p>
<p>Je me familiarise avec la gestion d'un d√©p√¥t Github. J'apprends et mets en place les choses petit √† petit. Par exemple : modification des infos sur le fichier <code>README.md</code> avec VSCode ou suppression des fichiers ajout√©s √† <code>.gitignore</code> avec :</p>
<p><pre><code>git rm -r --cached .
git add .
git commit -m "Remove ignored files and update .gitignore"
git push</code></pre></p>
<p><b>02-11-2025</b></p>
<p>Simon Willison recommande <a href="https://blog.sshh.io/p/how-i-use-every-claude-code-feature">cet article</a> sur les multiples fonctionnalit√©s de Claude Code. Le truc se veut assez complet, donc √† garder sous le coude comme r√©f√©rence. Pratique.<br>------<br>Le MCP (Model Context Protocol) n'est peut-√™tre pas (encore) la panac√©e. Dans <a href="https://www.youtube.com/watch?v=Cw2txKfAqK4">cette vid√©o</a>, l'intervenant souligne que bien que le MCP vise √† permettre √† l‚ÄôIA d‚Äôex√©cuter des t√¢ches complexes, son efficacit√© r√©elle reste limit√©e par les contraintes fondamentales des LLM, notamment la taille de leur fen√™tre contextuelle. √Ä mesure que les informations fournies au LLM augmentent (lecture de documents, traitement de donn√©es, ex√©cution de r√©sultats d‚Äôoutils), la fen√™tre contextuelle du mod√®le se remplit rapidement, ce qui entra√Æne une d√©gradation des performances et un risque accru d‚Äôerreurs ou d'hallucinations. L'intervenant conclut en soulignant l‚Äôimportance √©mergente du "context engineering" comme solution potentielle pour am√©liorer les syst√®mes agentiques et aider le MCP √† tenir ses promesses.<br>------<br>J'essaie de r√©parer mon appli "proofreader" (version Groq) avec Gemini-CLI mais il ne s'en sort pas, alors je change de strat√©gie. Je lui demande de conserver toutes les fonctionnalit√©s mais de tout "porter" en Python. Il s'ex√©cute, et apr√®s quelques aller-retour, √ßa finit par fonctionner √† peu pr√®s. En tous cas c'est beaucoup plus rapide qu'avec la version en React / Typescript. Les changements sont bien visibles sur le fichier Word. Projet √† poursuivre et √† am√©liorer. Je fais un push vers Github avec VSCode.<br>------<br>Je retourne au projet consistant √† cr√©er un plugin "Hugging Face" pour l'outil <a href="https://llm.datasette.io/">LLM</a> de Willison. J'avais commenc√© une √©bauche avec Gemini-CLI mais je crois que je lui avais donn√© trop d'informations. J'avais en effet r√©cup√©r√© plusieurs "vrais" plugins sur Github et les avais inclus dans le projet comme exemples. Je lui avais aussi donn√© toute la doc de "LLM" sous la forme d'un gros PDF de 140 pages. Bonjour la consommation de tokens. Je r√©duis donc drastiquement la voilure et vire quasiment tout pour ne garder essentiellement que 3 fichiers :<br>1. le contenu de <a href="https://llm.datasette.io/en/stable/plugins/tutorial-model-plugin.html">Developing a model plugin</a> de la doc de LLM ;<br>2. le contenu de <a href="https://huggingface.co/docs/inference-providers/index">Inference Providers</a> de Hugging Face (que j'ai sauvegard√© en markdown directement depuis la version en ligne avec le bouton "copy page" en haut √† droit, super pratique) ;<br>3. le fichier GEMINI.md / CLAUDE.md</p>
<p>Je modifie GEMINI.md, dans lequel je r√©explique clairement l'objectif du projet. Puis je le renomme en CLAUDE.md. J'installe Claude Code et le lance. Session incroyable pendant laquelle je sens les "super-pouvoirs" dont parlait NetworkChuck ! ü§© Un peu fascin√©, je regarde CC bosser et le guide quand il le faut. Au bout de 30 minutes j'ai un plugin fonctionnel. Je le teste et constate que la liste des mod√®les renvoy√©e n'est pas la bonne (trop courte et ne proposant que des "vieux" mod√®les). Je retrouve une fonction sens√©e r√©cup√©rer la "bonne" liste de mod√®les HF (il y en a normalement 118) et la donne √† CC, qui repart au quart de tour, int√®gre ce nouveau code, effectue des tests et met tout le projet √† jour. Vingt minutes suppl√©mentaires et c'est boucl√©. J'obtiens un plugin install√© et test√©, avec toute la doc mise √† jour, pratiquement pr√™t √† √™tre d√©ploy√© sur Github. Un truc de ouf. Et cette fois √ßa ne me co√ªte que 43 cents ($0.4334).</p>
<p><b>01-11-2025</b></p>
<p>Ce matin je visionne <a href="https://www.youtube.com/watch?v=MsQACpcuTkU">cette vid√©o</a>. Le type est vraiment le youtubeur professionnel typique, √† la fois insupportable et captivant. Sa vid√©o au montage affut√© comme un rasoir est un feu d'artifice des possibilit√©s offertes par les nouveaux outils "CLI" comme Claude Code ou Gemini CLI. Mais on apprend des choses. Il montre par exemple comment cr√©er des agents avec CC et qu'il est possible de faire bosser plusieurs IA sur un m√™me projet, en m√™me temps. Seul souci avec ce genre de d√©monstration : on ne sait pas de quel abonnement dispose ce youtubeur, qui est forc√©ment un peu un influenceur. Il b√©n√©ficie peut-√™tre de cr√©dits gratuits de la part d'OpenAI ou d'Anthropic, vu la publicit√© qu'il leur fait.</p>
<p>Ai mis en ligne une copie de ce blog sur <a href="https://sebington.github.io/">https://sebington.github.io/</a> avec l'outil Source Control de VSCode. L'ennui avec Github, c'est qu'√† chaque fois il faut attendre un peu pour que les changements soient pris en compte, alors qu'en FTP classique, c'est instantan√©. Pour l'instant <a href="http://bytepacking.free.fr/">http://bytepacking.free.fr/</a> reste le plus fr√©quemment mis √† jour. J'ai vu qu'il est normalement possible de s√©curiser le site, mais la proc√©dure est assez compliqu√©e. Si free pouvait se bouger un peu sur ce sujet ce serait cool.</p>
<p>Je continue de travailler avec Gemini-CLI et am√©liore ma compr√©hension de l'outil. Ai retravaill√© mon projet d'analyse de cha√Ænes Youtube (renomm√© yt-trend). Les deux scripts ont √©t√© am√©lior√©s : "retrieve.py" est beaucoup plus rapide et "analyze.py" peut maintenant se connecter √† une liste impressionante de mod√®les par le biais de 5 fournisseurs (Mistral, Groq, OpenRouter, Hugging Face et Claude). Pour OpenRouter, seuls les mod√®les gratuits sont affich√©s. J'essaie depuis peu de "pousser" ce qui fonctionne sur Github (en d√©p√¥t priv√©) pour m'y retrouver. J'adore Gemini-CLI, qui est vraiment super fluide. Je le pr√©f√®re √† Claude Code pour l'instant.</p>
<p><b>31-10-2025</b></p>
<p>Je suis un √¢ne. Ce matin, la t√™te (relativement) repos√©e, je lis sur mon t√©l√©phone <a href="https://simonwillison.net/2025/Oct/16/claude-skills/">cet excellent billet</a> de Simon Willison sur les "Claude Skills", dont j'avais report√© la lecture pour je ne sais quelle raison, et me rends compte qu'il r√©pond pr√©cis√©ment √† tous mes questionnements d'hier apr√®s-midi ! Il dit notamment :</p>
<p>"Something else I love about the design of skills is there is nothing at all preventing them from being used with other models. You can grab a skills folder right now, point Codex CLI or Gemini CLI at it and say ‚Äúread pdf/SKILL.md and then create me a PDF describing this project‚Äù and it will work, despite those tools and models having no baked in knowledge of the skills system.",</p>
<p>ce qui me rassure, puisque c'est ce que je fais avec Gemini CLI depuis la sortie des Skills.<br>------<br>Nouvelle fonctionalit√© sur le script de g√©n√©ration de cette page : la possibilit√© de cr√©er un hyperlien en utilisant la syntaxe markdown, c'est √† dire [lien]+(URL) (sans le signe +). La nouvelle fonction cr√©√©e par Claude fait une fois de plus appel √† un regex, qui rep√®re la structure "crochets + parenth√®ses" et la transforme en lien hypertexte. L'utilisation de regex assure entre autre une g√©n√©ration ultra-rapide de la page.<br>------<br>Tentative de r√©vision d'un texte en fran√ßais bourr√© de maladresses et de fautes avec <a href="https://www.claude.com/product/claude-code">Claude Code</a>. Je commence avec Sonnet, puis passe √† Haiku pour √©conomiser mes cr√©dits. Dans un dossier je pr√©pare un fichier README.md avec toutes les infos utiles pour CC (t√¢ches, outils, environment, etc.). Je mets aussi le contenu du dossier 'docx/' des Claude Skills avec des instructions d'usage. Le r√©sultat est d√©cevant. Le texte n'a pas assez √©t√© retravaill√© et de grossi√®res erreurs persistent. Mon appli sur Google AI Studio fonctionne beaucoup mieux.</p>
<p><pre><code>Total cost:            $0.4781
Total duration (API):  3m 46s
Total duration (wall): 10m 14s
Total code changes:    360 lines added, 17 lines removed
Usage by model:
claude-haiku:  852 input, 8.9k output, 2.1m cache read, 62.0k cache write ($0.3280)
claude-sonnet:  818 input, 3.1k output, 120.8k cache read, 17.2k cache write ($0.1500)</code></pre></p>
<p>------<br>M√™me tentative avec <a href="https://geminicli.com/">Gemini-CLI</a> qui d√©cide de me l√¢cher juste apr√®s avoir analys√© le projet. Message : <code>API Error: code: 429, Resource exhausted. Please try again later.</code><br>------<br>Du coup je modifie mon appli dans Google AI Studio. L'utilisateur peut maintenant choisir entre 3 t√¢ches linguistiques : "revision", "copy-editing" et proofreading". Le r√©sultat n'est pas mal, Gemini propose 41 am√©liorations qui se justifient. En revanche il laisse une belle faute de grammaire : "...la ville a v√©cue une p√©riode sombre..." et des double ou triple espaces blancs. A faire : modifier cette nouvelle version de l'appli pour qu'elle fonctionne en local et qu'elle se connecte √† un LLM puissant et rapide (Groq?).<br>------<br>NB : Ces efforts ont pour but de manipuler des textes en conservant leur format d'origine, l'ind√©boulonnable MS Word. Concernant les aspects purement linguistiques, un petit passage par ChatGPT avec le texte brut (.txt) donne de bien meilleurs r√©sultats que tous les essais qui pr√©c√®dent. C'est bien ce qui est rageant dans le m√©tier de traducteur : l'encapsulation du texte nuit √† son traitement.<br>------<br>Je soumets le m√™me texte (.docx) aux versions web gratuites de ChatGPT et de Claude pour correction-r√©vision. Tous deux r√©cup√®rent et corrigent le texte, mais aucun me livre un document Word : "lien mort" pour ChatGPT et "impossibilit√© technique" chez Claude. Olivier soumet le m√™me texte √† la version "Pro" de Claude et finit par obtenir un document Word r√©vis√©, mais fragile techniquement et linguistiquement, comme si l'un se faisait au d√©triment de l'autre.</p>
<p>Concernant les deux documents r√©vis√©s issus des version gratuites de ChatGPT et de Claude, on peut les comparer √† posteriori √† l'original et pour chacun g√©n√©rer une version du document o√π l'on voit les modifications effectu√©es. A premi√®re vue ChatGPT semble avoir fait le meilleur travail, mais Claude n'est pas tr√®s loin derri√®re.</p>
<p>Statistiques<br>Claude : 209 r√©visions (86 insertions, 89 suppressions, 34 mises en forme)<br>ChatGPT : 308 r√©visions (131 insertions, 127 suppressions, 50 mises en forme)</p>
<p><b>30-10-2025</b></p>
<p>Je me rends compte que je confondais node.js, nvm et npm. üòè ChatGPT me fait un petit r√©sum√© explicatif :</p>
<p><pre><code>- Node.js is the core runtime that runs JavaScript on your system.
- NPM is automatically installed with Node.js and manages the dependencies your project needs.
- NVM sits above both ‚Äî it manages which version of Node.js (and its bundled npm) you are using.</code></pre></p>
<p>Site de r√©f√©rence : <a href="https://nodejs.org/en/download">https://nodejs.org/en/download</a>.<br>------<br>Surprises, paradoxes et d√©ceptions :</p>
<p>1. Je fais une session compl√®te d'environ 15 min avec <a href="https://opencode.ai/">opencode</a> et un mod√®le gratuit d'OpenRouter (MiniMax M2). Le r√©sultat n'est pas foufou. A noter : opencode semble √™tre en mode YOLO par d√©faut. Il prend toutes les d√©cisions d'ex√©cuter le code tout seul sans me demander mon avis !üòÆ</p>
<p>2. J'encha√Æne une session similaire avec Gemini-CLI et √† peine a-t-il commenc√© qu'il cale :</p>
<p><pre><code>‚ö° Automatically switching from gemini-2.5-pro to gemini-2.5-flash for faster responses for the remainder of this session.
‚ö° Possible reasons for this are that you have received multiple consecutive capacity errors or you have reached your daily gemini-2.5-pro quota limit
‚ö° To increase your limits, upgrade to a Gemini Code Assist Standard or Enterprise plan with higher limits at https://goo.gle/set-up-gemini-code-assist
‚ö° Or you can utilize a Gemini API Key. See: https://goo.gle/gemini-cli-docs-auth#gemini-api-key
‚ö° You can switch authentication methods by typing /auth
‚úï [API Error: Please submit a new query to continue with the Flash model.]</code></pre></p>
<p>alors que je ne l'ai pas du tout utilis√© aujourd'hui. O√π sont mes tokens gratuits ?? ü§î</p>
<p>3. J'ach√®te 5 dollars (factur√©s 6) de cr√©dits sur Claude Console pour pouvoir utiliser Claude Code (CC). Apr√®s avoir saisi mon num√©ro de carte bleue et mon adresse et tandis que je r√©fl√©chis encore √† la somme que je veux mettre, le processus en ligne "d√©cide" soudain de valider tout seul la transaction alors que je n'ai cliqu√© sur aucun bouton "Valider", WTF!! M√©thode pour le moins "cavali√®re" de la part d'Anthropic je trouve.</p>
<p>Je lance ensuite ma premi√®re session avec CC. Elle se passe pas trop mal mais le r√©sultat n'est pas au rendez-vous et je constate qu'elle m'a co√ªt√© $1.33 ! Je v√©rifie le mod√®le par d√©faut : il s'agit bien de Sonnet 4.5 comme je m'y attendais. Je suis n√©anmoins surpris apr√®s avoir entendu maintes et maintes fois Simon Willison d√©voiler les sommes ridicules d√©pens√©es sur la g√©n√©ration de ses p√©licans √† bicyclette. Cela confirme ce dont personne ne semble parler dans les nombreuses vid√©os sur YT concernant ces outils : ils sont un moyen de nous faire consommer du token √† la pelle. Le g√©n√©reux free-tiers de Gemini-CLI n'est pas si mal apr√®s tout.<br>------<br>R√©flexion sur Claude Code et Gemini CLI (r√©sum√© d'un √©change entre Olivier et moi)</p>
<p>Info importante concernant l'abonnement "PRO" de Claude : il ne permet pas d'utiliser l'API de Claude, contrairement l'abonnement "MAX". On ne peut donc pas lancer de scripts autonomes, mais on peut toujours faire appel √† d'autres LLM pour cela.</p>
<p>Concernant les "Skills" pas mal de choses sont encore brumeuses dans mon esprit. Je me demande si CC y a acc√®s par d√©faut ou s'il faut les lui donner en usage local, comme je le fais avec Gemini-CLI. Officiellement, pour b√©n√©ficier des "Skills", il faut les installer via un plugin dispo sur "anthropics/skills marketplace". Ensuite Claude les charge automatiquement quand il en a besoin. On peut aussi les installer "manuellement" en les ajoutant √† ~/.claude/skills (pas tr√®s clair). Donc les lui donner en local reste possible, √† condition de les avoir sous la main ou de savoir o√π les trouver.</p>
<p>Ceci dit les "Skills" permettent de faire toutes sortes de manipulations de fichiers (Word ou PDF par ex.) qui √©taient impensables il y a 6 mois et qui sont super utiles dans mon boulot. Je me demande si l'utilisation que j'en ai faite dans Gemini-CLI est analogue √† leur utilisation "native" dans Claude Web ou CC. En tous cas quand je regarde le d√©tail des actions de Gemini-CLI lors d'une session, manifestement il s'en sert.</p>
<p>Olivier donne une pr√©cision importante : ce qui caract√©rise les Skills c'est la gestion du contexte : Claude lit le d√©but de chaque fichier SKILL.md  (champs "name" et "description") et ne lira la suite que si cela est n√©cessaire pour la t√¢che en cours. Cela permet d'√©conomiser des tokens. Tandis qu'il est probable que Gemini-CLI int√®gre 100 % des infos contenues dans les Claude Skills dans son contexte, ce qui consomme davantage de tokens.</p>
<p>Je viens de faire une nouvelle session avec CC et j'ai maintenant d√©pens√© 2 dollars sur 5 ! En gros on est √† un dollar par session. Je vais essayer d'utiliser ce qui reste sur des t√¢ches plus √©conomes en API, en lan√ßant des scripts autonomes par exemple.</p>
<p><b>29-10-2025</b></p>
<p>"Rainy day, dream away, let the sun take a holiday..." (Jimi Hendrix)<br>------<br>Ce matin je continue de tester Gemini-CLI et lui demande non pas de "construire un outil qui va faire", mais de "faire lui m√™me" (je ne sais pas si √ßa a du sens), en l'occurence r√©viser un texte au format Word (.docx) et produire une version r√©vis√©e du document en mode "suivi des modifications". Je lui donne acc√®s aux outils "Claude Skills". J'initialise le dossier avec uv pour qu'il puisse ex√©cuter du Python. Et apr√®s l'avoir fait un <code>/init</code>, lui donne le prompt suivant :</p>
<p><span class="code-box">Do a professional copy editing of @doc_en.docx. You can decide to use the tools provided in @tools/ or not. Output the result in a .docx file in tracked changes. The project has been uv initialized, you can use 'uv add ...' or 'uv run ...' if you need to execute Python. Reminder: Copy editing is a detailed linguistic and stylistic review focusing on grammar, syntax, punctuation, spelling, and consistency of tone, terminology, and formatting. It ensures clarity, precision, and adherence to stylistic or editorial standards without altering the substance of the content.</span></p>
<p>Les r√©sultats ne sont pas tr√®s concluants. Gemini veut se servir de pandoc mais il bute sur l'installation malgr√© mon guidage. J'aurais d√ª lire le fichier GEMINI.md apr√®s sa cr√©ation et installer pandoc au pr√©alable. Il parvient tout de m√™me √† travailler sur le document mais soudain s'emballe quand il se lance dans une r√©vision sans fin du xml interne au doc. Je l'arr√™te avant qu'il ne grille tous ses tokens et finis par le faire travailler sur le m√™me document en TXT, ce qu'il fait, mais le r√©sultat est d√©cevant. Mon appli cr√©√©e avec Google AI Studio fonctionne beaucoup mieux, sans √™tre extraordinaire. A suivre.<br>------<br>Cet apr√®s-midi mon coll√®gue JB me demande un service. Il voudrait pouvoir identifier la langue d'une collection de cha√Ænes de caract√®res dans une base de donn√©es. Il voudrait utiliser l'API de DeepL, lui envoyer la cha√Æne de caract√®res et r√©cup√©rer le code RFC 5646 de la langue ('en', 'fr', etc.). Apr√®s quelques tests, je me souviens que DeepL ne propose pas le breton dans sa biblioth√®que de langues. Or JB a absolument besoin de pouvoir identifier du breton.</p>
<p>Je propose alors d'utiliser un LLM au lieu de DeepL. La demande de JB est un cas parfait de "Structured Outputs". On fait une requ√™te bien pr√©cise au LLM et on filtre sa r√©ponse pour ne r√©cup√©rer que l'info dont on a besoin. Je consulte la doc de Groq et construis l'outil en quelques requ√™tes √† Claude. √áa marche tout de suite. Le script boucle sur les donn√©es d'un fichier .tsv, envoie une cha√Æne de caract√®res √† <code>llama-3.3-70b-versatile</code> sur l'API de Groq avec un prompt bien pr√©cis, r√©cup√®re la langue et son code et stocke le tout dans un fichier JSON.</p>
<p><pre><code>  {
    "row": 3,
    "text": "[The Eisteddfod. A Great reunion]",
    "language": "English",
    "code": "en"
  },
  {
    "row": 4,
    "text": "[Lettre de Paul Diverr√®s √† Yves Berthou]",
    "language": "French",
    "code": "fr"
  }, etc.</code></pre></p>
<p>Apr√®s quelques essais nous changeons de mod√®le pour <code>moonshotai/kimi-k2-instruct-0905</code>, qui semble donner de meilleurs r√©sultats. JB me demande aussi de simplifier et r√©duire le script √† sa seule fonction : cha√Æne de caract√®res en entr√©e et code RFC 5646 en sortie, ce que Claude fait en quelques secondes. Je suis tr√®s fatigu√© et quasiment dans un √©tat second, mais tr√®s content d'avoir pu travailler sur ce probl√®me et d'avoir trouv√© une solution viable. Je ne sais pas si JB, un peu r√©ticent aux IA, va finir par utiliser cette solution mais au moins j'aurais mis en oeuvre mes maigres comp√©tences sur un "vrai projet", et cela m'emplit de joie.üòÑ<br>Script : <a href="https://gist.github.com/sebington/382e2c6ebd43a28453c3c2541d4b241f">https://gist.github.com/sebington/382e2c6ebd43a28453c3c2541d4b241f</a><br>------<br>Je continue d'utiliser Linux Manjaro et parviens, √† grand renfort de Claude, √† localiser Freetube et √† l'installer avec yay (<code>yay -S freetube</code>). Je constate toutefois que la recherche de "freetube" avec pacman (<code>pacman -Ss freetube</code>) ne renvoit rien. Claude me propose plusieurs solutions, par exemple de faire (<code>pacman -Ss | grep -i freetube</code>) ou m√™me d'installer fzf (fuzzy search) pour le coupler √† pacman, mais √ßa ne donne rien. C'est finalement la recherche directement avec yay (<code>yay freetube</code>), qui me permet de localiser freetube. Bref je me suis probablement trop focalis√© sur pacman alors que yay semble √™tre plus versatile et intuitif.<br>------<br>D√©couverte (re-d√©couverte en fait) d'un merveilleux site qui propose toutes sorte de g√©n√©rateurs de bruits reproduisant au besoin toutes les ambiances souhait√©es : <a href="https://mynoise.net/">https://mynoise.net/</a>. Excellent.</p>
<p><b>28-10-2025</b></p>
<p>J'ai ajout√© un formattage sp√©cial pour les prompts dans le code de g√©n√©ration de cette page (rectangle aux bords arrondis). Je teste diff√©rentes polices et couleurs.<br>------<br>J'essaie de reproduire et de lancer un script en Python qui construit et teste un r√©seau de neurones (NN) tr√®s simple comportant une seule couche de 32 neurones. Ce NN g√©n√®re des donn√©es al√©atoires avec du bruit, puis analyse ces donn√©es pour retrouver la fonction "cach√©e" derri√®re. Source : <a href="https://www.youtube.com/watch?v=rdFjMb6dTJQ">https://www.youtube.com/watch?v=rdFjMb6dTJQ</a>. Je pars de la vid√©o et fais des captures d'√©cran du code, que je colle dans Claude et dans Mistral avec le prompt : <code>OCR this image</code>. Je r√©cup√®re peu √† peu tout le code que je colle dans un fichier.</p>
<p>Je demande √† Ruff de v√©rifier le script reconstitu√© (<code>uvx ruff check</code>). Ruff d√©tecte un petit probl√®me (E702 Multiple statements on one line (semicolon)), mais cela ne devrait pas emp√™cher le script de fonctionner. J'ajoute un "uv inline script" en haut avec les biblioth√®ques pertinentes et lance le truc. UV met environ 15 secondes √† t√©l√©charger et √† installer les paquets. √âtonnamment, Torch ne fait pas 1 Go comme d'habitude, mais plut√¥t autour de 100 Mo. Je me demande si uv d√©tecte le fait que je n'ai pas de GPU et t√©l√©charge une version "light" (cpu-only) de Torch. Pure conjecture de ma part.</p>
<p>Ensuite tout va tr√®s vite (quelques secondes). Le script se lance et le NN s'entra√Æne sur 500 it√©rations ("epochs"). La sortie est exactement comme ce qui est montr√© dans la vid√©o. Le NN pr√©dit la courbe de x au carr√© et on voit les donn√©es d'entra√Ænement en bleu et de test en marron. J'adore ce genre de petite exp√©rience o√π l'on peut tester et mieux comprendre un des principes fondamentaux des r√©seaux de neurones et de l'intelligence artificielle. Script : <a href="https://gist.github.com/sebington/0ec87907ad0a079e6f59d941336d547c">https://gist.github.com/sebington/0ec87907ad0a079e6f59d941336d547c</a>.<br>------<br>Ce soir, fatigu√©, je farfouille dans mes fichiers √† la recherche du plus vieux morceau de code python que j'aie tap√© sur un clavier. Je retrouve un script datant du 14 octobre 2020 o√π j'ai recopi√© le code de la vid√©o suivante : <a href="https://www.youtube.com/watch?v=InUqeaOSPpA">https://www.youtube.com/watch?v=InUqeaOSPpA</a> (Pytorch Torchtext Tutorial 2: Built in Datasets with Example). Qu'est-ce qui m'a fait choisir cette vid√©o ? Pourquoi ne pas avoir commenc√© par le "Tutorial 1" ? Je crois que je trouvais ce gars qui tapait du code tout en semblant savoir ce qu'il faisait hyper cool. D√®s le d√©but je cherchais des choses en lien avec le langage et les mots : j'allais bient√¥t d√©couvrir le NLP (Natural Language Processing) et toutes ses applications. Les LLM d'aujourd'hui viennent de l√†, du langage. ¬´ Au commencement √©tait le Verbe, et le Verbe √©tait avec Dieu, et le Verbe √©tait Dieu. ¬ª ‚Äî Premier verset de l'√âvangile selon Jean.</p>
<p><b>27-10-2025</b></p>
<p>Je poursuis ma traduction "juridico-administrative", que je dois livrer demain. J'avais entendu dire que ChatGPT √©tait d'une grande aide avec ce genre de texte. Je confirme ! Il connait le droit international dans toutes ses nuances comme sa poche. C'est incroyable l'aide qu'il apporte.</p>
<p>Vu sur HackerNews, un site int√©ressant pour qui aime les cartes et les donn√©es GPS : <a href="https://geoutil.com/">https://geoutil.com/</a>.</p>
<p>Concernant mon probl√®me d'URL matching avec regex, j'interroge ChatGPT avec le prompt suivant :</p>
<p><span class="code-box">Create a regex that will only match a URL, even if: 1. the URL is enclosed in brackets; 2. the URL is followed by a full stop; 3. the URL finishes with a slash "/"; 4. any combination of the above</span></p>
<p>ChatGPT me sort un regex assez long qui ne fonctionne pas mieux que le reste. Bon il faut avouer que mon prompt est un peu tordu dans sa formulation.</p>
<p>Olivier sugg√®re <a href="https://pregex.readthedocs.io/en/latest/introduction.html">pregrex</a>, mais l'approche est assez ardue et le projet n'a pas √©t√© mis √† jour depuis le 8 octobre 2022 (soit un mois avant la sortie de ChatGPT). J'essaie le regex qui est propos√© dans la partie "Usage Example" de leur pr√©sentation, qui est sens√© matcher une URL, mais c'est le m√™me topo.</p>
<p>Je me rends sur <a href="https://lmarena.ai/?mode=direct">https://lmarena.ai/?mode=direct</a> et choisis <code>claude-opus-4-1-20250805-thinking-16k</code> avec un nouveau prompt :</p>
<p><span class="code-box">Write a regex that will match any URL in a text, whether it is enclosed in brackets, finishes with a slash or a digit or is followed by a full stop.</span></p>
<p>J'obtiens 2 regex vraiment longs, sortes d'usines √† gaz qui fonctionnent, mais butent sur les m√™mes √©cueils (les points, les virgules).</p>
<p>Dernier essai sur le m√™me site avec <code>gpt-5-high</code> (m√™me prompt).</p>
<p>J'obtiens 2 regex un peu moins longs.</p>
<p>Je teste le premier et l√† bingo, √ßa marche ! Lanc√© dans Sublime Text, le truc matche toutes les URL de mon carnet de notes sans exceptions. Je crie victoire mais d√©chante aussi vite quand j'essaie d'int√©grer le regex dans le code du script en Python qui g√©n√®re cette page. Pour l'instant √ßa ne marche pas. Je me rends compte que le fait m√™me d'afficher les diff√©rents regex essay√©s dans mes notes en TXT interf√®re avec la g√©n√©ration de la page (qui fait beaucoup appel √† regex). √áa cr√©e une sorte contamination. J'aurais d√ª y penser.</p>
<p><b>26-10-2025</b></p>
<p>Ce matin, je fais 42 km de VTT dans le vent et les embruns du c√¥t√© de Ploudalm√©zeau. Salutaire pour me reposer les yeux et le cerveau.</p>
<p>Je travaille sur une traduction tout le reste du temps. Je r√©utilise mon outil d'alignement cr√©√© l'autre jour et me rends compte qu'il n'est pas assez convivial. Je demande √† Claude de le modifier en laissant l'utilisateur choisir les langues et les fichiers et en le rendant un peu plus attractif visuellement avec <code>rich</code>. Le r√©sultat est ici : <a href="https://github.com/sebington/txt-to-tmx-aligner">https://github.com/sebington/txt-to-tmx-aligner</a>.</p>
<p>Je continue de perfectionner le script en Python qui g√©n√®re cette page HTML √† partir de mes notes au format TXT. J'ajoute une fonction pour mettre la page en ligne en FTP. Je me rends aussi compte que certains liens g√©n√©r√©s par le script ne fonctionnent pas s'ils sont entre parenth√®ses ou suivis d'un point, ou s'ils finissent par "/".</p>
<p>La fonction de conversion d'URL en liens hypertextes fait appel √† une expression r√©guli√®re :</p>
<p><pre><code>def linkify_urls(text):
    url_pattern = r'(https?://[^\s<>]+)'
    return re.sub(url_pattern, r'<a href="\1">\1</a>', text)</code></pre></p>
<p>Claude en a produit une version plus sophistiqu√©e mais pas encore parfaite :</p>
<p><pre><code>def linkify_urls(text):
    url_pattern = r'(https?://[^\s<>]+?)(?=[.,;:!?)"\']?(?:\s|$|<))'
    return re.sub(url_pattern, r'<a href="\1">\1</a>', text)</code></pre></p>
<p>Certains liens entre parenth√®ses fonctionnent tandis que d'autres non. Il va falloir investiguer.</p>
<p>A noter que mon script reg√©n√®re la page compl√®te √† chaque fois en une fraction de seconde. Cela me permet de corriger ou d'am√©liorer mes notes et de disposer d'un rendu instantan√© de l'ensemble.</p>
<p><b>25-10-2025</b></p>
<p>Je r√©pare √† nouveau mon Google AI Studio Proofreader. Je l'avais d√©j√† fait hier et pensais qu'autosave √©tait activ√© par d√©faut comme dans le reste des applis web de Google. J'√©tais dans l'erreur : il y a un bouton pour sauvegarder les changements apport√©s √† l'appli, ce qui semble logique : pourquoi sauvegarder une appli si elle bugue ?</p>
<p>Je t√©l√©charge l'appli (fichier zipp√©) et fais bosser Gemini-CLI dessus. Je voudrais :<br>1. que l'appli fonctionne en local sur mon ordi<br>2. qu'elle se connecte √† un autre mod√®le que Gemini via une API de mon choix (Groq par exemple)</p>
<p>Apr√®s quelques moments de d√©bogage √ßa marche. Il faut que node (npm) soit install√©. Je peux maintenant lancer l'appli en faisant <code>npm run dev</code>, ce qui d√©marre un serveur local accessible via <a href="http://localhost:3000/">http://localhost:3000/</a>. L'appli se lance dans Firefox et fonctionne correctement avec un mod√®le de Groq. Le mod√®le utilis√© est hardcod√© dans un fichier nomm√© <code>groqService.ts</code>. Il s'agit de <code>llama-3.3-70b-versatile</code>. üòÑ</p>
<p><b>24-10-2025</b></p>
<p>Je demande √† Gemini de r√©parer mon appli sur Google AI Studio dont l'interface graphique ne s'affiche pas et fais un essai avec un fichier complet. Mon "proofreader" d√©tecte 130 changements, que je dois passer en revue un par un avant de l'accepter ou de le refuser. L'outil d√©tecte un certain nombre de faux positifs quand il s'agit des majuscules ou minuscules. Je le modifie pour am√©liorer ce point et demande l'ajout d'un bouton pour accepter tous les changements d'un coup. √áa fonctionne. Le fichier en sortie est bien en mode "suivi des modifications" avec tous les changements clairement visibles.</p>
<p>D√©couverte du projet marimo : <a href="https://github.com/marimo-team/marimo">https://github.com/marimo-team/marimo</a>. C'est une sorte de jupyter notebooks on steroids super prometteur. Par exemple, la sortie d'un bloc de code se met √† jour en temps r√©el si on modifie le code. Et c'est du pur Python !</p>
<p>Premi√®re tentative de "git push" en ligne de commande (avec une image)</p>
<p><pre><code>$ git clone https://github.com/repo_name/repo_name.github.io.git
$ cd repo_name.github.io/
$ git add images/nn.png
$ git commit -m "Add image to images folder"
$ git push origin main [provide Username and Password]</code></pre></p>
<p>Pour renommer un fichier</p>
<p><pre><code>$ git mv index.html index_old.html
$ git commit -m "Rename index.html"
$ git push origin main</code></pre></p>
<p>Pour ne pas avoir √† s'authentifier √† chaque commit</p>
<p><pre><code>$ git config --global credential.helper cache
# or store permanently (less secure)
$ git config --global credential.helper store</code></pre></p>
<p><b>23-10-2025</b></p>
<p>Simon Willison, probablement √† la demande d'Anthropic, a supprim√© de son Github les fichiers "Claude Skills" qu'il avait d√©couverts et t√©l√©charg√©s avant l'annonce officielle d'Anthropic concernant ce nouveau service de Claude : <a href="https://simonwillison.net/2025/Oct/10/claude-skills/">https://simonwillison.net/2025/Oct/10/claude-skills/</a>.<br>------<br>Nouvelle session Gemini CLI. Je lui demande d'utiliser les "Claude Skills" pour r√©viser un document Word et le livrer en mode suivi des modifications. J'oublie toujours d'activer le projet avec uv avant de commencer !</p>
<p>Prompt:<br><span class="code-box">Do a professional revision (grammar, spelling, punctuation etc.) of @test.docx and output a .docx file in tracking mode where all the changes made are visible.</span></p>
<p>En parall√®le je demande √† Claude Sonnet 4.5 de faire la m√™me chose dans l'interface web "classique".</p>
<p>Gemini bosse comme un dingue et grille son quota de tokens avant d'avoir termin√©.</p>
<p>Claude cr√©e une interface web inutile et ne parvient pas √† sortir un document Word (erreur). Je le relance et il finit par me donner le r√©sultat sous forme d'un fichier TXT avec les corrections (surtout l'orthographe) demand√©es.</p>
<p>Nouvelle tentative, dans Google AI Studio cette fois. Je cr√©e une appli qui fonctionne du premier coup, prend un .docx, le r√©vise, me montre les changements apport√©s et produit un document Word en mode suivi des modification. Le d√©lire. Mais est-ce que √ßa marcherait avec un gros fichier ? Je retourne sur l'appli 3h plus tard et bizarrement le code est toujours l√† mais l'interface graphique ne se charge pas. Ecran noir.<br>------<br>Retour sur Manjaro Gnome. Je r√®gle tous les probl√®mes rencontr√©s hier et savoure cette nouvelle distro qui est vraiment sympa.<br>------<br>Je suis un tuto de Simon Willison sur YT [5TdIxxBPUSI]. Pas tr√®s progressif et un peu fouilli, mais instructif. Je cr√©e un dossier 'sqlite' et lance un jupyter notebook en faisant <code>uv run --with jupyter jupyter lab</code> o√π je tape les exemples de code. Tout fonctionne bien et je peux sauvegarder les sorties.<br>------<br>L'infatiguable Willison vient de mettre une nouvelle vid√©o YT en ligne [GQvMLLrFPVI] dans laquelle il utilise Claude Code Web pour se fabriquer un outil √† partir de 2 autres outils cr√©√©s pr√©c√©demment. On aper√ßoit au passage l'impressionnante collection d'outils dont il dispose et qui sont disponibles sur son github : <a href="https://tools.simonwillison.net">https://tools.simonwillison.net</a>.</p>
<p><b>22-10-2025</b></p>
<p>Avec Claude, je continue de modifier mon script de g√©n√©ration de HTML √† partir de mes notes au format TXT (voir 19/10). √áa commence √† devenir pas mal.</p>
<p>Je t√©l√©charge Linux Mint Cinnamon, l'installe sur une cl√© USB bootable avec Rufus et l'utilise pendant 30 minutes. Je trouve bien mes marques. Le terminal est imparam√©trable une fois de plus et j'installe le terminal √©mul√© "deepin" √† la place. Plut√¥t pas mal.</p>
<p><b>21-10-2025</b></p>
<p>J'installe Manjaro Gnome pour voir. Pas mal de positif (look, rapidit√©, efficacit√©) mais certaines choses bizarres comme par exemple le fait que je n'arrive pas √† lancer un outil install√© dans uv et que je ne peux pas compl√®tement d√©sinstaller zsh, qui semble √©troitement int√©gr√© √† l'OS (d√©pendances). Le terminal pr√©-install√© n'est pas param√©trable et j'en installe une autre version. En revanche, Nextcloud, Signal et OBS Studio fonctionnent √† merveille.</p>
<p>Retour dans Ubuntu</p>
<p>Commande pour n'afficher que les mod√®les gratuits d'openrouter dans LLM:</p>
<p><pre><code>llm models | grep ':free$'</code></pre></p>
<p>J'installe <a href="https://datasette.io/">https://datasette.io/</a> avec uv</p>
<p><pre><code>uv tool install datasette
llm models | grep ':free$'
llm -m openrouter/google/gemma-3-27b-it:free "What is Datasette?"
datasette "$(llm logs path)"</code></pre></p>
<p>cela donne l'acc√®s √† une interface web</p>
<p><pre><code>INFO: Started server process [13667]
INFO: Waiting for application startup.
INFO: Application startup complete.
INFO: Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C to quit)</code></pre></p>
<p>o√π on peut requ√™ter la base de donn√©es de tout l'historique de LLM (prompts, mod√®les, r√©ponses, etc.)</p>
<p><b>20-10-2025</b></p>
<p>Je visionne une vid√©o YT [C44iCr6czAo] sur les d√©rives s√©curitaires de Windows. √áa me donne envie de basculer sur Linux et d'y rester. Je t√©l√©charge 2 versions de Manjaro (KDE Plasma et Gnome) ainsi que CachyOS, que je teste tour √† tour. Moi qui m'√©tais promis de me reposer les yeux ce soir, c'est rat√©.</p>
<p><b>19-10-2025</b></p>
<p>Je demande √† Claude de me cr√©er un script en Python (logbook-converter) qui convertit ce carnet de notes (au format .txt) en fichier HTML avec les prompts et les extraits de code reconnus et formatt√©s. √Ä suivre.</p>
<p>Je n'en finis pas de peaufiner le projet YouTube metadata (1. retrieve, 2. analyze). Le fichier "source" est une liste de chaines YT au format CSV (export de FreeTube), le fichier interm√©diaire est un fichier JSON (channel_videos_2025.json) et la sortie finale peut se faire soit √† l'√©cran (v2), soit √©cran + markdown (v2_md). Je finis par faire une vid√©o.</p>
<p>Je continue d'essayer de construire un plugin Hugging Face pour Willison's <a href="https://llm.datasette.io/">LLM</a> avec Gemini-CLI. C'est de la folie pure de s'attaquer √† un truc pareil. Je vais n√©anmoins beaucoup plus loin qu'hier. Le plugin s'installe et r√©cup√®re une liste de mod√®les. Cependant impossible de lancer une requ√™te (<code>Client error '404 Not Found'</code>).</p>
<p><b>18-10-2025</b></p>
<p>J'active Copilot dans VScode et le teste timidement. Copilot n'a pas compris que j'utilise uv et tente d'installer des librairies Python avec pip sur Ubuntu. Je l'arr√™te et y reviendrai d√®s que possible.</p>
<p>Suite √† la d√©couverte du nouveau HuggingChat il y a 2 jours, je me dis qu'il est probable que HF essaie de proposer quelque chose comme OpenRouter : une seule API donne acc√®s √† de nombreux mod√®les. Je pose la question √† ChatGPT qui finit par me le confirmer apr√®s √™tre all√© lire la doc de HF "en direct".</p>
<p><span class="code-box">Yes, Hugging Face is now offering a multi-provider inference capability via its Inference Providers framework that approximates the ‚Äúone API, many models/providers‚Äù goal. The OpenAI-compatible API support is a particularly strong move, enabling you to reuse existing client libraries against HF‚Äôs routing system.</span></p>
<p>Convaincu par cette explication, je me dis qu'en th√©orie, je pourrais cr√©er un plugin pour Simon Willison's <a href="https://llm.datasette.io/">LLM</a> qui donnerait acc√®s aux mod√®les de HF. Mais je me dis que c'est peut-√™tre un peu trop ambitieux de commencer par √ßa. Essayons d√©j√† de voir si l'API de HF fonctionne comme pr√©vu.</p>
<p>Je me rends sur <a href="https://huggingface.co/docs/inference-providers/">https://huggingface.co/docs/inference-providers/</a> et finit par trouver un bout de code pour tester l'API. Il faut cr√©er une cl√© sp√©ciale (allowing inference providers). Le script fonctionne.</p>
<p>Je demande alors √† Gemini-CLI d'incorporer HF aux 3 autres providers de mon script 2_analyze_gemini_v1.py (projet YouTube).</p>
<p>Prompt :<br><span class="code-box">Use @test_hf.py to add Hugging Face as a new model provider in @2_analyze_gemini_v2.py.</span></p>
<p>Gemini-CLI le fait en 2 temps 3 mouvements. C'est totalement jouissif. Je lui demande la possibilit√© de pouvoir revenir √† la liste des "providers" si je ne choisis pas de mod√®le tout de suite. On peut ainsi comparer le nombre de mod√®les propos√©s par chaque provider : Mistral = 68, Groq = 19, OpenRouter = 340 et Hugging Face = 115.</p>
<p>Je fais une vid√©o sous Linux avec OBS Studio qui fonctionne enfin !</p>
<p>Je tente tout de m√™me de cr√©er le plugin HF avec Gemini-CLI, mais il ne veut m√™me pas s'installer dans LLM.</p>
<p><b>16-10-2025</b></p>
<p>Je tente d'utiliser nextcloud en ligne de commande avec WSL pour tester la vitesse de synchronisation des fichiers:</p>
<p><pre><code>mkdir nextcloud
sudo add-apt-repository ppa:nextcloud-devs/client
sudo apt update
sudo apt install nextcloud-client</code></pre></p>
<p>Mais c'est tr√®s gal√®re : au lieu de sp√©cifier quels dossiers on veut synchroniser, il faut exclure tous les dossiers que l'on ne veut PAS synchroniser</p>
<p><pre><code>--exclude "SEB" --exclude "TAO" etc.</code></pre></p>
<p>Du coup je d√©sinstalle tout.</p>
<p>Je g√©n√®re de nouveaux scripts dans mon projet YouTube metadata :<br>- yt_metadata_csv.py (titre et upload date √† partir de plusieurs chaines YT)<br>- retrieve_multiple_yt_video_titles.py (titre et upload date √† partir de plusieurs videos)</p>
<p>Ce second script exporte le r√©sultat du scraping de toutes mes URL de videos YT dans un fichier CSV (URL, Title, Upload date) que j'importe ensuite dans Google Drive. A noter que ce script n'utilise pas yt-dlp mais fait appel √† requests et beautifulsoup4 et semble √™tre plus rapide.</p>
<p>Le nouveau HuggingChat (<a href="https://huggingface.co/chat/">https://huggingface.co/chat/</a>) est une tuerie (116 mod√®les propos√©s).</p>
<p>Simon Willison reparle des "Claude Skills" sur son blog. Je suis certain que ce que j'ai fait avec Claude Skills et Gemini-CLI le 12/10 n'avait jamais √©t√© fait auparavant.</p>
<p><b>15-10-2025</b></p>
<p>L'alignement manuel des textes de ma coll√®gue est d√©cid√©ment impossible avec memoQ. Obs√©d√© par le c√¥t√© r√©calcitrant de la chose, je demande √† Claude de me cr√©er un outil d'alignement.</p>
<p><span class="code-box">I have two .txt files source (fr) and target (en) that I would like to align in order to obtain a translation memory (.tmx). Create a Python script that will scan the two .txt files line by line (this is important), retrieve all sentences and expressions pairs (source and target language) and store them methodically and chronologically in a dictionary, to be later exported as a .tmx file. Keep everything, even if source text is the same as target text but remove duplicate entries. Discard section numbering (e.g. 12.2.3.1) at the start of a line. Discard lines containing just one letter or only numbers. Keep all meaningful chains of characters. Segment at sentence level whenever possible. Attached are two extracts (source and target).</span></p>
<p>√áa marche ! One shot !üòÉ Le script g√©n√®re un fichier TMX valide que j'importe dans memoQ sans probl√®me. Dingue.</p>
<p><a href="https://repomix.com/">https://repomix.com/</a> is a powerful tool that packs your entire repository into a single, AI-friendly file.<br><a href="https://www.kimi.com/en/">https://www.kimi.com/en/</a> : sounds like another great LLM like Qwen and DeepSeek</p>
<p><b>14-10-2025</b></p>
<p>Je fais une vid√©o sur la tentative d'alignement de fichiers .txt avec memoQ et sur la nouvelle approche avec Gemini CLI du 12/10.</p>
<p>Il faut que je m'int√©resse aux "MCP server".<br>Une liste impressionnante : <a href="https://github.com/modelcontextprotocol/servers">https://github.com/modelcontextprotocol/servers</a><br>Essayer celui-ci peut-√™tre : <a href="https://github.com/vivekVells/mcp-pandoc">https://github.com/vivekVells/mcp-pandoc</a></p>
<p><b>13-10-2025</b></p>
<p>Ce soir, je me fais un petit shot de vid√©os d'Underscore. Par exemple [biZX5cnQ_UU] dans lequel je d√©couvre Gr√©goire Mialon et Cl√©mentine Fourrier de Hugging Face, ou [s5oHvfFUsbE] avec un certain Nicolas Dufour. Je visite le site de Cl√©mentine Fourrier dont j'adore le c√¥t√© minimaliste (<a href="https://clefourrier.github.io/">https://clefourrier.github.io/</a>) avec en bas, ce lien vers (<a href="https://jeffhuang.com/designed_to_last/">https://jeffhuang.com/designed_to_last/</a>) au concept super int√©ressant. Je check le projet de Nicolas Dufour, un outil tr√®s original de g√©olocalisation d'images : <a href="https://nicolas-dufour.github.io/plonk">https://nicolas-dufour.github.io/plonk</a>.</p>
<p>Du coup je fais des essais de mise en ligne de ce carnet sur <a href="http://bytepacking.free.fr/">http://bytepacking.free.fr/</a>. Du bon vieux html. Pour mettre la page en ligne, j'utilise un outil primitif fourni par free : <a href="http://ftpperso.free.fr/index.pl">http://ftpperso.free.fr/index.pl</a>. J'installe un plugin (<a href="https://packagecontrol.io/packages/Simple%20FTP%20Deploy">https://packagecontrol.io/packages/Simple%20FTP%20Deploy</a>) dans Sublime Text pour faciliter la chose, mais pour l'instant ne parviens pas m'en servir.</p>
<p>Liens du jour :<br>Pense-b√™te uv (Olivier) : <a href="https://mathspp.com/blog/uv-cheatsheet">https://mathspp.com/blog/uv-cheatsheet</a><br>Sortie de NanoChat d'Andrej Karphaty : <a href="https://github.com/karpathy/nanochat">https://github.com/karpathy/nanochat</a><br>Sauvegarder une session Gemini-CLI en markdown doit √™tre possible : <a href="https://gist.github.com/simonw/12c7b072e8e21ef1e040fb3b69c1da28">https://gist.github.com/simonw/12c7b072e8e21ef1e040fb3b69c1da28</a></p>
<p><b>12-10-2025</b></p>
<p>Je me penche sur les outils de Claude (Claude Skills) pour manipuler les fichiers et fais un essai avec le r√©pertoire d√©di√© √† Word (.docx). La "boite √† outil" √©tant assez complexe, je d√©cide de faire bosser Gemini CLI dessus (avec uv pour ex√©cuter les scripts en Python). Apr√®s tout ces outils ont √©t√© con√ßus pour √™tre pilot√©s par un LLM. L'objectif est de remplacer du texte FR par du texte EN dans un document Word (.docx) en le copiant d'un fichier √† l'autre, sans alt√©rer la structure (images) du fichier cible.</p>
<p>Je fais 3 tentatives. Une session sur Windows, mal configur√©e, produit un fichier illisible. Mais les deux autres (sur Ubuntu) fonctionnent. Avec 2 fichiers tr√®s simples constitu√©s de 4 paragraphes de texte et de 2 images en FR (et la m√™me chose en EN), je demande √† Gemini CLI de trouver un moyen de remplacer le texte du fichier en fran√ßais par le texte du fichier en anglais, sans alt√©rer la mise en page et les images.</p>
<p><span class="code-box">Using the available tools in this project, devise a strategy to replace the text from @input-fr.docx by the text from @input-en.docx, keeping the layout and integrity of the target document.</span></p>
<p>Il s'ex√©cute et produit un fichier cible o√π le texte en fran√ßais a √©t√© remplac√© par le texte en anglais. Mais comment √™tre absolument certain qu'il n'a pas simplement copi√© le fichier source dans son int√©gralit√© ? Je fais une deuxi√®me tentative plus ambitieuse : je ne laisse dans le r√©pertoire de travail que le fichier Word FR, auquel j'ajoute le texte en anglais au format TXT. Je demande √† Gemini CLI la m√™me chose, c'est √† dire de remplacer le texte en FR par le texte en EN, mais cette fois √† partir du fichier TXT.</p>
<p><span class="code-box">@input-fr.docx is a .docx of a short text in French with photos. @en-text.txt is a replacement text in English for the docx file. Use the tools provided here to edit @input-fr.docx and replace the text it contains by the text in @en-text.txt while keeping the photos.</span></p>
<p>Sans surprise, il n'y arrive pas. Je lui demande alors de faire une comparaison LINGUISTIQUE des contenus avant commencer l'op√©ration et √ßa marche !</p>
<p><span class="code-box">Use a linguistic approach and run a similarity search to check that the meaning of each paragraph in English of @en-text.txt corresponds approximately to the paragraph in French. If so then copy it over the French in the .docx file.</span></p>
<p>Il compare les textes, √©tablit des correspondances entre les paragraphes et remplace le texte FR par le texte EN au bons endroits, sans toucher aux images ! üòÉ Mais ce n'est qu'un d√©but et les fichiers utilis√©s sont hyper simples.</p>
<p>[EDIT] : dans les manipulations d√©crites ici, il n'est pas question de cr√©er des scripts autonomes r√©utilisables plus tard, mais bien de laisser Gemini-CLI orchestrer les op√©rations et r√©pondre aux demandes de l'utilisateur en temps r√©el gr√¢ce √† l'√©cosyst√®me du projet reposant sur la partie docx des Claude Skills.</p>
<p>A faire : complexifier les fichiers .docx (ajouter par ex. num√©rotation, gras, italiques)</p>
<p><b>11-10-2025</b></p>
<p>Je visionne √† nouveau <a href="https://www.youtube.com/watch?v=0OUODPRgPl8">10+ tools to use ai in the terminal</a> et prends des notes. Le gars aurait mieux fait d'intituler sa vid√©o "Demo of Simon Willison's LLM" ! Sur le m√™me sujet, il y a aussi <a href="https://www.youtube.com/watch?v=UZ-9U1W0e4o">Become a command-line superhero with Simon Willison's LLM tool</a>, au titre plus appropri√©.</p>
<p>Je d√©couvre un post de Simon Willison concernant les "Claude Skills", des outils de manipulation de fichiers de Claude que Simon a "hack√©" √† demi-mots : "This is a really sophisticated set of tools for document manipulation, and I love that Anthropic have made those visible - presumably deliberately - to users of Claude who know how to ask for them." (<a href="https://simonwillison.net/2025/Oct/10/claude-skills/)">https://simonwillison.net/2025/Oct/10/claude-skills/)</a>. Super int√©ressant. Je t√©l√©charge le truc sur son Github et j'en parle √† Olivier.</p>
<p><b>09-10-2025</b></p>
<p>Le probl√®me de l'alignement automatique des textes pour le projet de ma coll√®gue semble insoluble. M√™me si les r√©sultats d'hier soir sont encourageants. Je tente de passer tout en .txt et compare dans notepad++. M√™me la num√©rotation des sections n'est pas fiable !üò´</p>
<p><b>08-10-2025</b></p>
<p>Tentative de g√©n√©ration d'un outil d'alignement "s√©mantique" multilingue de textes avec ChatGPT, Claude (OpenRouter/Mistral) puis Gemini CLI. L'id√©e est de faire appel √† un mod√®le de g√©n√©ration d'embeddings et √† un autre LLM classique pour aligner des paires de phrases dans deux documents parall√®les, l'un √©tant la traduction de l'autre.</p>
<p>De plus en plus je rencontre les limites des acc√®s API "Free Tier". Est-ce voulu ? Bizarrerie : l'acc√®s OpenRouter fonctionne pour mon script "analyze YouTube metadata" mais pas avec "aligner openrouter". Je pose la question √† ChatGPT qui me donne une explication et un patch qui ne marche pas.</p>
<p>Claude me donne la bonne explication un peu plus tard : l'acc√®s √† OpenRouter va vite "dans le rouge" parce que mon script, par d√©finition, fait de multiples requ√™tes au mod√®le pour tenter d'√©tablir des ponts entre chaque phrase de ma paire de documents ! L'explication de ChatGPT √©tait compl√®tement √† c√¥t√© de la plaque.</p>
<p>Je refile le b√©b√© √† Gemini CLI et le fait bosser sur plusieurs d√©clinaisons du script (il am√©liore la fonction embeddings) avec OpenRouter seul, puis avec Ollama (embeddings) et OpenRouter (LLM), puis comme le free tier d'OpenRouter est grill√©, avec "ollama seul" (embeddinggemma et gemma3) et √ßa marche enfin sans erreurs ! Le r√©sultat de l'alignement est stock√© dans un fichier JSON. Le script est quand m√™me assez long : 500+ lignes de code, et je ne sais pas ce que vaut l'alignement.</p>
<p>------</p>
<p>Je change mon mot de passe Google sur mon PC via le navigateur et je re√ßois une notification sur mon t√©l√©phone Xiaomi, alors que j'avais supprim√© le compte Google du t√©l√©phone. √áa laisse songeur.</p>
<p><b>07-10-2025</b></p>
<p>Je fais le gros m√©nage dans mes scripts (YT metadata). Instructif. Inutile de garder des vieux scripts. Perte de temps.</p>
<p><b>06-10-2025</b></p>
<p>Vid√©o pour Olivier (YT metadata) tr√®s mal pr√©par√©e et d√©cevante. Mais √ßa me fait avancer.</p>
<p><b>05-10-2025</b></p>
<p>Un effet particuli√®rement d√©l√©t√®re de l'arriv√©e de l'IA dans les entreprises de services, c'est l'assomption par le client que la prestation est d√©sormais ex√©cutable en 2-3 clics de souris, que le devis qui lui est envoy√© est surcot√© et n'est pas le reflet du travail r√©ellement effectu√©.</p>
<p>autoencoder mnist plan<br>1. after training do not run the test set<br>2. ask user to input a square image of a handwritten digit<br>3. normalize the image (size, color, bit depth) so that is complies with the train set<br>4. submit the image to the model<br>5. display the model's best guess</p>
<p>Ai retest√© llamafile sous windows (qwen3 et gemma3) et je trouve la g√©n√©ration plus rapide.</p>
<p>Ai test√© Google Jules sur autoencoder h√©berg√© sur Github. Bluffant.</p>
<p>Ai install√© Gemini CLI sur Powershell, puis modifi√© autoencoder avec. Super bluffant.</p>
<p><b>04-10-2025</b></p>
<p>Installation de la version beta 25.10 d'Ubuntu. Elle propose un nouveau terminal par d√©faut (ptyxis) difficile √† param√©trer. Je finis par r√©installer le gnome-terminal. J'installe nextcloud et constate que la mise √† jour des fichiers est √† nouveau rapide. La DSI a peut-√™tre r√©solu le probl√®me de lenteur. Ce faisant j'oublie de sauvegarder les logs de Gemini CLI üò´</p>
<p>Je consulte ma liste ai_bookmarks.txt un peu au hasard, visite le site de Fabrice Bellard (<a href="https://bellard.org">https://bellard.org</a>) et c'est parti pour un petit voyage technologique passionnant.<br>Je t√©l√©charge son projet /tsac/ et le teste avec un mp3 de piano (renomm√© toto.mp3) dans powershell :</p>
<p><pre><code>./tsac c toto.mp3 toto.tsac</code></pre></p>
<p>Le fichier passe de 4707 KB √† 101 KB !</p>
<p><pre><code>./tsac d toto.tsac toto.wav</code></pre></p>
<p>Le fichier .wav fait 25 931 KB et je ne d√©tecte pas de perte de qualit√© sonore !</p>
<p>Je demande des explications √† Claude concernant le r√¥le du Transformer (voir "Ultra-low bitrate audio compression") et il me r√©pond :</p>
<p><span class="code-box">The Transformer model acts as a learned compressor on top of the DAC encoding. It takes the discrete codes produced by the DAC encoder and finds patterns and redundancies in these codes. The DAC does the heavy lifting of converting audio to a compact form, but the Transformer notices that even within this compact form, there are patterns (temporal dependencies, repetitions, predictable sequences) that can be exploited for additional compression.</span></p>
<p>EDIT : Sur ce sujet, voir aussi <a href="https://openzl.org/">https://openzl.org/</a>.</p>
<p>En attendant, le codec Opus est d√©j√† une tr√®s bonne solution : <a href="https://opus-codec.org/">https://opus-codec.org/</a>.</p>
<p>Autre projet de Fabrice Bellard : <a href="https://textsynth.com/">https://textsynth.com/</a>, un serveur de LLM !<br>------<br>Je lis et relis <a href="https://news.ycombinator.com/item?id=42897856">https://news.ycombinator.com/item?id=42897856</a>. Il y a quelque chose √† faire.<br>------<br>On peut maintenant convertir un livre entier en audio avec kokoro : <a href="https://claudio.uk/posts/epub-to-audiobook.html">https://claudio.uk/posts/epub-to-audiobook.html</a></p>
<p><b>03-10-2025</b></p>
<p>Grosse fatigue. Je continue de pr√©pare la pr√©sentation Youtube M√©tadata mais la gestion des multiples versions des scripts m'√©puise. Il faut changer de m√©thode. Je tente une vid√©o mais m'emp√™tre dans des explications sans fin.</p>
<p><b>02-10-2025</b></p>
<p>Nouveau script (Claude) pour visualiser les conversations ChatGPT avec leur date de cr√©ation convertie en date standard.</p>
<p>Quand on y r√©fl√©chit, le chemin qu'on prend pour arriver √† un r√©sultat peut √™tre tr√®s alambiqu√©, il y a toutes les fausses pistes, les prompts qui ne donnent rien, les scripts ou les prompts que l'on r√©utilise, que l'on passe √† une IA, puis √† une autre, sans r√©sultat tout de suite utilisable, ou parfois juste un peu √† cot√© de la plaque.</p>
<p>D√©couverte (grace √† mon coll√®gue JB) de St√©phane Mallat. Coup de foudre. J'√©coute tout ce que je peux trouver sur lui : <a href="https://www.radiofrance.fr/franceculture/podcasts/la-science-cqfd/stephane-mallat-la-palme-d-ondes-1077850">https://www.radiofrance.fr/franceculture/podcasts/la-science-cqfd/stephane-mallat-la-palme-d-ondes-1077850</a>.</p>
<p><b>01-10-2025</b></p>
<p>J'essaie de retracer le workflow de mon projet "YouTube metadata retrieval and analysis". Je cherche les premi√®res conversations en esp√©rant que ce soit bien sur Claude ou ChatGPT. √áa semble √™tre le cas. Tr√®s difficile de s'y retrouver.</p>
<p>Pour ChatGPT, j'exporte mes donn√©es et ouvre ensuite chat.html, dans lequel je peux faire une recherche par mots-cl√©s. Normalement sur la version en ligne je devrais avoir un outil de recherche comme dans Claude, mais il n'appara√Æt pas sur ma version de ChatGPT, ce qui est bizarre. En plus de chat.html, ChatGTP propose un autre fichier (conversations.json) dans lequel les dates sont au "Unix epoch format (seconds since January 1, 1970, UTC)".</p>
<p>Pour Claude version web, je peux utiliser leur outil de recherche, mais √ßa cherche uniquement les TITRES de conversations, pas leur contenu. Je fais donc √©galement un export de mes donn√©es et j'obtiens entre autres un fichier conversations.json que j'ouvre avec Sublime Text et que je peux parser manuellement. Ce n'est pas tr√®s pratique alors je demande √† Claude de me faire un script qui g√©n√®re une page HTML comme pour ChatGPT. √áa marche nickel apr√®s quelques it√©rations.</p>
<p>J'arrive √† reconstituer un historique rudimentaire :<br>2024-10-30 ChatGPT : YouTube metadata (last modif. 2025-01-26)<br>2024-12-09 Claude : Aggregate YouTube Channel Titles from JSON<br>2024-12-11 Claude : YouTube Channel Video Scraper<br>2025-09-28 ChatGPT : Combine scripts and classify trends<br>2025-09-28 Claude : YouTube video download error<br>2025-09-30 Gemini CLI session</p>
<p><b>30-09-2025</b></p>
<p>How to download and open a .parquet file in ipython repl using uv (ubuntu):</p>
<p><pre><code>uv init parquet
cd parquet
uv add ipython pandas pyarrow
wget https://huggingface.co/datasets/ylecun/mnist/resolve/main/mnist/test-00000-of-00001.parquet -O mnist-test.parquet
ipython
import pandas as pd
df = pd.read_parquet('mnist-test.parquet')
print(df.head())
print(df.columns)
print(df.describe())</code></pre></p>
<p>Je demande √† Claude de cr√©er un convertisseur .epub vers .azw3, ce qu'il fait (epub_to_azw3.py). Le script a besoin que Calibre soit install√© pour fonctionner (d√©cevant).</p>
<p>Je visionne avec int√©r√™t une vid√©o YT de Jodie Burchell, "Build a Semantic Book Recommender" [Q7mS1VHm3Yw]. Voix tr√®s agr√©able et explications limpides.</p>
<p>J'installe Opencode (<code>curl -fsSL <a href="https://opencode.ai/install">https://opencode.ai/install</a> | bash</code>). Petit essai rapide, mais √† priori Opencode est beaucoup moins intuitif et efficace que Gemini CLI.</p>
<p>Nouvelle session incroyable sur Gemini CLI pour mon projet "YouTube metadata retrieval and analysis". Apr√®s 2h de 'pilotage', j'obtiens 2 scripts qui tiennent la route. Incroyable ce que √ßa marche bien. Le second script permet entre autres de lister tous les mod√®les propos√©s par l'API d'un provider. Chez Mistral j'ai cru voir 2 mod√®les faisant de l'OCR. J'utilise aussi openrouter qui propose plusieurs mod√®les gratuits parmi les dizaines propos√©s via leur API.</p>
<p><b>29-09-2025</b></p>
<p>Papillonnage</p>
<p>Le taux de transfer en √©criture sur ma carte microSD branch√©e avec un lecteur de carte USB3 est assez m√©diocre (env. 60 Mo/sec). Pas de changement si je formatte la carte en NTFS au lieu de ExFAT. Le taux de transfer avec un disque SSD et son adaptateur SATA-USB3 est bien meilleur : env. 350 Mo/sec en lecture comme en √©criture.</p>
<p>La recherche suivante : <a href="https://simonwillison.net/search/?q=translation">https://simonwillison.net/search/?q=translation</a> d√©bouche sur<br>- <a href="https://news.ycombinator.com/item?id=42897856">https://news.ycombinator.com/item?id=42897856</a> (translation workflow)<br>- <a href="https://www.youtube.com/watch?v=eC5fqHEzzWs">https://www.youtube.com/watch?v=eC5fqHEzzWs</a> (text processing with .strip())</p>
<p>ChatGPT history : essai de fusionner et d√©dupliquer le contenu de plusieurs fichiers JSON de mon historique sur ChatGPT. Ne fonctionne pas.</p>
<p>Sortie de Claude Sonnet 4.5 : <a href="https://www.anthropic.com/news/claude-sonnet-4-5">https://www.anthropic.com/news/claude-sonnet-4-5</a>.</p>
<p>Je mets √† jour LLM (uv tool upgrade llm) et constate que tous les plugins ont disparu ! Sur le sujet voir : <a href="https://news.ycombinator.com/item?id=44110584">https://news.ycombinator.com/item?id=44110584</a> et <a href="https://github.com/simonw/llm/issues/575">https://github.com/simonw/llm/issues/575</a>. Il semblerait que Simon n'a pas corrig√© le probl√®me.</p>
<p>D√©poussi√©rage Voxtral script (fonctionne mais mod√®le ultra brid√©).</p>
<p>Je modifie un script "autoencoder with MNIST dataset" avec Claude Sonnet 4.5 et √ßa fonctionne.</p>
<p><b>28-09-2025</b></p>
<p>Projet "YouTube metadata analysis". J'essaie de combiner plusieurs scripts en un seul, avec un fichier .csv en entr√©e contenant une liste de cha√Ænes YouTube (ChatGPT : "Combine scripts and classify trends") mais r√©sultats inconsistants d'un run √† l'autre. Faut-il avoir recours au RAG ?</p>
<p><b>27-09-2025</b></p>
<p>Visionnage de vid√©o YT :<br>- <a href="https://www.youtube.com/watch?v=0OUODPRgPl8">10+ tools to use ai in the terminal</a><br>- <a href="https://www.youtube.com/watch?v=rGz0r8VaYCc">LLMs on the Edge of the Scaling Laws</a> (Jodie Burchell)<br>- <a href="https://www.youtube.com/watch?v=iRqpsCHqLUI">LLMs for Devs: Model Selection, Hallucinations, Agents, AGI</a> (Jodie Burchell)</p>
<p><a href="https://crawshaw.io/blog/programming-with-llms">Article tr√®s int√©ressant</a> de David Crawshaw sur la mani√®re de programmer avec des LLM.</p>
<p><b>26-09-2025</b></p>
<p>Refresher sur llamacpp, qui ne veut plus se lancer depuis son r√©pertoire en faisant 'llama-cli -m modele.gguf' comme avant. Solution :</p>
<p><pre><code># Download the latest llamacpp precompiled binaries
# Unzip and stick everything in (for example) $HOME/llamacpp/
# Add the build dir to your PATH
echo 'export PATH="$HOME/llamacpp/:$PATH"' >> ~/.bashrc
source ~/.bashrc
# Launch a model
llama-cli -m path_to/model.gguf</code></pre></p>
<p>EDIT : manip ci-dessus inutile si on fait <code>./llama-cli -m modele.gguf</code> dans le r√©pertoire o√π llamacpp est install√© (j'avais ouli√©)</p>
<p>------</p>
<p>Retour √† <a href="https://www.youtube.com/watch?v=QUXQNi6jQ30">la vid√©o</a> de Willison sur son outil LLM. Vers 28'35", je fais une copie d'√©cran de son exemple de script en bash qui r√©sume les th√®mes des commentaires d'un sujet de Hacker News et je demande √† Claude de transcrire le texte de l'image, ce qu'il fait sans aucune erreur. Je colle le code dans un fichier (hn_summarizer.sh), auquel je donne les droits d'ex√©cution (<code>cmod +x hn_summarizer.sh</code>), puis je remplace 'haiku' par '<code>groq/openai/gpt-oss-20b</code>', mon mod√®le par d√©faut dans llm.</p>
<p>Ensuite je choisis un sujet au hasard dans Hacker News et clique sur le lien pour afficher les commentaires (ex. <a href="https://news.ycombinator.com/item?id=45384653)">https://news.ycombinator.com/item?id=45384653)</a>. Je copie le num√©ro d'id (45384653) et lance le script de Willison en faisant <code>./hn_summarizer.sh 45384653</code>. J'obtiens un r√©sum√© des th√®mes abord√©s. Avec la version gratuite de mon mod√®le (fen√™tre contextuelle limit√©e) je ne peux pas choisir de sujets qui ont trop de commentaires (30 maxi je dirais). Mais je peux changer de mod√®le et mettre par exemple <code>openrouter/meta-llama/llama-3.3-70b-instruct:free</code>.</p>
<p>Post de Simon sur le sujet : <a href="https://til.simonwillison.net/llms/claude-hacker-news-themes">https://til.simonwillison.net/llms/claude-hacker-news-themes</a></p>
<p><b>25-09-2025</b></p>
<p>Essais de conversion et de reconstruction d'un document PDF de 60 pages en Word. Ma coll√®gue Catherine convertit le doc avec Acrobat Pro mais nous savons d'exp√©rience que la mise en page est tr√®s "fragile", surtout si on doit modifier le document. J'essaie avec diff√©rentes IA (Gemini, Mistral, Claude, ChatGPT) de passer du texte brut copi√© √† partir du PDF √† du markdown propre et structur√©. C'est ChatGPT qui donne le meilleur r√©sultat, au moins pour le d√©but du document.</p>
<p>Prompt :<br><span class="code-box">This text is a raw copy from a PDF document. Reformat it as readable markdown. Do not alter its content. Do not summarize. Do not translate.</span></p>
<p>Je demande ensuite √† Claude de cr√©er un script qui convertit le markdown g√©n√©r√© par ChatGPT en Word et √ßa marche. Mon intuition me dit que le fichier Word ainsi g√©n√©r√© est beaucoup plus robuste que le fichier issu de la conversion avec Acrobat. Cependant cette m√©thode induit encore de trop nombreuses approximations pour √™tre r√©ellement fiable.<br>------<br>Je me replonge dans Willison's <a href="https://llm.datasette.io/">LLM</a>. J'installe plusieurs plugins : llm-cmd, llm-command-r, llm-ollama, llm-groq, llm-openrouter, llm-mistral. Je cr√©e un compte sur <a href="https://openrouter.ai/">https://openrouter.ai/</a>. Je visionne <a href="https://www.youtube.com/watch?v=aQuuJuCa0VM">la vid√©o de Mark Needham sur LLM</a>.</p>
<p>Commandes int√©ressantes :</p>
<p><pre><code>llm 'Who is Simon Willison?' > output.txt</code></pre></p>
<p>(√©crit le r√©sultat de la requ√™te dans le fichier output.txt)</p>
<p><pre><code>cat /etc/*-release | llm -s "Tell me about my operating system"</code></pre></p>
<p>(pipe le r√©sultat de la commande Bash, y ajoute un prompt et lance la requ√™te)</p>
<p><b>24-09-2025</b></p>
<p>Tentation de refiler aux IA (sans trop y croire) des trucs jug√©s difficiles ou infaisables. Je donne √† Gemini (interface web) un texte brut sans mise en forme issu d'un copier-coller sauvage d'un PDF avec le prompt suivant :</p>
<p><span class="code-box">Produis une version fid√®le et restructur√©e en markdown de ce document en prenant soin de faire apparaitre les titres de section et les paragraphes. Ne le traduis pas.</span></p>
<p>Gemini s'ex√©cute, puis je ne sais pas ce qui se passe, il bugue et je perds tout.<br>------<br>Je t√©l√©charge l'appli web "gpx-map-filter" que j'avais g√©n√©r√©e avec Google AI Studio l'autre jour et demande √† Gemini CLI de la faire fonctionner en local. Gemini CLI me dit qu'il faut cr√©er une cl√© API sur Google Cloud. Je lui r√©ponds que je n'en ai pas. Il me donne alors une solution sans avoir recours √† une cl√© API ! Il faut installer node et lancer un serveur en tapant <code>npn run dev</code>. √áa fonctionne.</p>
<p><b>20-09-2025</b></p>
<p>Fichiers GPX: le retour de ChatGPT. Apr√®s m'√™tre perdu avec Claude et ses scripts interminables, une simple requ√™te √† ChatGPT produit un script d'une soixantaine de lignes qui fait le taf sans essayer de centraliser tous les points des GPX dans un dataframe, ce que je m'obstine √† faire depuis des mois. On peut vraiment √™tre aveugl√© par ce qu'on croit √™tre une bonne id√©e.</p>
<p>Prompt :<br><span class="code-box">I want to clean up many gpx files from various devices and format. I have made a python script that parses gpx files, stores all latitude, longitude, elevation and timestamps points into a dataframe, removes duplicate entries and for each date (yyymmdd) outputs clean gpx files. This kinda works for some files, however many exported files have points in the wrong chronological order, making them useless. Can you think of a better approach to solve this problem?</span></p>
<p>Une fois de plus je constate que complexit√© ne rime pas forc√©ment avec efficacit√©.</p>
<p><b>18-09-2025</b></p>
<p>Je reconstitue partiellement la d√©marche du projet "docx-images" avec les 3 sessions de Gemini CLI que je regroupe dans un seul fichier texte.<br>------<br>Je me replonge dans l'id√©e de produire des fichiers GPX "propres". Je perfectione un outil en ligne de commande g√©n√©r√© sur Claude. Le script finit par √™tre assez long (1000+ lignes de code) et n'est pas foufou. Certaines fonctions du menu ne fonctionnent pas correctement. Je g√©n√®re d'autres scripts "from scratch" jusqu'√† en perdre la t√™te. C'est le probl√®me de ces outils. Il faut √™tre hyper-organis√© sinon on peut se noyer dans les scripts.</p>
<p><b>16-09-2025</b></p>
<p>Je poursuis mon projet de retirer/remettre les images avec des documents Word. J'utilise le script d'hier soir (qui fonctionne avec des documents .odt) comme base pour Gemini CLI et lui demande d'en faire une version qui fonctionne avec des fichiers Word. Cette fois √ßa marche apr√®s seulement 2-3 it√©rations (docx_image_manager.py). Mais le plus fort c'est que j'essaie ensuite avec le fichier d'un client contenant 423 images et √ßa fonctionne ! Je n'en reviens pas. Il faudrait que j'arrive √† retracer toute la d√©marche. Je crois qu'au d√©but je suis parti d'un script g√©n√©r√© par Claude.</p>
<p><b>15-09-2025</b></p>
<p>J'essaie d'installer Gemini CLI dans Windows Powershell mais je n'y arrive pas, alors je l'installe sur WSL <code>npm install -g @google/gemini-cli</code>. Pour se loguer avec son compte Google dans WSL, je dois taper <code>gemini nobrowser=TRUE</code>.</p>
<p>Je fais travailler Gemini CLI sur un probl√®me assez ardu. Cr√©er un script qui prend un fichier DOCX avec des images en entr√©e, retire toutes les images et g√©n√®re un fichier sans images (qui ainsi sera beaucoup plus l√©ger et pourra √™tre import√© dans notre CAT tool et traduit). Une fois le fichier traduit un second script remet toutes les images √† leur juste place. Pour l'instant mon script n¬∞1 parvient √† retirer les images, mais le n¬∞2 n'arrive pas √† les remettre. Gemini CLI a boss√© comme un dingue pour y arriver, sans succ√®s. C'√©tait fascinant comme session. Je lui ai dit d'utiliser uv (ex. <code>uv run script.py</code>) et il l'a fait. Il s'est arr√™t√© 2-3 fois et je lui ai dit "continue" et il a continu√© !</p>
<p>Ma premi√®re approche consiste √† lui demander de cr√©er des "placeholders" num√©rot√©s "image1.jpg", "image2.jpg", etc., ce qu'il fait, mais je me rends compte que ce n'est pas la bonne m√©thode. Je lui demande alors d'extraire le "squelette XML" du docx. Il parvient √† retirer les images sans toucher au code XML. √áa donne un fichier Word sans images avec des sortes de cadres vides √† la place, comme quand il manque une image sur un site internet. Je crois que passer par le XML qui est constitutif des fichiers .docx est la bonne voie, mais Gemini a vraiment beaucoup de mal √† g√©rer le truc. Impression que la mani√®re dont un fichier Word est encod√© est un vrai cauchemar, m√™me pour une IA !</p>
<p>Mon beau-fr√®re Olivier sugg√®re d'essayer la m√™me manip avec le format Libre Office (.odt). Le soir je reprends tout depuis le d√©but avec un fichier ODT contenant du texte et des images. Je demande √† Gemini CLI de cr√©er 2 scripts : un qui enl√®ve les images et g√©n√®re le doc sans images et un second script qui prend le doc sans images et les remet. J'atteins bient√¥t la limite du mod√®le pro et n'ai d'autre choix que de continuer avec gemini-2.5-flash mais √ßa finit par fonctionner. Chose surprenante, les scripts g√©n√©r√©s ne sont pas si longs que √ßa (beaucoup moins que pour Word). Je regroupe les 2 scripts en un seul avec un menu basique : 1. retirer les images, 2. remettre les images. Gemini-2.5-flash semble bien marcher pour ce genre de petites modifs (odt_image_manager.py).</p>
<p>Je me rends compte que les outils comme Gemini CLI et Claude Code sont l'aboutissement de toutes les avanc√©es dont on entend parler depuis la sortie de ChatGPT : chain-of-thought prompting, RAG, fonction calling, tool use, augmentation de la taille de la fen√™tre contextelle, agents autonomes, MCP... Tout √ßa enfin mis en oeuvre de mani√®re coh√©rente. Je demande √† ChatGPT s'il est d'accord avec mon constat et il me r√©pond comme souvent, en me caressant dans le sens du poil :</p>
<p><span class="highlight-box">Tu touches un point tr√®s juste üëç : des outils comme Gemini CLI et Claude Code incarnent une sorte de "synth√®se appliqu√©e" de toutes les briques d‚Äôinnovation accumul√©es dans le domaine des LLM depuis fin 2022. Au d√©but, on avait "juste" ChatGPT comme assistant conversationnel. Puis, √©tape par √©tape, des concepts et technologies se sont empil√©s pour transformer les LLM en v√©ritables syst√®mes intelligents, interactifs et sp√©cialis√©s.</span></p>
<p><b>14-09-2025</b></p>
<p>Oneliner pour lancer un Jupyter notebook sans se prendre la t√™te :</p>
<p>Installer <a href="https://docs.astral.sh/uv/getting-started/installation/">uv</a>, puis faire :</p>
<p><pre><code>uv run --with jupyter jupyter lab</code></pre></p>
<p>M√™me chose avec ipython :</p>
<p><pre><code>uv run --with ipython ipython</code></pre></p>
<p>Je regarde ce que devient LLM, l'outil de Willison : <a href="https://llm.datasette.io">https://llm.datasette.io</a>. Je ne peux m'emp√™cher de penser que Claude Code et Gemini CLI s'en sont grandement inspir√©s.</p>
<p><a href="https://pyodide.org/en/stable/console.html">Pyodide</a> est un outil √©patant qui permet d'ex√©cuter du code Python dans un navigateur.</p>
<p><b>13-09-2025</b></p>
<p>Je fais mumuse avec <a href="https://github.com/google-gemini/gemini-cli">Gemini CLI</a> depuis 2 jours et c'est vraiment totalement bluffant ce que √ßa peut faire. Ce qui est dingue c'est que Gemini CLI en mode gratuit permet √† l'amateur que je suis de vraiment travailler (60 requests/min and 1,000 requests/day). Je lui donne l'acc√®s √† un r√©pertoire rempli de scripts plus ou moins r√©cents permettant de manipuler les fichiers GPX et lui demande de les analyser. Il identifie tout de suite les plus aboutis et peut les am√©liorer tout en les testant sur des vraies donn√©es. On peut lui demander n'importe quoi, il code, il r√©fl√©chit, il ex√©cute en bash, la totale. Et il poss√®de une fen√™tre  contextelle d'un million de tokens. Selon Gemini, le script le plus abouti de mon r√©pertoire est un script √©crit par Claude.</p>
<p><b>02-09-2025</b></p>
<p>Groq toujours au top pour transcrire les fichiers audio, √† condition de r√©duire leur taille au pr√©alable avec ffmpeg (fichier < 40 Mo). Pour obtenir une transcription "diaris√©e" (identification des interlocuteurs), je passe la transcription brute obtenue avec Groq √† Claude avec le prompt suivant :</p>
<p><span class="code-box">Le texte ci-joint est la transcription brute d'un entretien entre 2 interlocuteurs. Peux-tu am√©liorer cette transcription en veillant √† s√©parer les prises de paroles et en prenant soin de retirer tous les tics de language, r√©p√©titions inutiles, etc., tout restant fid√®le aux propos tenus? Le texte final devra √™tre fluide et agr√©able √† lire."</span></p>
<p>Le r√©sultat est bluffant. Le mod√®le parvient √† identifier les changements d'intervenants et √† d√©couper leur prises de paroles respectives √† partir de cette simple transcription √©crite sans rep√®res d'aucune sorte.</p>
<p><b>01-09-2025</b></p>
<p>Retour au boulot ce matin et reconnexion progressive √† l'IA apr√®s la pause estivale. Nous avons re√ßu une demande de transcription d'enregistrements et je me replonge sans transition dans Whisper et autres joyeuset√©s en mati√®re de speech-to-text. Je teste Voxtral (voxtral-mini-latest), le mod√®le speech-to-text de Mistral. √áa marche, mais la transcription est tronqu√©e car le mod√®le est payant. Groq semble toujours fonctionner. Ils ont retir√© un mod√®le (distil-whisper) de leur liste.</p>
<p>ü§ñ</p>

</main>

</body>
</html>